<!DOCTYPE html><html lang="zh-CN,en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/uploads/icon/drop/128x128.png"><link rel="icon" type="image/png" sizes="32x32" href="/uploads/icon/drop/32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/uploads/icon/drop/16x16.png"><link rel="mask-icon" href="/uploads/icon/drop/drop.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"cwscn.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1,width:240},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!1,mediumzoom:!0,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"fadeIn"}},path:"search.xml"}</script><meta name="description" content="每当想要根据给定输入预测某个结果，并且还有输入/输出对的示例时，都应该使用监督学习。这些输入/输出对构成了训练集，我们利用它来构建机器学习模型。我们的目标是对从未见过的新数据做出准确预测。监督学习通常需要人力来构建训练集，但之后的任务本来非常费力甚至无法完成，现在却可以自动完成，通常速度也很快。 分类和回归 监督机器学习问题主要有两种，分别叫做分类（classification）与回归（regre"><meta name="keywords" content="Python 机器学习基础教程,监督"><meta property="og:type" content="article"><meta property="og:title" content="监督学习"><meta property="og:url" content="https://cwscn.github.io/notes/introduction-to-ml-with-python/supervised-learning/index.html"><meta property="og:site_name" content="春夏秋冬"><meta property="og:description" content="每当想要根据给定输入预测某个结果，并且还有输入/输出对的示例时，都应该使用监督学习。这些输入/输出对构成了训练集，我们利用它来构建机器学习模型。我们的目标是对从未见过的新数据做出准确预测。监督学习通常需要人力来构建训练集，但之后的任务本来非常费力甚至无法完成，现在却可以自动完成，通常速度也很快。 分类和回归 监督机器学习问题主要有两种，分别叫做分类（classification）与回归（regre"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/trade-off-of-model-complexity-against-training-and-test-accuracy.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/datasets-forge.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/datasets-wave.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/decision-boundary-and-decision-function-of-gbrt.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/decision-boundary-and-predict-proba-of-gbrt.png"><meta property="og:updated_time" content="2020-08-07T13:44:46.104Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="监督学习"><meta name="twitter:description" content="每当想要根据给定输入预测某个结果，并且还有输入/输出对的示例时，都应该使用监督学习。这些输入/输出对构成了训练集，我们利用它来构建机器学习模型。我们的目标是对从未见过的新数据做出准确预测。监督学习通常需要人力来构建训练集，但之后的任务本来非常费力甚至无法完成，现在却可以自动完成，通常速度也很快。 分类和回归 监督机器学习问题主要有两种，分别叫做分类（classification）与回归（regre"><meta name="twitter:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/trade-off-of-model-complexity-against-training-and-test-accuracy.png"><link rel="canonical" href="https://cwscn.github.io/notes/introduction-to-ml-with-python/supervised-learning/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><link rel="stylesheet" href="//unpkg.com/video.js/dist/video-js.min.css"><link rel="stylesheet" href="/css/videojs-bilibili.css"><style>.aplayer.aplayer-arrow .aplayer-icon-loop,.aplayer.aplayer-arrow .aplayer-icon-order{display:inline-block}</style><title>监督学习 | 春夏秋冬</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">春夏秋冬</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">愿你走出半生 归来仍是少年</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档<span class="badge">209</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类<span class="badge">11</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签<span class="badge">153</span></a></li><li class="menu-item menu-item-收藏"><a href="/favlist/" rel="section"><i class="fa fa-star fa-fw"></i> 收藏</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div><meting-js server="netease" type="playlist" id="67155774" theme="#ff5555" loop="all" order="list" preload="none" volume="" mutex="" list-folded="NaN" fixed="true"></meting-js></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://cwscn.github.io/notes/introduction-to-ml-with-python/supervised-learning/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar/nekosensei.png"><meta itemprop="name" content="菜农陈文生"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="春夏秋冬"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 监督学习</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-08-07 21:44:46" itemprop="dateModified" datetime="2020-08-07T21:44:46+08:00">2020-08-07</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/notes/introduction-to-ml-with-python/supervised-learning/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/notes/introduction-to-ml-with-python/supervised-learning/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>每当想要根据给定输入预测某个结果，并且还有输入/输出对的示例时，都应该使用监督学习。这些输入/输出对构成了训练集，我们利用它来构建机器学习模型。我们的目标是对从未见过的新数据做出准确预测。监督学习通常需要人力来构建训练集，但之后的任务本来非常费力甚至无法完成，现在却可以自动完成，通常速度也很快。</p><h1 id="分类和回归">分类和回归</h1><p>监督机器学习问题主要有两种，分别叫做分类（classification）与回归（regression）。</p><p>分类问题的目标是预测类别标签（class label），这些标签来自预定义的可选列表。分类问题有时可分为而分类（binary classification）和多分类（multiclass classification）。</p><p>在二分类问题中，我们通常将其中一个类别称为正类（positive class），另一个类别称为反类（negative class）。而鸢尾花的例子则属于多分类问题。另一个多分类的例子是根据网站上的文本预测网站所用的语言。这里的类别就是预定义的语言列表。</p><p>回归任务的目标是预测一个连续值，编程术语叫做浮点数（floating-point number），数学术语叫做实数（real number）。根据教育水平、年龄和居住地来预测一个人的年收入，这就是回归的一个例子。在预测收入时，预测值是一个金额，可以在给定范围内任意取值。回归任务的另一个例子是，根据上一年的产量、天气和农场员工数等属性来预测农场的产量。同样，产量也可以取任意数值。</p><a id="more"></a><p>区分分类任务和回归任务有一个简单的方法，就是问一个问题：输出是否具有某种连续性。如果在可能的结果之间具有连续性，那么它就是一个回归问题。想想预测年收入的例子，输出具有非常明显的连续性。一年赚40000美元还是40001美元并没有实质差别，即时两者金额不同。如果我们的算法在本应预测40000美元时的预测结果是39990美元或40001美元，不必过分在意。</p><p>与此相反，对于识别网站语言的任务来说，并不存在程度问题。网站使用的要么是这种语言，要么是那种语言。在语言之间不存在连续性，在英语和法语之间不存在其他语言。</p><h1 id="泛化过拟合与欠拟合">泛化、过拟合与欠拟合</h1><p>在监督学习中，我们想要在训练数据上构建模型，然后能够对没见过的新数据（这些新数据与训练集具有相同的特性）做出准确预测。如果一个模型能够对没见过的数据做出准确预测，我们就说它能够从训练集泛化（generalize）到测试集。我们想要构建一个泛化精度尽可能高的模型。</p><p>通常来说，我们构建模型，使其在训练集上能够做出准确预测。如果训练集和测试集足够相似，我们预计模型在测试集上也能做出准确预测。不过在某些情况下这一点并不成立。例如，如果我们可以构建非常复杂的模型，那么在训练集上的精度可以想多高就多高。</p><p>为了说明这一点，我们来看一个虚构的例子。比如有一个新手数据科学家，已知之前船的买家记录和对买船不感兴趣的顾客记录，想要预测某个顾客是否会买船（在现实世界中，这实际上是一个非常复杂的问题。虽然我们知道其他顾客还没有从我们这里买过船，但他们可能已经在其他人那里买过了，或者仍在存钱并打算将来再买）。目标是向可能购买的人发送促销电子邮件，而不去打扰那些不感兴趣的顾客。</p><p>判断一个算法在新数据上表现好坏的唯一度量，就是在测试集上的评估。然而从直觉上看（在数学上也可以证明这一点，奥卡姆剃刀理论），我们认为简单的模型对新数据的泛化能力更好。如果规律是“年龄大于50岁的人想要买船”，并且还可以解释所有顾客的行为，那么我们将更相信这条规律，而不是与年龄、子女和婚姻状况都有关系的那条规律。因此，我们总想找到最简单的模型。构建一个对现有信息量来说过于复杂的模型，这被称之为过拟合（overfitting）。如果你在拟合模型时过分关注训练集的细节，得到了一个在训练集上表现很好、但不能泛化到新数据上的模型，那么就存在过拟合。与之相反，如果你的模型过于简单——比如说，“有房子的人都买船”——那么你可能无法抓住数据的全部内容以及数据中的变化，你的模型甚至在训练集上的表现就很差。选择过于简单的模型被称为欠拟合（underfitting）。</p><p>我们的模型越复杂，在训练数据上的预测结果就越好。但是，如果我们的模型过于复杂，我们开始过多关注训练集中每个单独的数据点，模型就不能很好地泛化到新数据上。二者之间存在一个最佳位置，可以得到最好的泛化性能。这就是我们想要的模型。</p><p><img src="/uploads/image/introduction-to-ml-with-python/trade-off-of-model-complexity-against-training-and-test-accuracy.png"></p><h2 id="模型复杂度与数据集大小的关系">模型复杂度与数据集大小的关系</h2><p>需要注意，模型复杂度与训练数据集中的输入的变化密切相关：数据集中包含的数据点的变化范围越大，在不发生过拟合的前提下你可以使用的模型就越复杂。通常来说，收集更多的数据点可以有更大的变化范围，所以更大的数据集可以用来构建更复杂的模型。但是，仅复制相同的数据点或收集非常相似的数据是无济于事的。</p><p>收集更多数据，适当构建更复杂的模型，对监督学习任务往往特别有用。在现实世界中，你往往能够决定收集多少数据，这可能比模型调参更为有效。永远不要低谷更多数据的力量！</p><h1 id="监督学习算法">监督学习算法</h1><h2 id="一些样本数据集">一些样本数据集</h2><p>用模拟的forge数据集来说明分类算法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line">X, y = mglearn.datasets.make_forge()</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line">plt.legend([<span class="string">'Class 0'</span>, <span class="string">'Class 1'</span>], loc=<span class="number">4</span>)</span><br><span class="line">plt.xlabel(<span class="string">'First feature'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Second feature'</span>)</span><br><span class="line">print(X.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(26, 2)  # 26行数据，每个数据有2个特征</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/datasets-forge.png"></p><p>用模拟的wave数据集来说明回归算法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">40</span>)</span><br><span class="line">plt.plot(X, y, <span class="string">'o'</span>)</span><br><span class="line">plt.ylim(<span class="number">-3</span>, <span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Target'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/datasets-wave.png"></p><p><strong>从特征较少的数据集（也叫低维数据集）中得出的结论可能并不适用于特征较多的数据集（也叫高维数据集）。</strong></p><p>分类数据集，乳腺癌数据集。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">print(<span class="string">'Keys:'</span>, cancer.keys())</span><br><span class="line">print(<span class="string">'Shape:'</span>, cancer.data.shape)</span><br><span class="line">print(<span class="string">'Feature names:\n&#123;&#125;'</span>.format(cancer.feature_names))  <span class="comment"># 30个特征的名字</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">print(<span class="string">'Counter:'</span>, &#123;n: v <span class="keyword">for</span> n, v <span class="keyword">in</span>  <span class="comment"># 按恶性和良性计数</span></span><br><span class="line">                   zip(cancer.target_names, np.bincount(cancer.target))&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Keys: dict_keys([&apos;data&apos;, &apos;target&apos;, &apos;target_names&apos;, &apos;DESCR&apos;, &apos;feature_names&apos;, &apos;filename&apos;])</span><br><span class="line">Shape: (569, 30)</span><br><span class="line">Feature names:</span><br><span class="line">[&apos;mean radius&apos; &apos;mean texture&apos; &apos;mean perimeter&apos; &apos;mean area&apos;</span><br><span class="line"> &apos;mean smoothness&apos; &apos;mean compactness&apos; &apos;mean concavity&apos;</span><br><span class="line"> &apos;mean concave points&apos; &apos;mean symmetry&apos; &apos;mean fractal dimension&apos;</span><br><span class="line"> &apos;radius error&apos; &apos;texture error&apos; &apos;perimeter error&apos; &apos;area error&apos;</span><br><span class="line"> &apos;smoothness error&apos; &apos;compactness error&apos; &apos;concavity error&apos;</span><br><span class="line"> &apos;concave points error&apos; &apos;symmetry error&apos; &apos;fractal dimension error&apos;</span><br><span class="line"> &apos;worst radius&apos; &apos;worst texture&apos; &apos;worst perimeter&apos; &apos;worst area&apos;</span><br><span class="line"> &apos;worst smoothness&apos; &apos;worst compactness&apos; &apos;worst concavity&apos;</span><br><span class="line"> &apos;worst concave points&apos; &apos;worst symmetry&apos; &apos;worst fractal dimension&apos;]</span><br><span class="line">Counter: &#123;&apos;malignant&apos;: 212, &apos;benign&apos;: 357&#125;</span><br></pre></td></tr></table></figure><p>回归数据集，波士顿房价数据集。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston = load_boston()</span><br><span class="line">print(boston.data.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(506, 13)</span><br></pre></td></tr></table></figure><p>对于我们的目的而言，我们需要扩展这个数据集，输入特征不仅包括这13个测量结果，还包括这些特征之间的乘积（也叫交互项）。换句话说，我们不仅将犯罪率和公路可达性作为特征，还将犯罪率和公路可达性的乘积作为特征。这个导出的数据集可以用mglearn.load_extended_boston函数加载。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line">print(X.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(506, 104)</span><br></pre></td></tr></table></figure><h2 id="k近邻">k近邻</h2> <a href="/notes/introduction-to-ml-with-python/k-nearest-neighbors/" title="前往文章">前往文章</a><h2 id="线性模型">线性模型</h2> <a href="/notes/introduction-to-ml-with-python/linear-models/" title="前往文章">前往文章</a><h2 id="朴素贝叶苏分类器">朴素贝叶苏分类器</h2> <a href="/notes/introduction-to-ml-with-python/naive-bayes-classifiers/" title="前往文章">前往文章</a><h2 id="决策树">决策树</h2> <a href="/notes/introduction-to-ml-with-python/decision-trees/" title="前往文章">前往文章</a><h2 id="决策树集成">决策树集成</h2> <a href="/notes/introduction-to-ml-with-python/decision-tree-ensemble/" title="前往文章">前往文章</a><h2 id="核支持向量机">核支持向量机</h2> <a href="/notes/introduction-to-ml-with-python/kernelized-support-vector-machine/" title="前往文章">前往文章</a><h2 id="神经网络深度学习">神经网络（深度学习）</h2> <a href="/notes/introduction-to-ml-with-python/neural-networks-deep-learning/" title="前往文章">前往文章</a><h1 id="分类器的不确定度估计">分类器的不确定度估计</h1><p>我们还没有谈到<code>scikit-learn</code>接口的另一个有用之处，就是分类器能够给出预测的不正确度估计。一般来说，你感兴趣的不仅是分类器会预测一个测试点属于哪个类别，还包括它对这个预测的置信度。在实践中，不同类型的错误会在现实应用中导致非常不同的结果。想象一个用于测试癌症的医疗应用。假阳性预测可能只会让“患者”接受额外的测试，但假阴性却可能导致重病没有得到治疗。</p><p><code>scikit-learn</code>中有两个函数可用于获取分类器的不确定度估计：<code>decision_function</code>和<code>predict_proba</code>。大多数分类器（但不是全部）都至少有其中一个函数，很多分类器两个都有。我们来构建一个GradientBoostingClassifier分类器（同时拥有<code>decision_function</code>和<code>predict_proba</code>两个方法），看一下这两个函数对一个模拟的二维数据集的作用。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line">X, y = make_circles(noise=<span class="number">0.25</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了便于说明，我们将两个类别重命名为"blue"和"red"</span></span><br><span class="line">y_named = np.array([<span class="string">'blue'</span>, <span class="string">'red'</span>])[y]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以对任意个数组调用train_test_split</span></span><br><span class="line"><span class="comment"># 所有数组的划分都是一致的</span></span><br><span class="line">X_train, X_test, y_train_named, y_test_named, y_train, y_test = \</span><br><span class="line">    train_test_split(X, y_named, y, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建梯度提升模型</span></span><br><span class="line">gbrt = GradientBoostingClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">gbrt.fit(X_train, y_train_named)</span><br></pre></td></tr></table></figure><h2 id="决策函数">决策函数</h2><p>对于二分类的情况，<code>decision_function</code>返回值的形状是<code>(n_samples,)</code>，为每个样本都返回一个浮点数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'X_test.shape: &#123;&#125;'</span>.format(X_test.shape))</span><br><span class="line">print(<span class="string">'Decision function shape: &#123;&#125;'</span>.format(</span><br><span class="line">    gbrt.decision_function(X_test).shape))</span><br><span class="line"><span class="comment"># 显示decision_function的前几个元素</span></span><br><span class="line">print(<span class="string">'Decision function:\n&#123;&#125;'</span>.format(gbrt.decision_function(X_test)[:<span class="number">6</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X_test.shape: (25, 2)</span><br><span class="line">Decision function shape: (25,)</span><br><span class="line">Decision function:</span><br><span class="line">[ 4.13592629 -1.7016989  -3.95106099 -3.62599351  4.28986668  3.66166106]</span><br></pre></td></tr></table></figure><p>这个值表示模型对该数据点属于“正”类的置信程度。正值表示对正类的偏好，负值表示对“反类”（其他类）的偏好。</p><p>我们可以通过仅查看决策函数的正负号来再现预测值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Thresholded decision function:\n&#123;&#125;'</span>.format(</span><br><span class="line">    gbrt.decision_function(X_test) &gt; <span class="number">0</span>))</span><br><span class="line">print(<span class="string">"Predictions:\n&#123;&#125;"</span>.format(gbrt.predict(X_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Thresholded decision function:</span><br><span class="line">[ True False False False  True  True False  True  True  True False  True</span><br><span class="line">  True False  True False False False  True  True  True  True  True False</span><br><span class="line"> False]</span><br><span class="line">Predictions:</span><br><span class="line">[&apos;red&apos; &apos;blue&apos; &apos;blue&apos; &apos;blue&apos; &apos;red&apos; &apos;red&apos; &apos;blue&apos; &apos;red&apos; &apos;red&apos; &apos;red&apos; &apos;blue&apos;</span><br><span class="line"> &apos;red&apos; &apos;red&apos; &apos;blue&apos; &apos;red&apos; &apos;blue&apos; &apos;blue&apos; &apos;blue&apos; &apos;red&apos; &apos;red&apos; &apos;red&apos; &apos;red&apos;</span><br><span class="line"> &apos;red&apos; &apos;blue&apos; &apos;blue&apos;]</span><br></pre></td></tr></table></figure><p>对于二分类问题，“反类”始终是<code>classes_</code>属性的第一个元素，“正”类是<code>classes_</code>的第二个元素。因此，如果你想要完全再现predict的输出，需要利用<code>classes_</code>属性。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将布尔值True/False转换成0和1</span></span><br><span class="line">greater_zero = (gbrt.decision_function(X_test) &gt; <span class="number">0</span>).astype(int)</span><br><span class="line"><span class="comment"># 利用0和1作为classes_的索引</span></span><br><span class="line">pred = gbrt.classes_[greater_zero]</span><br><span class="line"><span class="comment"># pred与gbrt.predict的输出完全相同</span></span><br><span class="line">print(<span class="string">'pred is equal to predictions: &#123;&#125;'</span>.format(</span><br><span class="line">    np.all(pred == gbrt.predict(X_test))))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pred is equal to predictions: True</span><br></pre></td></tr></table></figure><p><code>decision_function</code>可以在任意范围取值，这取决于数据和模型参数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decision_function = gbrt.decision_function(X_test)</span><br><span class="line">print(<span class="string">"Decision function minimum: &#123;:.2f&#125; maximum: &#123;:.2f&#125;"</span>.format(</span><br><span class="line">    np.min(decision_function), np.max(decision_function)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Decision function minimum: -7.69 maximum: 4.29</span><br></pre></td></tr></table></figure><p>由于可以任意缩放，因此<code>decision_function</code>的输出往往很难解释。</p><p>在下面的例子中，我们利用颜色编码在二维平面中画出所有点的<code>decision_function</code>，还有决策边界。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">13</span>, <span class="number">5</span>))</span><br><span class="line">mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[<span class="number">0</span>], alpha=<span class="number">.4</span>,</span><br><span class="line">    fill=<span class="literal">True</span>, cm=mglearn.cm2)</span><br><span class="line">scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[<span class="number">1</span>],</span><br><span class="line">    alpha=<span class="number">.4</span>, cm=mglearn.ReBl)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> axes:</span><br><span class="line">    <span class="comment"># 画出训练点和测试点</span></span><br><span class="line">    mglearn.discrete_scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], y_test,</span><br><span class="line">        markers=<span class="string">'^'</span>, ax=ax)</span><br><span class="line">    mglearn.discrete_scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], y_train,</span><br><span class="line">        markers=<span class="string">'o'</span>, ax=ax)</span><br><span class="line">    ax.set_xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Feature 1'</span>)</span><br><span class="line">cbar = plt.colorbar(scores_image, ax=axes.tolist())</span><br><span class="line">axes[<span class="number">0</span>].legend([<span class="string">'Test class 0'</span>, <span class="string">'Test class 1'</span>, <span class="string">'Train class 0'</span>,</span><br><span class="line">    <span class="string">'Train class 1'</span>], ncol=<span class="number">4</span>, loc=(<span class="number">.1</span>, <span class="number">1.05</span>))</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/decision-boundary-and-decision-function-of-gbrt.png"></p><p>既给出预测结果，又给出分类器的置信程度，这样给出的信息量更大。但在上面的决策函数图像（右）中，很难分辨出两个类别之间的边界。</p><h2 id="预测概率">预测概率</h2><p><code>predict_proba</code>的输出是每个类别的概率，通常比<code>decision_function</code>的输出更容易理解。对于二分类问题，它的形状始终是<code>(n_samples, 2)</code>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Shape of probabilities: &#123;&#125;'</span>.format(gbrt.predict_proba(X_test).shape))</span><br><span class="line"><span class="comment"># 显示predict_proba的前几个元素</span></span><br><span class="line">print(<span class="string">'Predicted probabilities:\n&#123;&#125;'</span>.format(</span><br><span class="line">    gbrt.predict_proba(X_test[:<span class="number">6</span>])))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Shape of probabilities: (25, 2)</span><br><span class="line">Predicted probabilities:</span><br><span class="line">[[0.01573626 0.98426374]</span><br><span class="line"> [0.84575649 0.15424351]</span><br><span class="line"> [0.98112869 0.01887131]</span><br><span class="line"> [0.97406775 0.02593225]</span><br><span class="line"> [0.01352142 0.98647858]</span><br><span class="line"> [0.02504637 0.97495363]]</span><br></pre></td></tr></table></figure><p>每行的第一个元素是第一个类别的估计概率，第二个元素是第二个类别的估计概率。由于<code>predict_proba</code>的输出是一个概率，因此总是在0和1之间，两个类别的元素之和始终为1，因此只有一个类别的概率超过50%。这个类别就是模型的预测结果（由于概率是浮点数，所以不太可能两个都等于0.500。但如果出现了这种情况，预测结果是随机选择的）。</p><p>在上一个输出中可以看到，分类器对大部分点的置信程度都是相对较高的。<strong>不确定度大小实际上反映了数据依赖于模型和参数的不确定度。过拟合更强的模型可能会做出置信程度更高的预测，即使可能是错的。复杂度越低的模型通常对预测的不确定度越大。</strong>如果模型给出的不确定度符合实际情况，那么这个模型被称为<strong>校正</strong>（calibrated）模型。在校正模型中，如果预测有70%的确定度，那么它在70%的情况下正确。</p><p>在下面的例子中，我们再次给出该数据集的决策边界，以及类别1的预测概率。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">13</span>, <span class="number">5</span>))</span><br><span class="line">mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[<span class="number">0</span>], alpha=<span class="number">.4</span>,</span><br><span class="line">    fill=<span class="literal">True</span>, cm=mglearn.cm2)</span><br><span class="line">scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[<span class="number">1</span>],</span><br><span class="line">    alpha=<span class="number">.4</span>, cm=mglearn.ReBl, function=<span class="string">'predict_proba'</span>)  <span class="comment"># proba</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> axes:</span><br><span class="line">    <span class="comment"># 画出训练点和测试点</span></span><br><span class="line">    mglearn.discrete_scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], y_test,</span><br><span class="line">        markers=<span class="string">'^'</span>, ax=ax)</span><br><span class="line">    mglearn.discrete_scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], y_train,</span><br><span class="line">        markers=<span class="string">'o'</span>, ax=ax)</span><br><span class="line">    ax.set_xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Feature 1'</span>)</span><br><span class="line">cbar = plt.colorbar(scores_image, ax=axes.tolist())</span><br><span class="line">axes[<span class="number">0</span>].legend([<span class="string">'Test class 0'</span>, <span class="string">'Test class 1'</span>, <span class="string">'Train class 0'</span>,</span><br><span class="line">    <span class="string">'Train class 1'</span>], ncol=<span class="number">4</span>, loc=(<span class="number">.1</span>, <span class="number">1.05</span>))</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/decision-boundary-and-predict-proba-of-gbrt.png"></p><p>这张图中的边界更加明确，不确定的小块区域清晰可见。</p><h2 id="多分类问题的不确定度">多分类问题的不确定度</h2><p>到目前为止，我们只讨论了二分类问题中的不确定度估计。但<code>decision_function</code>和<code>predict_proba</code>也适用于多分类问题。我们将这两个函数应用于鸢尾花（Iris）数据集，这是一个三分类数据集。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    iris.data, iris.target, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">gbrt = GradientBoostingClassifier(learning_rate=<span class="number">0.01</span>, random_state=<span class="number">0</span>)</span><br><span class="line">gbrt.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Decision function shape: &#123;&#125;'</span>.format(gbrt.decision_function(X_test).shape))</span><br><span class="line"><span class="comment"># 显示决策函数的前几个元素</span></span><br><span class="line">print(<span class="string">'Decision function:\n&#123;&#125;'</span>.format(gbrt.decision_function(X_test)[:<span class="number">6</span>, :]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Decision function shape: (38, 3)</span><br><span class="line">Decision function:</span><br><span class="line">[[-1.9957153   0.04758118 -1.92721297]</span><br><span class="line"> [ 0.0614655  -1.90755689 -1.92793177]</span><br><span class="line"> [-1.99058105 -1.87637856  0.09686741]</span><br><span class="line"> [-1.9957153   0.04758118 -1.92721297]</span><br><span class="line"> [-1.99730166 -0.13469231 -1.20341532]</span><br><span class="line"> [ 0.0614655  -1.90755689 -1.92793177]]</span><br></pre></td></tr></table></figure><p>对于多分类的情况，<code>decision_function</code>的形状为<code>(n_samples, n_classes)</code>，每一列对应每个类别的“确定度分数”，分数较高的类别可能性更大，得分较低的类别可能性较小。你可以找出每个数据点的最大元素，从而利用这些分数再现预测结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Argmax of decision function:\n&#123;&#125;'</span>.format(</span><br><span class="line">    np.argmax(gbrt.decision_function(X_test), axis=<span class="number">1</span>)))</span><br><span class="line">print(<span class="string">'Predictions:\n&#123;&#125;'</span>.format(gbrt.predict(X_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Argmax of decision function:</span><br><span class="line">[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1</span><br><span class="line"> 0]</span><br><span class="line">Predictions:</span><br><span class="line">[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1</span><br><span class="line"> 0]</span><br></pre></td></tr></table></figure><p><code>predict_proba</code>输出的形状相同，也是<code>(n_samples, n_classes)</code>。同样，每个数据点所有可能类别的概率之和为1。同样可以通过计算<code>predict_proba</code>的argmax来再现预测结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示predict_proba的前几个元素</span></span><br><span class="line">print(<span class="string">'Predicted probabilities:\n&#123;&#125;'</span>.format(gbrt.predict_proba(X_test)[:<span class="number">6</span>]))</span><br><span class="line"><span class="comment"># 显示每行的和都是1</span></span><br><span class="line">print(<span class="string">'Sums: &#123;&#125;'</span>.format(gbrt.predict_proba(X_test)[:<span class="number">6</span>].sum(axis=<span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 通过计算predict_proba的argmax来再现预测结果</span></span><br><span class="line">print(<span class="string">'Argmax of predicted probabilities:\n&#123;&#125;'</span>.format(</span><br><span class="line">    np.argmax(gbrt.predict_proba(X_test), axis=<span class="number">1</span>)))</span><br><span class="line">print(<span class="string">'Predictions:\n&#123;&#125;'</span>.format(gbrt.predict(X_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Predicted probabilities:</span><br><span class="line">[[0.10217734 0.78840063 0.10942203]</span><br><span class="line"> [0.7834712  0.1093673  0.1071615 ]</span><br><span class="line"> [0.09818079 0.11005862 0.79176059]</span><br><span class="line"> [0.10217734 0.78840063 0.10942203]</span><br><span class="line"> [0.10360014 0.66723882 0.22916105]</span><br><span class="line"> [0.7834712  0.1093673  0.1071615 ]]</span><br><span class="line">Sums: [1. 1. 1. 1. 1. 1.]</span><br><span class="line">Argmax of predicted probabilities:</span><br><span class="line">[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1</span><br><span class="line"> 0]</span><br><span class="line">Predictions:</span><br><span class="line">[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1</span><br><span class="line"> 0]</span><br></pre></td></tr></table></figure><p>总之，<code>predict_proba</code>和<code>decision_function</code>的形状始终相同，都是<code>(n_samples, n_classes)</code>——除了二分类特殊情况下的<code>decision_function</code>。对于二分类的情况，<code>decision_function</code>只有一列，对应“正”类<code>classes_[1]</code>。这主要是由于历史原因。</p><p>如果有<code>n_classes</code>列，你可以通过计算每一列的argmax来再现预测结果。但如果类别是字符串，或者整数，但不是从0开始的连续整数的话，一定要小心。如果你想要对比predict的结果与<code>decision_function</code>或<code>predict_proba</code>的结果，一定要用分类器的<code>classes_</code>属性来获取真实的属性名称。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用iris数据集的类别名称来表示每一个目标值</span></span><br><span class="line">named_target = iris.target_names[y_train]</span><br><span class="line">logreg.fit(X_train, named_target)</span><br><span class="line">print(<span class="string">'unique classes in training data: &#123;&#125;'</span>.format(logreg.classes_))</span><br><span class="line">print(<span class="string">'predictions: &#123;&#125;'</span>.format(logreg.predict(X_test)[:<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">argmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'argmax of decision function: &#123;&#125;'</span>.format(argmax_dec_func[:<span class="number">10</span>]))</span><br><span class="line">print(<span class="string">'argmax combined with classes_: &#123;&#125;'</span>.format(</span><br><span class="line">    logreg.classes_[argmax_dec_func][:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unique classes in training data: [&apos;setosa&apos; &apos;versicolor&apos; &apos;virginica&apos;]</span><br><span class="line">predictions: [&apos;versicolor&apos; &apos;setosa&apos; &apos;virginica&apos; &apos;versicolor&apos; &apos;versicolor&apos; &apos;setosa&apos;</span><br><span class="line"> &apos;versicolor&apos; &apos;virginica&apos; &apos;versicolor&apos; &apos;versicolor&apos;]</span><br><span class="line">argmax of decision function: [1 0 2 1 1 0 1 2 1 1]</span><br><span class="line">argmax combined with classes_: [&apos;versicolor&apos; &apos;setosa&apos; &apos;virginica&apos; &apos;versicolor&apos; &apos;versicolor&apos; &apos;setosa&apos;</span><br><span class="line"> &apos;versicolor&apos; &apos;virginica&apos; &apos;versicolor&apos; &apos;versicolor&apos;]</span><br></pre></td></tr></table></figure><h1 id="小结">小结</h1><p>欠拟合是指一个模型无法获取训练数据中的所有变化；过拟合是指模型过分关注训练数据，但对新数据的泛化性能不好。</p><p>关于何时使用哪种模型，下面是一份快速总结。</p><ul><li><p>最近邻</p><p>适用于小型数据集，是很好的基准模型，很容易理解。</p></li><li><p>线性模型</p><p>非常可靠的首选算法，适用于非常大的数据集，也适用于高维数据。</p></li><li><p>朴素贝叶斯</p><p>只适用于分类问题。比线性模型速度还快，适用于非常大的数据集和高维数据。精度通常要低于线性模型。</p></li><li><p>决策树</p><p>进度很快，不需要数据缩放，可以可视化，很容易理解。</p></li><li><p>随机森林</p><p>几乎总是比单棵决策树的表现要好，鲁棒性很好，非常强大。不需要数据缩放。不适用于高维稀疏数据。</p></li><li><p>梯度提升决策树</p><p>精度通常比随机森林略高。与随机森林相比，训练速度满，但预测速度更快，需要的内存也更少。比随机森林需要更多的参数调节。</p></li><li><p>支持向量机</p><p>对于特征含义相似的中等大小的数据集很强大。需要数据缩放，对参数敏感。</p></li><li><p>神经网络</p><p>可以构建非常复杂的模型，特别是对于大型数据集而言。对数据缩放敏感，对参数选取敏感。大型网络需要很长的训练时间。</p></li></ul><p>面对新数据集，通常最好先从简单模型开始，比如线性模型、朴素贝叶斯或最近邻分类器，看能得到什么样的结果。对数据有了进一步了解之后，你可以考虑用于构建更复杂模型的算法，比如随机森林、梯度提升决策树、SVM或神经网络。</p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/decision-tree-ensemble/" rel="bookmark">决策树集成</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/decision-trees/" rel="bookmark">决策树</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/k-nearest-neighbors/" rel="bookmark">k近邻</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/kernelized-support-vector-machine/" rel="bookmark">核支持向量机</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/linear-models/" rel="bookmark">线性模型</a></div></li></ul><footer class="post-footer"><div class="post-tags"> <a href="/tags/introduction-to-ml-with-python/" rel="tag"># Python 机器学习基础教程</a> <a href="/tags/supervised/" rel="tag"># 监督</a></div><div class="post-nav"><div class="post-nav-item"><a href="/notes/fluent-python/class-metaprogramming/" rel="prev" title="类元编程"><i class="fa fa-chevron-left"></i> 类元编程</a></div><div class="post-nav-item"> <a href="/notes/introduction-to-ml-with-python/k-nearest-neighbors/" rel="next" title="k近邻">k近邻<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#分类和回归"><span class="nav-number">1.</span> <span class="nav-text">分类和回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#泛化过拟合与欠拟合"><span class="nav-number">2.</span> <span class="nav-text">泛化、过拟合与欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#模型复杂度与数据集大小的关系"><span class="nav-number">2.1.</span> <span class="nav-text">模型复杂度与数据集大小的关系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#监督学习算法"><span class="nav-number">3.</span> <span class="nav-text">监督学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一些样本数据集"><span class="nav-number">3.1.</span> <span class="nav-text">一些样本数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k近邻"><span class="nav-number">3.2.</span> <span class="nav-text">k近邻</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性模型"><span class="nav-number">3.3.</span> <span class="nav-text">线性模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#朴素贝叶苏分类器"><span class="nav-number">3.4.</span> <span class="nav-text">朴素贝叶苏分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树"><span class="nav-number">3.5.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树集成"><span class="nav-number">3.6.</span> <span class="nav-text">决策树集成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#核支持向量机"><span class="nav-number">3.7.</span> <span class="nav-text">核支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络深度学习"><span class="nav-number">3.8.</span> <span class="nav-text">神经网络（深度学习）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分类器的不确定度估计"><span class="nav-number">4.</span> <span class="nav-text">分类器的不确定度估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策函数"><span class="nav-number">4.1.</span> <span class="nav-text">决策函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预测概率"><span class="nav-number">4.2.</span> <span class="nav-text">预测概率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多分类问题的不确定度"><span class="nav-number">4.3.</span> <span class="nav-text">多分类问题的不确定度</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#小结"><span class="nav-number">5.</span> <span class="nav-text">小结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="菜农陈文生" src="/uploads/avatar/nekosensei.png"><p class="site-author-name" itemprop="name">菜农陈文生</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">209</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">11</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">153</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/cncws" title="GitHub → https://github.com/cncws" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a></span><span class="links-of-author-item"><a href="mailto:1031616423@qq.com" title="E-Mail → mailto:1031616423@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">菜农陈文生</span></div><script src="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="//cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script><script src="/js/aplayer-el.js"></script><script src="//unpkg.com/video.js/dist/video.min.js"></script><script src="/js/videojs-bilibili.js"></script><script src="/js/videojs-bilibili-el.js"></script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script src="/js/local-search.js"></script><div id="pjax"><script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Axl15EIRi5o5AatKaxXxV4Oq-gzGzoHsz',
      appKey     : 'E0qm04UjsP0qQN1l8ME3GQ25',
      placeholder: "Just go go",
      avatar     : 'identicon',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script></div></body></html>
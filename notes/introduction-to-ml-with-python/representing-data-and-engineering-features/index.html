<!DOCTYPE html><html lang="zh-CN,en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/uploads/icon/drop/128x128.png"><link rel="icon" type="image/png" sizes="32x32" href="/uploads/icon/drop/32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/uploads/icon/drop/16x16.png"><link rel="mask-icon" href="/uploads/icon/drop/drop.svg" color="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="NKt2iJb3Hnl6-Sm7LB-fTT7LRyi9cg5yZrB-zd0ohtk"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"cwscn.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1,width:240},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!1,mediumzoom:!0,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"fadeIn"}},path:"search.xml"}</script><meta name="description" content="到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的连续特征（continuous feature）。对于许多应用而言，数据的收集方式并不是这样。一种常见的特征类型就是分类特征（categorical feature），也叫离散特征（discrete feature）。分类特征的例子包括产品的品牌、产品的颜色或产品的销售部门。这些都是描述一件产品的属性，但它们不以连续的方"><meta name="keywords" content="Python 机器学习基础教程"><meta property="og:type" content="article"><meta property="og:title" content="数据表示与特征工程"><meta property="og:url" content="https://cwscn.github.io/notes/introduction-to-ml-with-python/representing-data-and-engineering-features/index.html"><meta property="og:site_name" content="春夏秋冬"><meta property="og:description" content="到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的连续特征（continuous feature）。对于许多应用而言，数据的收集方式并不是这样。一种常见的特征类型就是分类特征（categorical feature），也叫离散特征（discrete feature）。分类特征的例子包括产品的品牌、产品的颜色或产品的销售部门。这些都是描述一件产品的属性，但它们不以连续的方"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/difference-between-tree-and-linear.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/linear-and-tree-on-binned-data.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/linear-regression-combined-on-binned-data.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/linear-regression-product-on-binned-data.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/polynomial-linear-regression.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/svr-on-source-data.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/appearances-count-of-poisson.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/appearances-count-of-log-poisson.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/features-selected-by-selectpercentile.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/features-selected-by-selectfrommodel.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/features-selected-by-rfe.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/datasets-citibike.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/random-forest-on-citibike.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/random-forest-on-citibike-hour.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/random-forest-on-citibike-hour-week.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/linear-regression-on-citibike.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/ridge-on-citibike.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/ridge-on-citibike-poly.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/coef-of-linear-regression-on-citibike.png"><meta property="og:updated_time" content="2020-08-02T23:54:34.398Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="数据表示与特征工程"><meta name="twitter:description" content="到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的连续特征（continuous feature）。对于许多应用而言，数据的收集方式并不是这样。一种常见的特征类型就是分类特征（categorical feature），也叫离散特征（discrete feature）。分类特征的例子包括产品的品牌、产品的颜色或产品的销售部门。这些都是描述一件产品的属性，但它们不以连续的方"><meta name="twitter:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/difference-between-tree-and-linear.png"><link rel="canonical" href="https://cwscn.github.io/notes/introduction-to-ml-with-python/representing-data-and-engineering-features/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><link rel="stylesheet" href="//unpkg.com/video.js/dist/video-js.min.css"><link rel="stylesheet" href="/css/videojs-bilibili.css"><style>.aplayer.aplayer-arrow .aplayer-icon-loop,.aplayer.aplayer-arrow .aplayer-icon-order{display:inline-block}</style><title>数据表示与特征工程 | 春夏秋冬</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">春夏秋冬</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">愿你走出半生 归来仍是少年</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档<span class="badge">245</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类<span class="badge">13</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签<span class="badge">161</span></a></li><li class="menu-item menu-item-收藏"><a href="/favlist/" rel="section"><i class="fa fa-star fa-fw"></i> 收藏</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div><meting-js server="netease" type="playlist" id="67155774" theme="#ff5555" loop="all" order="list" preload="none" volume="" mutex="" list-folded="NaN" fixed="true"></meting-js></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://cwscn.github.io/notes/introduction-to-ml-with-python/representing-data-and-engineering-features/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar/nekosensei.png"><meta itemprop="name" content="菜农陈文生"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="春夏秋冬"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 数据表示与特征工程</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-08-03 07:54:34" itemprop="dateModified" datetime="2020-08-03T07:54:34+08:00">2020-08-03</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a></span></span><span id="/notes/introduction-to-ml-with-python/representing-data-and-engineering-features/" class="post-meta-item leancloud_visitors" data-flag-title="数据表示与特征工程" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/notes/introduction-to-ml-with-python/representing-data-and-engineering-features/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/notes/introduction-to-ml-with-python/representing-data-and-engineering-features/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的<strong>连续特征</strong>（continuous feature）。对于许多应用而言，数据的收集方式并不是这样。一种常见的特征类型就是<strong>分类特征</strong>（categorical feature），也叫<strong>离散特征</strong>（discrete feature）。分类特征的例子包括产品的品牌、产品的颜色或产品的销售部门。这些都是描述一件产品的属性，但它们不以连续的方式变化。</p><p>无论数据包含哪种类型的特征，数据表示方式都会对机器学习模型的性能产生巨大影响。数据缩放非常重要。如果你没有缩放数据（比如，缩放到单位方差），那么你用厘米还是英寸表示测量数据的结果将会不同。用额外的特征<strong>扩充</strong>（augment）数据也很有帮助，比如添加特征的交互项（乘积）或更一般的多项式。</p><p>对于某个特定应用来说，如何找到最佳数据表示，这个问题被称为<strong>特征工程</strong>（feature engineering），它是数据科学家和机器学习从业者在尝试解决现实世界问题时主要任务之一。用正确的方式表示数据，对监督模型性能的影响比所选择的精确参数还要大。</p><a id="more"></a><h1 id="分类变量">分类变量</h1><p>作为例子，我们将使用美国成年人收入的数据集。adult数据集的任务是预测一名工人的收入是高于50 000美元还是低于50 000美元。这个数据集的特征包括工人的年龄、雇佣方式、教育水平、性别、每周工作时长、职业，等等。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件中没有包含列名称的表头，因此我们传入header=None</span></span><br><span class="line"><span class="comment"># 然后在"names"中显式地提供名称</span></span><br><span class="line">data = pd.read_csv(os.path.join(mglearn.datasets.DATA_PATH, <span class="string">"adult.data"</span>),</span><br><span class="line">    header=<span class="literal">None</span>, index_col=<span class="literal">False</span>,</span><br><span class="line">    names=[<span class="string">'age'</span>, <span class="string">'workclass'</span>, <span class="string">'fnlwgt'</span>, <span class="string">'education'</span>, <span class="string">'education-num'</span>,</span><br><span class="line">           <span class="string">'marital-status'</span>, <span class="string">'occupation'</span>, <span class="string">'relationship'</span>, <span class="string">'race'</span>, <span class="string">'gender'</span>,</span><br><span class="line">           <span class="string">'capital-gain'</span>, <span class="string">'capital-loss'</span>, <span class="string">'hours-per-week'</span>, <span class="string">'native-country'</span>,</span><br><span class="line">           <span class="string">'income'</span>])</span><br><span class="line"><span class="comment"># 只选其中几列</span></span><br><span class="line">data = data[[<span class="string">'age'</span>, <span class="string">'workclass'</span>, <span class="string">'education'</span>, <span class="string">'gender'</span>, <span class="string">'hours-per-week'</span>,</span><br><span class="line">             <span class="string">'occupation'</span>, <span class="string">'income'</span>]]</span><br><span class="line"></span><br><span class="line">display(data.head())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   age          workclass  ...          occupation  income</span><br><span class="line">0   39          State-gov  ...        Adm-clerical   &lt;=50K</span><br><span class="line">1   50   Self-emp-not-inc  ...     Exec-managerial   &lt;=50K</span><br><span class="line">2   38            Private  ...   Handlers-cleaners   &lt;=50K</span><br><span class="line">3   53            Private  ...   Handlers-cleaners   &lt;=50K</span><br><span class="line">4   28            Private  ...      Prof-specialty   &lt;=50K</span><br><span class="line"></span><br><span class="line">[5 rows x 7 columns]</span><br></pre></td></tr></table></figure><p>在这个数据集中，age（年龄）和hours-per-week（每周工作时长）是连续特征。但workclass（工作类型）、education（教育程度）、gender（性别）、occupation（职业）都是分类特征，它们都来自一系列固定的可能取值（而不是一个范围），表示的是定性属性（而不是数量）。</p><p>假设我们想要在这个数据上学习一个Logistic回归分类器。Logistic回归利用下列公式进行预测：</p><p><span class="math display">\[\hat y=w[0]*x[0]+w[1]*x[1]+\dots+w[p]*x[p]+b&gt;0\]</span></p><p>其中 <span class="math inline">\(w[i]\)</span> 和 <span class="math inline">\(b\)</span> 是从训练集中学到的系数，<span class="math inline">\(x[i]\)</span> 是输入特征。当 <span class="math inline">\(x[i]\)</span> 是数字时这个公式才有意义，但如果 <span class="math inline">\(x[2]\)</span> 是 &quot;Masters&quot; 或 &quot;Bachelors&quot; 的话，这个公式则没有意义。显然，在应用Logistic回归时，我们需要换一种方式来表示数据。</p><h2 id="one-hot编码虚拟变量">One-Hot编码（虚拟变量）</h2><p>表示分类变量最常用的方法就是使用 <strong>one-hot编码</strong>（one-hot-encoding）或 <strong>N取一编码</strong>（one-out-of-N encoding），也叫<strong>虚拟变量</strong>（dummy variable）。虚拟变量背后的思想是将一个分类变量替换为一个或多个新特征，新特征取值为 0 和 1。对于线性二分类（以及 scikit-learn 中其他所有模型）的公式而言，0 和 1 这两个值是有意义的，我们可以对每个类别引入一个新特征，从而表示任意数量的类别。</p><p>比如说 workclass 特征的可能取值包括 &quot;Government Employee&quot;、&quot;Private Employee&quot;、&quot;Self Employed&quot; 和 &quot;Self Employed Incorporated&quot;。为了编码这4个可能的取值，我们创建了4个新特征，分别叫作 &quot;Government Employee&quot;、&quot;Private Employee&quot;、&quot;Self Employed&quot; 和 &quot;Self Employed Incorporated&quot;。如果一个人的 workclass 取某个值，那么对应的特征取值为 1，其他特征均取值为 0。因此，对每个数据点来说，4个新特征中只有一个的取值为 1。这就是它叫作 one-hot 编码或 N取一编码的原因。</p><blockquote><p>我们使用的 one-hot 编码与统计学中使用的虚拟编码（dummy encoding）非常相似，但并不完全相同。在统计学中，通常将具有 k 个可能取值的分类特征编码为 k-1 个特征（都等于零表示最后一个可能取值）。这么做是为了简化分析（更专业的说法是，这可以避免使数据矩阵秩亏）。</p></blockquote><p>将数据转换为分类变量的 one-hot 编码有两种方法：一种是使用 pandas，一种是使用 scikit-learn。</p><h3 id="检查字符串编码的分类数据">检查字符串编码的分类数据</h3><p>读取完数据集之后，最好先检查每一列是否包含有意义的分类数据。在处理人工（比如网站用户）输入的数据时，可能没有固定的类别，拼写和大小写也存在差异，因此可能需要预处理。检查列的内容有一个好方法，就是使用 pandas Series（Series 是 DataFrame 中单列对应的数据类型）的 values_counts 函数，以显示唯一值及其出现次数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(data.gender.value_counts())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> Male      21790</span><br><span class="line"> Female    10771</span><br><span class="line">Name: gender, dtype: int64</span><br></pre></td></tr></table></figure><p>在这个数据集中性别刚好有两个值：Male 和 Female，这说明数据格式已经很好，可以用 one-hot 编码来表示。在实际的应用中，你应该检查所有列的值。</p><p>用 pandas 编码数据有一种非常简单的方法，就是使用 get_dummies 函数。get_dummies 函数自动变换所有具有对象类型（比如字符串）的列或所有分类的列（这是 pandas 中的一个特殊概念，还没有讲到）。</p><blockquote><p><a href="https://devdocs.io/pandas~0.25/reference/api/pandas.get_dummies" target="_blank" rel="noopener"><code>pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)</code></a></p><p>Convert categorical variable into dummy/indicator variables.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Original features:\n"</span>, list(data.columns), <span class="string">"\n"</span>)</span><br><span class="line">data_dummies = pd.get_dummies(data)</span><br><span class="line">print(<span class="string">'Features after get_dummies:\n'</span>, list(data_dummies.columns))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Original features:</span><br><span class="line"> [&apos;age&apos;, &apos;workclass&apos;, &apos;education&apos;, &apos;gender&apos;, &apos;hours-per-week&apos;, &apos;occupation&apos;, &apos;income&apos;]</span><br><span class="line"></span><br><span class="line">Features after get_dummies:</span><br><span class="line"> [&apos;age&apos;, &apos;hours-per-week&apos;, &apos;workclass_ ?&apos;, &apos;workclass_ Federal-gov&apos;, &apos;workclass_ Local-gov&apos;, &apos;workclass_ Never-worked&apos;, &apos;workclass_ Private&apos;, &apos;workclass_ Self-emp-inc&apos;, &apos;workclass_ Self-emp-not-inc&apos;, &apos;workclass_ State-gov&apos;, &apos;workclass_ Without-pay&apos;, &apos;education_ 10th&apos;, &apos;education_ 11th&apos;, &apos;education_ 12th&apos;, &apos;education_ 1st-4th&apos;, &apos;education_ 5th-6th&apos;, &apos;education_ 7th-8th&apos;, &apos;education_ 9th&apos;, &apos;education_ Assoc-acdm&apos;, &apos;education_ Assoc-voc&apos;, &apos;education_ Bachelors&apos;, &apos;education_ Doctorate&apos;, &apos;education_ HS-grad&apos;, &apos;education_ Masters&apos;, &apos;education_ Preschool&apos;, &apos;education_ Prof-school&apos;, &apos;education_ Some-college&apos;, &apos;gender_ Female&apos;, &apos;gender_ Male&apos;, &apos;occupation_ ?&apos;, &apos;occupation_ Adm-clerical&apos;, &apos;occupation_ Armed-Forces&apos;, &apos;occupation_ Craft-repair&apos;, &apos;occupation_ Exec-managerial&apos;, &apos;occupation_ Farming-fishing&apos;, &apos;occupation_ Handlers-cleaners&apos;, &apos;occupation_ Machine-op-inspct&apos;, &apos;occupation_ Other-service&apos;, &apos;occupation_ Priv-house-serv&apos;, &apos;occupation_ Prof-specialty&apos;, &apos;occupation_ Protective-serv&apos;, &apos;occupation_ Sales&apos;, &apos;occupation_ Tech-support&apos;, &apos;occupation_ Transport-moving&apos;, &apos;income_ &lt;=50K&apos;, &apos;income_ &gt;50K&apos;]</span><br></pre></td></tr></table></figure><p>可以看到，连续特征 age 和 hours-per-week 没有发生变化，而分类特征的每个可能取值都被扩展为一个新特征。</p><p>下面我们可以使用 values 属性将 data_dummies 数据框（DataFrame）转换为 NumPy 数组，然后在其上训练一个机器学习模型。在训练模型之前，注意要把目标变量（现在被编码为两个 income 列）从数据中分离出来。</p><blockquote><p>注意：andas 中的列索引包括范围的结尾，因此 'age':'occupation_ Transport-moving' 中包括 occupation_ Transport-moving。这与 NumPy数组的切片不同，后者不包括范围的末尾，例如 <code>np.arange(11)[0:10]</code>不包括索引编号为 10 的元素。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features = data_dummies.ix[:, <span class="string">'age'</span>:<span class="string">'occupation_ Transport-moving'</span>]</span><br><span class="line"><span class="comment"># 提取NumPy数组</span></span><br><span class="line">X = features.values</span><br><span class="line">y = data_dummies[<span class="string">'income_ &gt;50K'</span>].values</span><br><span class="line">print(<span class="string">'X.shape: &#123;&#125;  y.shape: &#123;&#125;'</span>.format(X.shape, y.shape))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X.shape: (32561, 44)  y.shape: (32561,)</span><br></pre></td></tr></table></figure><p>现在数据的表示方式可以被 scikit-learn 处理。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line">logreg.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">'Test score: &#123;:.2f&#125;'</span>.format(logreg.score(X_test, y_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test score: 0.81</span><br></pre></td></tr></table></figure><p>在这个例子中，我们对同时包含训练数据和测试数据的数据框调用 get_dummies。这一点很重要，可以确保训练集和测试集中分类变量的表示方式相同。</p><p>假设我们的训练集和测试集位于两个不同的数据框中。如果 workclass 特征的 &quot;Private Employee&quot; 取值没有出现在测试集中，那么 pandas 会认为这个特征只有3个可能的取值，因此只会创建 3 个新的虚拟特征。现在训练集和测试集的特征个数不相同，我们就无法将在训练集上学到的模型应用到测试集上。</p><p>更糟糕的是，假设 workclass 特征在训练集中有 &quot;Government Employee&quot; 和 &quot;Private Employee&quot; 两个值，而在测试集中有 &quot;Self Employed&quot; 和 &quot;Self Employed Incorporated&quot; 两个值。在两种情况下，pandas 都会创建两个新的虚拟特征，所有编码后的数据框的特征个数相同。但在训练集和测试集中的两个虚拟特征含义完全不同。如果我们在这个数据上构建机器学习模型，那么它的表现会很差。要想避免这个问题，可以在同时包含训练数据点和测试数据点的数据框上调用 get_dummies，确保调用 get_dummies后训练集和测试集的列名称相同，保证它们具有相同的语义。</p><h2 id="数字可以编码分类变量">数字可以编码分类变量</h2><p>在 adult 数据集的例子中，分类变量被编码为字符串。一方面，可能会有拼写错误；但另一方面，它明确地将一个变量标记为分类变量。（然而）无论是为了便于存储还是因为数据的收集方式，分类变量通常被编码为整数。例如，假设 adult 数据集中的人口普查数据是利用问卷收集的，workclass 的回答被记录为 0（在第一个框打勾）、1、2，等等。现在该列包含数字 0 到 8，而不是像 &quot;Private&quot; 这样的字符串。如果有人观察表示数据集的表格，很难一眼看出这个变量应该被视为连续变量还是分类变量。但是，如果知道这些数字表示的是就业状况，那么很明显它们是不同的状态，不应该用单个连续变量来建模。</p><blockquote><p>分类特征通常用整数进行编码。它们是数字并不意味着它们必须被视为连续特征。一个整数特征应该被视为连续的还是离散的，有时并不明确。如果在被编码的语义之间没有顺序关系（比如 workclass 的例子），那么特征必须被视为离散特征。对于其他情况（比如五星评分），哪种编码更好取决于具体的任务和数据，以及使用哪种机器学习算法。</p></blockquote><p>pandas的 get_dummies 函数将所有数字看作是连续的，不会为其创建虚拟变量。为了解决这个问题，你可以使用 scikit-learn 的 OneHotEncoder，指定哪些变量是连续的、哪些变量是离散的，你也可以将数据框中的数值列转换为字符串。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个DataFrame，包含一个整数特征和一个分类字符串特征</span></span><br><span class="line">demo_df = pd.DataFrame(&#123;<span class="string">'Integer Feature'</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        <span class="string">'Categorical Feature'</span>: [<span class="string">'socks'</span>, <span class="string">'fox'</span>, <span class="string">'socks'</span>, <span class="string">'box'</span>]&#125;)</span><br><span class="line">display(demo_df)</span><br><span class="line"><span class="comment"># 使用 get_dummies 只会编码字符串特征，不会改变整数特征</span></span><br><span class="line">display(pd.get_dummies(demo_df))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   Integer Feature Categorical Feature</span><br><span class="line">0                0               socks</span><br><span class="line">1                1                 fox</span><br><span class="line">2                2               socks</span><br><span class="line">3                1                 box</span><br><span class="line">   Integer Feature  ...  Categorical Feature_socks</span><br><span class="line">0                0  ...                          1</span><br><span class="line">1                1  ...                          0</span><br><span class="line">2                2  ...                          1</span><br><span class="line">3                1  ...                          0</span><br><span class="line"></span><br><span class="line">[4 rows x 4 columns]</span><br></pre></td></tr></table></figure><p>如果你想为 &quot;Integer Feature&quot; 这一列创建虚拟变量，可以使用 columns 参数显示地给出想要编码的列。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">demo_df[<span class="string">'Integer Feature'</span>] = demo_df[<span class="string">'Integer Feature'</span>].astype(str)</span><br><span class="line">display(pd.get_dummies(demo_df, columns=[<span class="string">'Integer Feature'</span>, <span class="string">'Categorical Feature'</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   Integer Feature_0  ...  Categorical Feature_socks</span><br><span class="line">0                  1  ...                          1</span><br><span class="line">1                  0  ...                          0</span><br><span class="line">2                  0  ...                          1</span><br><span class="line">3                  0  ...                          0</span><br><span class="line"></span><br><span class="line">[4 rows x 6 columns]</span><br></pre></td></tr></table></figure><h1 id="分箱离散化线性模型与树">分箱、离散化、线性模型与树</h1><p>数据表示的最佳方法不仅取决于数据的语义，还取决于所使用的模型种类。线性模型与基于树的模型（比如决策树、梯度提升树和随机森林）是两种成员很多同时又非常常用的模型，它们在处理不同的特征表示时就具有非常不同的性质。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">100</span>)</span><br><span class="line">line = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>, endpoint=<span class="literal">False</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">reg = DecisionTreeRegressor(min_samples_split=<span class="number">3</span>).fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">"decision tree"</span>)</span><br><span class="line"></span><br><span class="line">reg = LinearRegression().fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">"linear regression"</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Regression output'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Input feature'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/difference-between-tree-and-linear.png"></p><p>线性模型只能对线性关系建模，对于单个特征的情况就是直线。决策树可以构建更为复杂的数据模型，但这强烈依赖于数据表示。有一种方法可以让线性模型在连续数据上变得更加强大，就是使用特征<strong>分箱</strong>（binning，也叫<strong>离散化</strong>，即 discretization）将其划分为多个特征。</p><p>我们假设将特征的输入范围划分成固定个数的<strong>箱子</strong>（bin），比如 10 个，那么数据点就可以用它所在的箱子来表示。为了确定这一点，我们首先需要定义箱子。在这个例子中，我们在 -3 和 3 之间定义 10 个均匀分布的箱子。我们用 np.linspace 函数创建 11 个元素，从而创建 10 个箱子，即两个连续边界之间的空间。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bins = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">11</span>)</span><br><span class="line">print(<span class="string">'bins: &#123;&#125;'</span>.format(bins))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bins: [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]</span><br></pre></td></tr></table></figure><p>接下来，我们记录每个数据点所属的箱子。这可以用 np.digitize 函数轻松计算出来。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">which_bin = np.digitize(X, bins=bins)</span><br><span class="line">print(<span class="string">'Data points:\n'</span>, X[:<span class="number">5</span>])</span><br><span class="line">print(<span class="string">'\nBin membership for data points:\n'</span>, which_bin[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Data points:</span><br><span class="line"> [[-0.75275929]</span><br><span class="line"> [ 2.70428584]</span><br><span class="line"> [ 1.39196365]</span><br><span class="line"> [ 0.59195091]</span><br><span class="line"> [-2.06388816]]</span><br><span class="line"></span><br><span class="line">Bin membership for data points:</span><br><span class="line"> [[ 4]</span><br><span class="line"> [10]</span><br><span class="line"> [ 8]</span><br><span class="line"> [ 6]</span><br><span class="line"> [ 2]]</span><br></pre></td></tr></table></figure><p>我们在这里做的是将 wave 数据集中单个连续输入特征变换为一个分类特征，用于表示数据点所在的箱子。要想在这个数据上使用 scikit-learn 模型，我们利用 preprocessing 模块的 OneHotEncoder 将这个离散特征变换为 one-hot 编码。 OneHotEncoder 实现的编码与 pandas.get_dummies 相同，但目前它只适用于值为整数的分类变量。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder" target="_blank" rel="noopener"><code>class sklearn.preprocessing.OneHotEncoder(n_values=None, categorical_features=None, categories=None, sparse=True, dtype=&lt;class ‘numpy.float64’&gt;, handle_unknown=’error’)</code></a></p><p>Encode categorical integer features as a one-hot numeric array.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用OneHotEncoder进行变换</span></span><br><span class="line">encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line">encoder.fit(which_bin)</span><br><span class="line">X_binned = encoder.transform(which_bin)</span><br><span class="line">print(X_binned[:<span class="number">5</span>])</span><br><span class="line"><span class="comment"># 由于我们指定了 10 个箱子，所以变换后的 X_binned 数据集现在包含 10 个特征</span></span><br><span class="line">print(<span class="string">'X_binned.shape: &#123;&#125;'</span>.format(X_binned.shape))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</span><br><span class="line"> [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</span><br><span class="line"> [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]</span><br><span class="line">X_binned.shape: (100, 10)</span><br></pre></td></tr></table></figure><p>下面我们在 one-hot 编码后的数据上构建新的线性模型和新的决策树模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">line_binned = encoder.transform(np.digitize(line, bins=bins))</span><br><span class="line"></span><br><span class="line">reg = LinearRegression().fit(X_binned, y)</span><br><span class="line">plt.plot(line, reg.predict(line_binned), label=<span class="string">'linear regression binned'</span>)</span><br><span class="line"></span><br><span class="line">reg = DecisionTreeRegressor(min_samples_split=<span class="number">3</span>).fit(X_binned, y)</span><br><span class="line">plt.plot(line, reg.predict(line_binned), label=<span class="string">'decision tree binned'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.vlines(bins, <span class="number">-3</span>, <span class="number">3</span>, linewidth=<span class="number">1</span>, alpha=<span class="number">.2</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Regresson output'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Input feature'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/linear-and-tree-on-binned-data.png"></p><p>虚线和实线完全重合，说明线性回归模型和决策树模型做出了完全相同的预测。对于每个箱子，二者都预测一个常数值。因为每个箱子内的特征是不变的，所以对于一个箱子内的所有点，任何模型都会预测相同的值。</p><p>比较对特征进行分箱前后模型学到的内容，我们发现，线性模型变得更加灵活了，因为现在它对每个箱子具有不同的取值，而决策树模型的灵活性降低了。分箱特征对基于树的模型通常不会产生更好的效果，因为这种模型可以学习在任何位置划分数据。从某种意义上来看，决策树可以学习如何分箱对预测这些数据最为有用。此外，决策树可以同时查看多个特征，而分箱通常针对的是单个特征。不过，线性模型的表现力在数据变换后得到了极大的提高。</p><p><strong>对于特定的数据集，如果有充分的理由使用线性模型——比如数据集很大、维度很高，但有些特征与输出的关系是非线性的——那么分箱是提高建模能力的好方法。</strong></p><h1 id="交互特征与多项式特征">交互特征与多项式特征</h1><p>想要丰富特征表示，特别是对于线性模型而言，另一种方法是添加原始数据的<strong>交互特征</strong>（interaction feature）和<strong>多项式特征</strong>（polynomial feature）。这种特征工程通常用于统计建模，但也常用于许多实际的机器学习应用中。</p><p>线性模型对 wave 数据集中的每个箱子都学到一个常数值。但我们知道，线性模型不仅可以学习偏移，还可以学习斜率。想要向分箱数据上的线性模型添加斜率，一种方法是重新加入原始特征。这样会得到 11 维的数据集。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_combined = np.hstack([X, X_binned])</span><br><span class="line">print(<span class="string">'X_combined.shape: &#123;&#125;'</span>.format(X_combined.shape))</span><br><span class="line"></span><br><span class="line">line_combined = np.hstack([line, line_binned])</span><br><span class="line">reg = LinearRegression().fit(X_combined, y)</span><br><span class="line">plt.plot(line, reg.predict(line_combined), label=<span class="string">'linear regression combined'</span>)</span><br><span class="line">plt.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> bin <span class="keyword">in</span> bins:</span><br><span class="line">    plt.plot([bin, bin], [<span class="number">-3</span>, <span class="number">3</span>], <span class="string">':'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Regresson output'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Input feature'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X_combined.shape: (100, 11)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/linear-regression-combined-on-binned-data.png"></p><p>在这个例子中，模型在每个箱子中都学到一个偏移，还学到一个斜率。学到的斜率在所有箱子中都相同——只有一个 x 轴特征，也就只有一个斜率。因为斜率在所有箱子中是相同的，所以它似乎不是很有用。我们更希望每个箱子都有一个不同的斜率。为了实现这一点，我们可以添加交互特征或乘积特征，用来表示数据点所在箱子以及数据点在 x 轴上的位置。这个特征是箱子指示符与原始特征的乘积。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_product = np.hstack([X_binned, X * X_binned])</span><br><span class="line">print(<span class="string">'X_product.shape: &#123;&#125;'</span>.format(X_product.shape))</span><br><span class="line"></span><br><span class="line">line_product = np.hstack([line_binned, line * line_binned])</span><br><span class="line"></span><br><span class="line">reg = LinearRegression().fit(X_product, y)</span><br><span class="line">plt.plot(line, reg.predict(line_product), label=<span class="string">'linear regression product'</span>)</span><br><span class="line">plt.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> bin <span class="keyword">in</span> bins:</span><br><span class="line">    plt.plot([bin, bin], [<span class="number">-3</span>, <span class="number">3</span>], <span class="string">':'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Regresson output'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Input feature'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X_product.shape: (100, 20)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/linear-regression-product-on-binned-data.png"></p><p>这个数据集有20个特征：数据点所在箱子的指示符（10 个特征）与原始特征和箱子指示符的乘积（10 个特征）。你可以将乘积特征看作每个箱子 x 轴特征的单独副本。它在箱子内等于原始特征，在其他位置等于零。现在这个模型中，每个箱子都有自己的偏移和斜率。</p><p>使用分箱是扩展连续特征的一种方法。另一种方法是使用原始特征的<strong>多项式</strong>（polynomial）。对于给定特征 x，我们可以考虑 x ** 2、x ** 3、x ** 4，等等。这在 preprocessing 模块的 PolynomialFeatures 中实现。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures" target="_blank" rel="noopener"><code>class sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)</code></a></p><p>Generate polynomial and interaction features.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 包含直到x ** 10的多项式</span></span><br><span class="line"><span class="comment"># 默认的"include_bias=True"添加恒等于1的常数特征</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">poly.fit(X)</span><br><span class="line">X_poly = poly.transform(X)</span><br><span class="line">print(<span class="string">"X_poly.shape: &#123;&#125;"</span>.format(X_poly.shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较X_poly和X的元素</span></span><br><span class="line">print(<span class="string">'Entries of X:\n'</span>, X[:<span class="number">5</span>])</span><br><span class="line">print(<span class="string">'Entries of X_poly:\n'</span>, X_poly[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以通过调用get_feature_names方法来获取特征的语义，给出每个特征的指数</span></span><br><span class="line">print(<span class="string">'Polynomial feature names:\n'</span>, poly.get_feature_names())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X_poly.shape: (100, 10)</span><br><span class="line">Entries of X:</span><br><span class="line"> [[-0.75275929]</span><br><span class="line"> [ 2.70428584]</span><br><span class="line"> [ 1.39196365]</span><br><span class="line"> [ 0.59195091]</span><br><span class="line"> [-2.06388816]]</span><br><span class="line">Entries of X_poly:</span><br><span class="line"> [[-7.52759287e-01  5.66646544e-01 -4.26548448e-01  3.21088306e-01</span><br><span class="line">  -2.41702204e-01  1.81943579e-01 -1.36959719e-01  1.03097700e-01</span><br><span class="line">  -7.76077513e-02  5.84199555e-02]</span><br><span class="line"> [ 2.70428584e+00  7.31316190e+00  1.97768801e+01  5.34823369e+01</span><br><span class="line">   1.44631526e+02  3.91124988e+02  1.05771377e+03  2.86036036e+03</span><br><span class="line">   7.73523202e+03  2.09182784e+04]</span><br><span class="line"> [ 1.39196365e+00  1.93756281e+00  2.69701700e+00  3.75414962e+00</span><br><span class="line">   5.22563982e+00  7.27390068e+00  1.01250053e+01  1.40936394e+01</span><br><span class="line">   1.96178338e+01  2.73073115e+01]</span><br><span class="line"> [ 5.91950905e-01  3.50405874e-01  2.07423074e-01  1.22784277e-01</span><br><span class="line">   7.26822637e-02  4.30243318e-02  2.54682921e-02  1.50759786e-02</span><br><span class="line">   8.92423917e-03  5.28271146e-03]</span><br><span class="line"> [-2.06388816e+00  4.25963433e+00 -8.79140884e+00  1.81444846e+01</span><br><span class="line">  -3.74481869e+01  7.72888694e+01 -1.59515582e+02  3.29222321e+02</span><br><span class="line">  -6.79478050e+02  1.40236670e+03]]</span><br><span class="line">Polynomial feature names:</span><br><span class="line"> [&apos;x0&apos;, &apos;x0^2&apos;, &apos;x0^3&apos;, &apos;x0^4&apos;, &apos;x0^5&apos;, &apos;x0^6&apos;, &apos;x0^7&apos;, &apos;x0^8&apos;, &apos;x0^9&apos;, &apos;x0^10&apos;]</span><br></pre></td></tr></table></figure><p>X_poly 的第一列与 X 完全对应，而其他列则是第一列的幂。将多项式特征与线性回归模型一起使用，可以得到经典的<strong>多项式回归</strong>（polynomial regression）模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">line_poly = poly.transform(line)</span><br><span class="line"></span><br><span class="line">reg = LinearRegression().fit(X_poly, y)</span><br><span class="line">plt.plot(line, reg.predict(line_poly), label=<span class="string">'polynomial linear regression'</span>)</span><br><span class="line">plt.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Regression output'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Input feature'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/polynomial-linear-regression.png"></p><p>多项式特征在这个一维数据上得到了非常平滑的拟合。但高次多项式在边界上或数据很少的区域可能有极端的表现。作为对比，下面是在原始数据上学到的核 SVM 模型，没有做任何变换。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gamma <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">10</span>]:</span><br><span class="line">    svr = SVR(gamma=gamma).fit(X, y)</span><br><span class="line">    plt.plot(line, svr.predict(line), label=<span class="string">'SVR gamma=&#123;&#125;'</span>.format(gamma))</span><br><span class="line"></span><br><span class="line">plt.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Regression output'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Input feature'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/svr-on-source-data.png"></p><p>使用更复杂的模型（即核 SVM），我们能够学到一个与多项式回归的复杂度类似的预测结果，且不需要进行显示的特征变换。</p><p>我们再次观察波士顿房价数据集，作为对交互特征和多项式特征更加实际的应用。现在来看一下这些特征的构造方式，以及多项式特征的帮助有多大。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">boston = load_boston()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    boston.data, boston.target, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 缩放数据</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scaled = scaler.fit_transform(X_test)</span><br><span class="line"><span class="comment"># 提取多项式特征和交互特征，次数最高为 2</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>).fit(X_train_scaled)</span><br><span class="line">X_train_poly = poly.transform(X_train_scaled)</span><br><span class="line">X_test_poly = poly.transform(X_test_scaled)</span><br><span class="line">print(<span class="string">'X_train.shape: &#123;&#125;'</span>.format(X_train.shape))</span><br><span class="line">print(<span class="string">'X_train_poly.shape: &#123;&#125;'</span>.format(X_train_poly.shape))</span><br><span class="line"><span class="comment"># 利用get_feature_names得到输入特征和输出特征之间的确切关系</span></span><br><span class="line">print(<span class="string">'Polynomial feature names:\n'</span>, poly.get_feature_names())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X_train.shape: (379, 13)</span><br><span class="line">X_train_poly.shape: (379, 105)</span><br><span class="line">Polynomial feature names:</span><br><span class="line"> [&apos;1&apos;, &apos;x0&apos;, &apos;x1&apos;, &apos;x2&apos;, &apos;x3&apos;, &apos;x4&apos;, &apos;x5&apos;, &apos;x6&apos;, &apos;x7&apos;, &apos;x8&apos;, &apos;x9&apos;, &apos;x10&apos;, &apos;x11&apos;, &apos;x12&apos;, &apos;x0^2&apos;, &apos;x0 x1&apos;, &apos;x0 x2&apos;, &apos;x0 x3&apos;, &apos;x0 x4&apos;, &apos;x0 x5&apos;, &apos;x0 x6&apos;, &apos;x0 x7&apos;, &apos;x0 x8&apos;, &apos;x0 x9&apos;, &apos;x0 x10&apos;, &apos;x0 x11&apos;, &apos;x0 x12&apos;, &apos;x1^2&apos;, &apos;x1 x2&apos;, &apos;x1 x3&apos;, &apos;x1 x4&apos;, &apos;x1 x5&apos;, &apos;x1 x6&apos;, &apos;x1 x7&apos;, &apos;x1 x8&apos;, &apos;x1 x9&apos;, &apos;x1 x10&apos;, &apos;x1 x11&apos;, &apos;x1 x12&apos;, &apos;x2^2&apos;, &apos;x2 x3&apos;, &apos;x2 x4&apos;, &apos;x2 x5&apos;, &apos;x2 x6&apos;, &apos;x2 x7&apos;, &apos;x2 x8&apos;, &apos;x2 x9&apos;, &apos;x2 x10&apos;, &apos;x2 x11&apos;, &apos;x2 x12&apos;, &apos;x3^2&apos;, &apos;x3 x4&apos;, &apos;x3 x5&apos;, &apos;x3 x6&apos;, &apos;x3 x7&apos;, &apos;x3 x8&apos;, &apos;x3 x9&apos;, &apos;x3 x10&apos;, &apos;x3 x11&apos;, &apos;x3 x12&apos;, &apos;x4^2&apos;, &apos;x4 x5&apos;, &apos;x4 x6&apos;, &apos;x4 x7&apos;, &apos;x4 x8&apos;, &apos;x4 x9&apos;, &apos;x4 x10&apos;, &apos;x4 x11&apos;, &apos;x4 x12&apos;, &apos;x5^2&apos;, &apos;x5 x6&apos;, &apos;x5 x7&apos;, &apos;x5 x8&apos;, &apos;x5 x9&apos;, &apos;x5 x10&apos;, &apos;x5 x11&apos;, &apos;x5 x12&apos;, &apos;x6^2&apos;, &apos;x6 x7&apos;, &apos;x6 x8&apos;, &apos;x6 x9&apos;, &apos;x6 x10&apos;, &apos;x6 x11&apos;, &apos;x6 x12&apos;, &apos;x7^2&apos;, &apos;x7 x8&apos;, &apos;x7 x9&apos;, &apos;x7 x10&apos;, &apos;x7 x11&apos;, &apos;x7 x12&apos;, &apos;x8^2&apos;, &apos;x8 x9&apos;, &apos;x8 x10&apos;, &apos;x8 x11&apos;, &apos;x8 x12&apos;, &apos;x9^2&apos;, &apos;x9 x10&apos;, &apos;x9 x11&apos;, &apos;x9 x12&apos;, &apos;x10^2&apos;, &apos;x10 x11&apos;, &apos;x10 x12&apos;, &apos;x11^2&apos;, &apos;x11 x12&apos;, &apos;x12^2&apos;]</span><br></pre></td></tr></table></figure><p>原始数据有 13 个特征，现在被扩展到 105 个交互特征。这些新特征表示两个不同的原始特征之间所有可能的交互项，以及每个原始特征的平方。第一个新特征是常数特征，这里的名称是 &quot;1&quot;。接下来的 13 个特征是原始特征（&quot;x0&quot; 到 &quot;x12&quot;）。然后是第一个特征的平方（&quot;x0^2&quot;）以及它与其他特征的组合。</p><p>我们对 Ridge 在有交互特征的数据集上和没有交互特征的数据集上的性能进行比较。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ridge = Ridge().fit(X_train_scaled, y_train)</span><br><span class="line">print(<span class="string">'Score without interactions: &#123;:.3f&#125;'</span>.format(ridge.score(X_test_scaled, y_test)))</span><br><span class="line">ridge = Ridge().fit(X_train_poly, y_train)</span><br><span class="line">print(<span class="string">'Score with interactions: &#123;:.3f&#125;'</span>.format(ridge.score(X_test_poly, y_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Score without interactions: 0.577</span><br><span class="line">Score with interactions: 0.741</span><br></pre></td></tr></table></figure><p>显然，在使用 Ridge 时，交互特征和多项式特征对性能有很大的提升。但如果使用更加复杂的模型（比如随机森林），情况会稍有不同。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">100</span>).fit(X_train_scaled, y_train)</span><br><span class="line">print(<span class="string">'Score without interactions: &#123;:.3f&#125;'</span>.format(rf.score(X_test_scaled, y_test)))</span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">100</span>).fit(X_train_poly, y_train)</span><br><span class="line">print(<span class="string">'Score with interactions: &#123;:.3f&#125;'</span>.format(rf.score(X_test_poly, y_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Score without interactions: 0.780</span><br><span class="line">Score with interactions: 0.755</span><br></pre></td></tr></table></figure><p>你可以看到，即使没有额外的特征，随机森林的性能也要优于 Ridge。添加交互特征和多项式特征实际上会略微降低其性能。</p><h1 id="单变量非线性变换">单变量非线性变换</h1><p>我们刚刚看到，<strong>添加特征的平方或立方可以改进线性回归模型</strong>。其他变换通常也对变换某些特征有用，特别是应用数学函数，比如 log、exp 或 sin。虽然基于树的模型只关注特征的顺序，但线性模型和神经网络依赖于每个特征的尺度和分布。如果在特征和目标之间存在非线性关系，那么建模就变得非常困难，特别是对于回归问题。log 和 exp 函数可以帮助调节数据的相对比例，从而改进线性模型或神经网络的学习效果。我们在第 2 章中对内存价格数据应用过这种函数。在处理具有周期性模式的数据时，sin 和 cos 函数非常有用。</p><p>大部分模型都在每个特征（在回归问题中还包括目标值）大致遵循高斯分布时表现最好，也就是说，每个特征的直方图应该具有类似于“钟形曲线”的形状。使用诸如 log 和 exp 之类的变换并不稀奇，却却是实现这一点的简单又有效的方法。在一种特别常见的情况下，这样的变换非常有用，就是处理整数计数数据时。计数数据是指类似“用户A多长时间登陆一次？”这样的特征。计数不可能取负值，并且通常遵循特定的统计模式。特征全都是整数值，而响应是连续的。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rnd = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line">X_org = rnd.normal(size=(<span class="number">1000</span>, <span class="number">3</span>))</span><br><span class="line">w = rnd.normal(size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">X = rnd.poisson(<span class="number">10</span> * np.exp(X_org))</span><br><span class="line">y = np.dot(X_org, w)</span><br><span class="line"></span><br><span class="line">print(X[:<span class="number">10</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ 56  81  25  20  27  18  12  21 109   7]</span><br></pre></td></tr></table></figure><p>第一个特征的前 10 个元素都是正整数，但除此之外很难找出特定的模式。如果我们计算每个值的出现次数，那么数值的分布将变得更清楚。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Number of feature appearances:\n'</span>, np.bincount(X[:, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">bins = np.bincount(X[:, <span class="number">0</span>])</span><br><span class="line">plt.bar(range(len(bins)), bins, color=<span class="string">'gray'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Number of appearances'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Value'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Number of feature appearances:</span><br><span class="line"> [28 38 68 48 61 59 45 56 37 40 35 34 36 26 23 26 27 21 23 23 18 21 10  9</span><br><span class="line"> 17  9  7 14 12  7  3  8  4  5  5  3  4  2  4  1  1  3  2  5  3  8  2  5</span><br><span class="line">  2  1  2  3  3  2  2  3  3  0  1  2  1  0  0  3  1  0  0  0  1  3  0  1</span><br><span class="line">  0  2  0  1  1  0  0  0  0  1  0  0  2  2  0  1  1  0  0  0  0  1  1  0</span><br><span class="line">  0  0  0  0  0  0  1  0  0  0  0  0  1  1  0  0  1  0  0  0  0  0  0  0</span><br><span class="line">  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/appearances-count-of-poisson.png"></p><p>特征 <code>X[:, 1]</code> 和 <code>X[:, 2]</code> 具有类似的性质。这种类型的数值分布（泊松分布）在实践中非常常见。但多数线性模型无法很好地处理这种数据。我们尝试拟合一个岭回归模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">score = Ridge().fit(X_train, y_train).score(X_test, y_test)</span><br><span class="line">print(<span class="string">'Test score: &#123;:.3f&#125;'</span>.format(score))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test score: 0.622</span><br></pre></td></tr></table></figure><p>从相对较小的分数中可以看出，Ridge 无法真正捕捉到 X 和 y 之间的关系。不过应用对数变换可能有用。由于数据取值中包括 0（对数在 0 处没有定义），所以我们不能直接应用 log，而是要计算 log(X + 1)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_log = np.log(X_train + <span class="number">1</span>)</span><br><span class="line">X_test_log = np.log(X_test + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.hist(X_train_log[:, <span class="number">0</span>], bins=<span class="number">25</span>, color=<span class="string">'gray'</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Number of apperances"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Value"</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/appearances-count-of-log-poisson.png"></p><p>在新数据上构建一个岭回归模型，可以得到更好地拟合。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">score = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)</span><br><span class="line">print(<span class="string">'Test score: &#123;:.3f&#125;'</span>.format(score))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test score: 0.875</span><br></pre></td></tr></table></figure><p>在这个例子中，所有特征都具有相同的性质，这在实践中是非常少见的情况。通常来说，只有一部分特征应该进行变换，有时每个特征的变换方式也各不相同。前面提到过，<strong>对基于树的模型而言，这种变换并不重要，但对线性模型来说可能至关重要。</strong>最回归的目标变量 y 进行变换有时也是一个好主意。尝试预计计数（比如订单数量）是一项相当常见的任务，而且使用 log(y + 1) 变换也往往有用。</p><p>分箱、多项式和交互项都对模型在给定数据集上的性能有很大影响，对于复杂度低的模型更是这样，比如线性模型和朴素贝叶斯模型。与之相反，基于树的模型通常能够自己发现重要的交互项，大多数情况下不需要显示地变换数据。其他模型，比如 SVM、最近邻和神经网络，有时可能会从分箱、交互项或多项式中受益，但其效果通常不如线性模型那么明显。</p><h1 id="自动化特征选择">自动化特征选择</h1><p>有了这么多种创建新特征的方法，你可能会想要增大数据的维度，使其远大于原始特征的数量。但是，添加更多特征会使所有模型变得更加复杂，从而增大过拟合的可能性。在添加新特征或处理一般的高纬度数据集时，最好将特征的数量减少到只包含最有用的那些特征，并删除其余特征。这样会得到泛化能力更好、更简单的模型。但你如何判断每个特征的作用有多大呢？有三种基本的策略：<strong>单变量统计</strong>（univariate statistics）、<strong>基于模型的选择</strong>（model-based selection）和<strong>迭代选择</strong>（iterative selection）。所有这些方法都是监督方法，即它们需要目标值来拟合模型。</p><h2 id="单变量统计">单变量统计</h2><p>在单变量统计中，我们计算每个特征和目标值之间的关系是否存在统计显著性，然后选择具有最高置信度的特征。对于分类问题，这也被称为<strong>方差分析</strong>（analysis of variance，ANOVA）。这些测试的一个关键性质就是它们是<strong>单变量的</strong>（univariate），即它们只单独考虑每个特征。因此，如果一个特征只有在与另一个特征合并时才具有信息量，那么这个特征将被舍弃。单变量测试的计算速度通常很快，并且不需要构建模型。另一方面，它们完全独立于你可能想要在特征选择之后应用的模型。</p><p>想要在 scikit-learn 中使用单变量特征选择，你需要选择一项测试——对分类问题通常是 f_classif（默认值），对回归问题通常是 f_regression——然后基于测试中确定的 p 值来选择一种舍弃特征的方法。所有舍弃参数的方法都使用阈值来舍弃所有 p 值过大的特征（意味着它们不可能与目标值相关）。计算阈值的方法各有不同，最简单的是 SelectKBest 和 SelectPercentile，前者选择固定数量的 k 个特征，后者选择固定百分比的特征。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest" target="_blank" rel="noopener"><code>class sklearn.feature_selection.SelectKBest(score_func=&lt;function f_classif&gt;, k=10)</code></a></p><p>Select features according to the k highest scores.</p><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile" target="_blank" rel="noopener"><code>class sklearn.feature_selection.SelectPercentile(score_func=&lt;function f_classif&gt;, percentile=10)</code></a></p><p>Select features according to a percentile of the highest scores.</p></blockquote><p>我们将分类的特征选择应用于 cancer 数据集。为了使任务更难一点，我们将向数据中添加一些没有信息量的噪声特征。我们期望特征选择能够识别没有信息量的特征并删除它们。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得确定性的随机数</span></span><br><span class="line">rng = np.random.RandomState(<span class="number">42</span>)</span><br><span class="line">noise = rng.normal(size=(len(cancer.data), <span class="number">50</span>))</span><br><span class="line"><span class="comment"># 向数据中添加噪声特征</span></span><br><span class="line"><span class="comment"># 前30个特征来自数据集，后50个是噪声</span></span><br><span class="line">X_w_noise = np.hstack([cancer.data, noise])</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_w_noise, cancer.target, random_state=<span class="number">0</span>, test_size=<span class="number">.5</span>)</span><br><span class="line"><span class="comment"># 使用f_classif（默认值）和SelectPercentile来选择50%的特征</span></span><br><span class="line">select = SelectPercentile(percentile=<span class="number">50</span>)</span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 对训练集进行变换</span></span><br><span class="line">X_train_selected = select.transform(X_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'X_train.shape:'</span>, X_train.shape)</span><br><span class="line">print(<span class="string">'X_train_selected.shape:'</span>, X_train_selected.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用get_support方法来查看那哪些特征被选中，它会返回所选特征的布尔遮罩（mask）</span></span><br><span class="line">mask = select.get_support()</span><br><span class="line">print(mask)</span><br><span class="line"><span class="comment"># 将遮罩可视化——黑色为True，白色为False</span></span><br><span class="line">plt.matshow(mask.reshape(<span class="number">1</span>, <span class="number">-1</span>), cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Sample index'</span>)</span><br><span class="line">plt.yticks([<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X_train.shape: (284, 80)</span><br><span class="line">X_train_selected.shape: (284, 40)</span><br><span class="line">[ True  True  True  True  True  True  True  True  True False  True False</span><br><span class="line">  True  True  True  True  True  True False False  True  True  True  True</span><br><span class="line">  True  True  True  True  True  True False False False  True False  True</span><br><span class="line"> False False  True False False False False  True False False  True False</span><br><span class="line"> False  True False  True False False False False False False  True False</span><br><span class="line">  True False False False False  True False  True False False False False</span><br><span class="line">  True  True False  True False False False False]</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/features-selected-by-selectpercentile.png"></p><p>从遮罩层的可视化中可以看出，大多数所选择的特征都是原始特征，并且大多数噪声特征都已被删除。但原始特征的还原并不完美。我们来比较 Logistic 回归在所有特征上的性能与仅使用所选特征时的性能。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对测试集进行变换</span></span><br><span class="line">X_test_selected = select.transform(X_test)</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">'Score with all features: &#123;:.3f&#125;'</span>.format(lr.score(X_test, y_test)))</span><br><span class="line"></span><br><span class="line">lr.fit(X_train_selected, y_train)</span><br><span class="line">print(<span class="string">'Score with only selected features: &#123;:.3f&#125;'</span>.format(lr.score(X_test_selected, y_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Score with all features: 0.930</span><br><span class="line">Score with only selected features: 0.940</span><br></pre></td></tr></table></figure><p>在这个例子中，删除噪声特征可以提高性能，即使丢失了某些原始特征。这是一个非常简单的假想示例，在真实数据上的结果要更加复杂。不过，如果特征量太大以至于无法构建模型，或者你怀疑许多特征完全没有信息量，那么单变量特征选择还是非常有用的。</p><h2 id="基于模型的特征选择">基于模型的特征选择</h2><p>基于模型的特征选择使用一个监督机器学习模型来判断每个特征的重要性，并且仅保留最重要的特征。用于特征选择的监督模型不需要与用于最终监督检模型的模型相同。特征选择模型需要为每个特征提供某种重要性度量，以便用这个度量对特征进行排序。决策树和基于决策树的模型提供了 feature_importances_ 属性，可以直接编码每个特征的重要性。线性模型系数的绝对值也可以用于表示特征重要性。L1 惩罚的线性模型学到的是稀疏系数，它只用到了特征的一个很小的子集。这可以被视为模型本身的一种特征选择形式，但也可以用作另一个模型选择特征的预处理步骤。与单变量选择不同，基于模型的选择同时考虑所有特征，因此可以获取交互项（如果模型能够获取它们的话）。要想使用基于模型的特征选择，我们需要使用 SelectFromModel 变换器。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel" target="_blank" rel="noopener"><code>class sklearn.feature_selection.SelectFromModel(estimator, threshold=None, prefit=False, norm_order=1, max_features=None)</code></a></p><p>Meta-transformer for selecting features based on importance weights.</p></blockquote><p>SelectFromModel类选出重要性度量（由监督模型提供）大于给定阈值的所有特征。我们用包含 100 棵树的随机森林分类器来计算特征重要性。这是一个相当复杂的模型，也比单变量测试要强大得多。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"></span><br><span class="line">select = SelectFromModel(</span><br><span class="line">    RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>),</span><br><span class="line">    threshold=<span class="string">'median'</span>)</span><br><span class="line"></span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line">X_train_l1 = select.transform(X_train)</span><br><span class="line">print(<span class="string">'X_train.shape:'</span>, X_train.shape)</span><br><span class="line">print(<span class="string">'X_train_l1.shape:'</span>, X_train_l1.shape)</span><br><span class="line"></span><br><span class="line">X_test_l1 = select.transform(X_test)</span><br><span class="line">score = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)</span><br><span class="line">print(<span class="string">'Test score: &#123;:.3f&#125;'</span>.format(score))</span><br><span class="line"></span><br><span class="line">mask = select.get_support()</span><br><span class="line">plt.matshow(mask.reshape(<span class="number">1</span>, <span class="number">-1</span>), cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Sample index'</span>)</span><br><span class="line">plt.yticks([<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X_train.shape: (284, 80)</span><br><span class="line">X_train_l1.shape: (284, 40)</span><br><span class="line">Test score: 0.951</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/features-selected-by-selectfrommodel.png"></p><h2 id="迭代特征选择">迭代特征选择</h2><p>在单变量测试中，我们没有使用模型，而在基于模型的选择中，我们使用了单个模型来选择特征。在迭代特征选择中，将会构建一系列模型，每个模型都使用不同数量的特征。有两种基本方法：开始时没有特征，然后逐个添加特征，直到满足某个终止条件；或者从所有特征开始，然后逐个删除特征，直到满足某个终止条件。由于构建了一系列模型，所以这些方法的计算成本要比前面的方法更高。其中一种特殊的方法是<strong>递归特征消除</strong>（recursive feature elimination，RFE），它从所有特征开始构建模型，并根据模型舍弃最不重要的特征，然后使用剩余特征来构建一个新模型，如此继续，直到仅剩下预设数量的特征。为了让这种方法能够运行，用于选择的模型需要提供某种确定特征重要性的方法，正如基于模型的选择所做的那样。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE" target="_blank" rel="noopener"><code>class sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)</code></a></p><p>Feature ranking with recursive feature elimination.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"></span><br><span class="line">select = RFE(RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>),</span><br><span class="line">    n_features_to_select=<span class="number">40</span>)</span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">mask = select.get_support()</span><br><span class="line">plt.matshow(mask.reshape(<span class="number">1</span>, <span class="number">-1</span>), cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Sample index'</span>)</span><br><span class="line">plt.yticks([<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/features-selected-by-rfe.png"></p><p>与单变量选择和基于模型的选择相比，迭代特征选择的结果更好，但仍然漏掉了一个特征。运行上述代码需要的时间也比基于模型的选择要长得多，因为对一个随机森林模型训练了 40 次，每运行一次删除一个特含。我们来测试一下使用 RFE 做特征选择时 Logistic 回归模型的精度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">score = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)</span><br><span class="line">print(<span class="string">'Test score: &#123;:.3f&#125;'</span>.format(score))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test score: 0.951</span><br></pre></td></tr></table></figure><p>我们还可以利用在 RFE 内使用的模型来进行预测。这仅使用被选择的特征集。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Test score: &#123;:.3f&#125;'</span>.format(select.score(X_test, y_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test score: 0.951</span><br></pre></td></tr></table></figure><p>这里，在 RFE 内部使用的随机森林的性能，与在所选特征上训练一个 Logistic 回归模型得到的性能相同。换句话说，只要我们选择了正确的特征，线性模型的表现就与随机森林一样好。</p><p>如果你不确定何时选择使用哪些特征作为机器学习算法的输入，那么自动化特征选择可能特别有用。它还有助于减少所需要的特征数量，加快预测速度，或允许可解释性更强的模型。在大多数现实情况下，使用特征选择不太可能大幅提升性能，但它仍是特征工程工具箱中一个非常有价值的工具。</p><h1 id="利用专家知识">利用专家知识</h1><p>对于特定应用来说，在特征工程中通常可以利用<strong>专家知识</strong>（expert knowledge）。虽然在许多情况下，机器学习的目的是避免创建一组专家设计的规则，但这并不意味着应该舍弃该应用或该领域的先验知识。通常来说，领域专家可以帮助找出有用的特征，其信息量比数据原始表示要大得多。</p><p>下面我们来看一个利用专家知识的特例。任务是预测在 Andreas 家门口的自行车出租。我们想要解决的任务是，对于给定的日期和时间，预测有多少人将会在 Andreas 的家门口租一辆自行车——这样他就知道是否还有自行车留给他。</p><p>我们首先将 2015 年 8 月的数据加载为一个 pandas 数据框。我们将数据重新采样为每 3 个小时一个数据，以得到每一天的主要趋势：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">citibike = mglearn.datasets.load_citibike()</span><br><span class="line">print(<span class="string">'Citi Bike data:\n'</span>, citibike.head())</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">xticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(), freq=<span class="string">'D'</span>)</span><br><span class="line">plt.xticks(xticks, xticks.strftime(<span class="string">'%a %m-%d'</span>), rotation=<span class="number">90</span>, ha=<span class="string">'left'</span>)</span><br><span class="line">plt.plot(citibike, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Data'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Rentals'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Citi Bike data:</span><br><span class="line"> starttime</span><br><span class="line">2015-08-01 00:00:00     3</span><br><span class="line">2015-08-01 03:00:00     0</span><br><span class="line">2015-08-01 06:00:00     9</span><br><span class="line">2015-08-01 09:00:00    41</span><br><span class="line">2015-08-01 12:00:00    39</span><br><span class="line">Freq: 3H, Name: one, dtype: int64</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/datasets-citibike.png"></p><p>观察此数据，我们可以清楚地区分每 24 小时中的白天和夜间。工作日和周末的模式似乎也有很大不同。在对这种时间序列上的预测任务进行评估时，我们通常希望<strong>从过去学习</strong>并<strong>预测未来</strong>。也就是说，在划分训练集和测试集的时候，我们希望使用某个特定日期之前的所有数据作为训练集，该日期之后的所有数据作为测试集。我们将使用前 184 个数据点（对应前 23 天）作为训练集，剩余的 64 个数据点（对应剩余的 8 天）作为测试集。</p><p>在我们的预测任务中，我们使用的唯一特征就是某一租车数量对应的日期和时间。因此输入特征是日期和时间，比如 2014-08-01 00:00:00，而输出是在接下来 3 小时内的租车数量（在这个例子中是 3）。</p><p>在计算机上存储日期的常用方式是使用 POSIX 时间，它是从 1970 年 1 月 1 日 00:00:00（也就是 Unix 时间的起点）起至现在的总秒数。首先，我们可以尝试使用这个单一整数特征作为数据表示。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取目标值</span></span><br><span class="line">y = citibike.values</span><br><span class="line"><span class="comment"># 利用"%s"将时间转换为POSIX时间</span></span><br><span class="line">X = np.reshape(citibike.index.strftime(<span class="string">"%s"</span>).astype(<span class="string">"int"</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，它可以将数据划分为训练集和测试集，构建模型并将结果可视化</span></span><br><span class="line">n_train = <span class="number">184</span>  <span class="comment"># 使用前184个数据点用于训练，剩余的数据点用于测试</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对给定特征集上的回归进行评估和作图的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_on_features</span><span class="params">(features, target, regressor)</span>:</span></span><br><span class="line">    <span class="comment"># 将给定特征划分为训练集和测试集</span></span><br><span class="line">    X_train, X_test = features[:n_train], features[n_train:]</span><br><span class="line">    <span class="comment"># 同样划分目标数组</span></span><br><span class="line">    y_train, y_test = target[:n_train], target[n_train:]</span><br><span class="line">    regressor.fit(X_train, y_train)</span><br><span class="line">    print(<span class="string">'Test-set R^2: &#123;:.2f&#125;'</span>.format(regressor.score(X_test, y_test)))</span><br><span class="line"></span><br><span class="line">    y_pred = regressor.predict(X_test)</span><br><span class="line">    y_pred_train = regressor.predict(X_train)</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">    plt.xticks(range(<span class="number">0</span>, len(X), <span class="number">8</span>), xticks.strftime(<span class="string">'%a %m-%d'</span>), rotation=<span class="number">90</span>, ha=<span class="string">'left'</span>)</span><br><span class="line">    plt.plot(range(n_train), y_train, label=<span class="string">'train'</span>)</span><br><span class="line">    plt.plot(range(n_train, len(y_test) + n_train), y_test, <span class="string">'-'</span>, label=<span class="string">'test'</span>)</span><br><span class="line"></span><br><span class="line">    plt.plot(range(n_train), y_pred_train, <span class="string">'--'</span>, label=<span class="string">'prediction train'</span>)</span><br><span class="line">    plt.plot(range(n_train, len(y_test) + n_train), y_pred, <span class="string">'--'</span>, label=<span class="string">'prediction test'</span>)</span><br><span class="line">    plt.legend(loc=(<span class="number">1.01</span>, <span class="number">0</span>))</span><br><span class="line">    plt.xlabel(<span class="string">'Date'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Rentals'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用随机森林回归</span></span><br><span class="line">regressor = RandomForestRegressor(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br><span class="line">eval_on_features(X, y, regressor)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test-set R^2: -0.04</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/random-forest-on-citibike.png"></p><p>在训练集上的预测结果相当好，这符合随机森林通常的表现。但对于测试集来说，预测结果是一条常数直线。<span class="math inline">\(R^2\)</span>为 -0.04，说明我们什么都没有学到。发生了什么？</p><p>问题在于特征和随机森林的组合。测试集中 POSIX 时间特征的值超出了训练集中特征取值的范围：测试集中数据点的时间戳要晚于训练集中的所有数据点。树以及随机森林无法<strong>外推</strong>（extrapolate）到训练集之外的特征范围。结果就是模型只能预测训练集中最近数据点的目标值，即最后一次观测到数据的时间。</p><p>显然，我们可以做到更好。这就是我们的“专家知识”的用武之地。通过观察训练数据中的租车数量图像，我们发现两个因素似乎非常重要：一天内的时间与一周的星期几。因此我们来添加这两个特征。我们从 POSIX 时间中学不到任何东西，所以删掉这个特征。首先，我们仅使用每天的时刻。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_hour = np.reshape(citibike.index.hour, (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">eval_on_features(X_hour, y, regressor)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test-set R^2: 0.60</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/random-forest-on-citibike-hour.png"></p><p><span class="math inline">\(R^2\)</span> 已经好多了，但预测结果显然没有抓住每周的模式，现在的预测结果对一周内的每天都具有相同的模式。下面我们添加一周的星期几作为特征。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_hour_week = np.hstack([np.reshape(citibike.index.dayofweek, (<span class="number">-1</span>, <span class="number">1</span>)),</span><br><span class="line">                          np.reshape(citibike.index.hour, (<span class="number">-1</span>, <span class="number">1</span>))])</span><br><span class="line">eval_on_features(X_hour_week, y, regressor)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test-set R^2: 0.84</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/random-forest-on-citibike-hour-week.png"></p><p>现在我们的模型通过考虑一周的星期几和一天内的时间捕捉到了周期性的行为。它的 <span class="math inline">\(R^2\)</span> 为 0.84，预测性能相当好。模型学到的内容可能是 8 月前 23 天中星期几与时刻每种组合的平均租车数量。这实际上不需要像随机森林这样复杂的模型，所以我们尝试一个更简单的模型——LinearRegression。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eval_on_features(X_hour_week, y, LinearRegression())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test-set R^2: 0.13</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/linear-regression-on-citibike.png"></p><p><strong>LinearRegression 的效果差得多，而且周期性模式看起来很奇怪。其原因在于我们用整数编码一周的星期几和一天内的时间，它们被解释为连续变量。</strong>因此，线性模型只能学到关于每天时间的线性函数——它学到的是，时间越晚，租车数量越多。但实际模式比这要复杂得多。我们可以通过将整数解释为分类变量（用 OneHotEncoder 进行变换）来获得这种模式。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">enc = OneHotEncoder()</span><br><span class="line">X_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()</span><br><span class="line">eval_on_features(X_hour_week_onehot, y, Ridge())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test-set R^2: 0.62</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/ridge-on-citibike.png"></p><p>它给出了比连续特征编码好得多的匹配。现在线性模型为一周内的每天都学到了一个系数，为一天内的每个时刻都学到了一个系数。也就是说，一周七天共享“一天内每个时刻”的模式。</p><p>利用交互特征，我们可以让模型为星期几和时刻的每一种组合都学到一个系数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly_transformer = PolynomialFeatures(degree=<span class="number">2</span>, interaction_only=<span class="literal">True</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">X_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)</span><br><span class="line">lr = Ridge()</span><br><span class="line">eval_on_features(X_hour_week_onehot_poly, y, lr)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test-set R^2: 0.85</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/ridge-on-citibike-poly.png"></p><p>这一变换最终得到一个性能与随机森林类似的模型。这个模型的一大优点是，可以很清楚地看到学到的内容：对每个星期几和时刻的交互项学到了一个系数。我们可以将模型学到的系数作图，而这对于随机森林来说是不可能的（随机森林也有关于特征重要性的参数 <code>feature_importances_</code>，为何不可能？）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为时刻和星期几创建名称</span></span><br><span class="line">hour = [<span class="string">'%02d:00'</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">24</span>, <span class="number">3</span>)]</span><br><span class="line">day = [<span class="string">'Mon'</span>, <span class="string">'Tue'</span>, <span class="string">'Wed'</span>, <span class="string">'Thu'</span>, <span class="string">'Fri'</span>, <span class="string">'Sat'</span>, <span class="string">'Sun'</span>]</span><br><span class="line">features = day + hour</span><br><span class="line"><span class="comment"># 利用 get_feature_names 方法对 PolynomialFeatures 提取的所有交互特征进行命名</span></span><br><span class="line">features_poly = poly_transformer.get_feature_names(features)</span><br><span class="line">features_nonzero = np.array(features_poly)[lr.coef_ != <span class="number">0</span>]</span><br><span class="line">coef_nonzero = lr.coef_[lr.coef_ != <span class="number">0</span>]</span><br><span class="line"><span class="comment"># 将线性模型学到的系数可视化</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">2</span>))</span><br><span class="line">plt.plot(coef_nonzero, <span class="string">'o'</span>)</span><br><span class="line">plt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=<span class="number">90</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature name'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature magnitude'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/coef-of-linear-regression-on-citibike.png"></p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/cross-validation/" rel="bookmark">交叉验证</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/decision-tree-ensemble/" rel="bookmark">决策树集成</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/clustering/" rel="bookmark">聚类</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/" rel="bookmark">降维、特征提取与流形学习</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/notes/introduction-to-ml-with-python/decision-trees/" rel="bookmark">决策树</a></div></li></ul><footer class="post-footer"><div class="post-tags"> <a href="/tags/introduction-to-ml-with-python/" rel="tag"># Python 机器学习基础教程</a></div><div class="post-nav"><div class="post-nav-item"><a href="/mac/useful-mac-apps/" rel="prev" title="Useful Mac Apps"><i class="fa fa-chevron-left"></i> Useful Mac Apps</a></div><div class="post-nav-item"> <a href="/programming/leetcode/binary-search/" rel="next" title="704. Binary Search">704. Binary Search<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#分类变量"><span class="nav-number">1.</span> <span class="nav-text">分类变量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#one-hot编码虚拟变量"><span class="nav-number">1.1.</span> <span class="nav-text">One-Hot编码（虚拟变量）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#检查字符串编码的分类数据"><span class="nav-number">1.1.1.</span> <span class="nav-text">检查字符串编码的分类数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数字可以编码分类变量"><span class="nav-number">1.2.</span> <span class="nav-text">数字可以编码分类变量</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分箱离散化线性模型与树"><span class="nav-number">2.</span> <span class="nav-text">分箱、离散化、线性模型与树</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#交互特征与多项式特征"><span class="nav-number">3.</span> <span class="nav-text">交互特征与多项式特征</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#单变量非线性变换"><span class="nav-number">4.</span> <span class="nav-text">单变量非线性变换</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#自动化特征选择"><span class="nav-number">5.</span> <span class="nav-text">自动化特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#单变量统计"><span class="nav-number">5.1.</span> <span class="nav-text">单变量统计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于模型的特征选择"><span class="nav-number">5.2.</span> <span class="nav-text">基于模型的特征选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#迭代特征选择"><span class="nav-number">5.3.</span> <span class="nav-text">迭代特征选择</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#利用专家知识"><span class="nav-number">6.</span> <span class="nav-text">利用专家知识</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="菜农陈文生" src="/uploads/avatar/nekosensei.png"><p class="site-author-name" itemprop="name">菜农陈文生</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">245</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">161</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/cncws" title="GitHub → https://github.com/cncws" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a></span><span class="links-of-author-item"><a href="mailto:1031616423@qq.com" title="E-Mail → mailto:1031616423@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">菜农陈文生</span></div><script src="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="//cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script><script src="/js/aplayer-el.js"></script><script src="//unpkg.com/video.js/dist/video.min.js"></script><script src="/js/videojs-bilibili.js"></script><script src="/js/videojs-bilibili-el.js"></script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Axl15EIRi5o5AatKaxXxV4Oq-gzGzoHsz',
      appKey     : 'E0qm04UjsP0qQN1l8ME3GQ25',
      placeholder: "Just go go",
      avatar     : 'identicon',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script></div></body></html>
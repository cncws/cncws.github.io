<!DOCTYPE html><html lang="zh-CN,en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/uploads/icon/drop/128x128.png"><link rel="icon" type="image/png" sizes="32x32" href="/uploads/icon/drop/32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/uploads/icon/drop/16x16.png"><link rel="mask-icon" href="/uploads/icon/drop/drop.svg" color="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="NKt2iJb3Hnl6-Sm7LB-fTT7LRyi9cg5yZrB-zd0ohtk"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"cwscn.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1,width:240},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!1,mediumzoom:!0,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"fadeIn"}},path:"search.xml"}</script><meta name="description" content="利用无监督学习进行数据变换可能有很多种目的。最常见的目的就是可视化、压缩数据，以及寻找信息量更大的数据表示以用于进一步的处理。 为了实现这些目的，最简单也最常用的一种算法就是主成分分析。我们也将学习另外两种算法：非负矩阵分解（NMF）和t-SNE，前者通常用于特征提取，后者通常用于二维散点的可视化。 主成分分析 主成分分析（principal component analysis，PCA）是一种旋"><meta name="keywords" content="无监督,预处理"><meta property="og:type" content="article"><meta property="og:title" content="降维、特征提取与流形学习"><meta property="og:url" content="https://cwscn.github.io/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/index.html"><meta property="og:site_name" content="春夏秋冬"><meta property="og:description" content="利用无监督学习进行数据变换可能有很多种目的。最常见的目的就是可视化、压缩数据，以及寻找信息量更大的数据表示以用于进一步的处理。 为了实现这些目的，最简单也最常用的一种算法就是主成分分析。我们也将学习另外两种算法：非负矩阵分解（NMF）和t-SNE，前者通常用于特征提取，后者通常用于二维散点的可视化。 主成分分析 主成分分析（principal component analysis，PCA）是一种旋"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/pca.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/feature-hist.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/pca-on-cancer.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/2-principal-components-of-cancer.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/datasets-lfw-people.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/pca-whitening.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/visualization-of-image-pca.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/break-down-image-to-weighted-summation-of-pca.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/pca-faces.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/pca-faces-scatter.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/nmf.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/nmf-faces.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/nmf-components.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/nmf-components-argsort-3.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/nmf-components-argsort-7.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/datasets-signals.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/nmf-and-pca-on-signals.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/datasets-digits.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/pca-digits.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/t-sne-digits.png"><meta property="og:updated_time" content="2020-08-22T06:01:44.785Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="降维、特征提取与流形学习"><meta name="twitter:description" content="利用无监督学习进行数据变换可能有很多种目的。最常见的目的就是可视化、压缩数据，以及寻找信息量更大的数据表示以用于进一步的处理。 为了实现这些目的，最简单也最常用的一种算法就是主成分分析。我们也将学习另外两种算法：非负矩阵分解（NMF）和t-SNE，前者通常用于特征提取，后者通常用于二维散点的可视化。 主成分分析 主成分分析（principal component analysis，PCA）是一种旋"><meta name="twitter:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/pca.png"><link rel="canonical" href="https://cwscn.github.io/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><link rel="stylesheet" href="//unpkg.com/video.js/dist/video-js.min.css"><link rel="stylesheet" href="/css/videojs-bilibili.css"><style>.aplayer.aplayer-arrow .aplayer-icon-loop,.aplayer.aplayer-arrow .aplayer-icon-order{display:inline-block}</style><title>降维、特征提取与流形学习 | 春夏秋冬</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">春夏秋冬</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">愿你走出半生 归来仍是少年</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档<span class="badge">255</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类<span class="badge">24</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签<span class="badge">108</span></a></li><li class="menu-item menu-item-收藏"><a href="/favlist/" rel="section"><i class="fa fa-star fa-fw"></i> 收藏</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div><meting-js server="netease" type="playlist" id="67155774" theme="#ff5555" loop="all" order="list" preload="none" volume="" mutex="" list-folded="NaN" fixed="true"></meting-js></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://cwscn.github.io/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar/nekosensei.png"><meta itemprop="name" content="菜农陈文生"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="春夏秋冬"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 降维、特征提取与流形学习</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-08-22 14:01:44" itemprop="dateModified" datetime="2020-08-22T14:01:44+08:00">2020-08-22</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book/" itemprop="url" rel="index"><span itemprop="name">书籍</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book/introduction-to-ml-with-python/" itemprop="url" rel="index"><span itemprop="name">Python 机器学习基础教程</span></a></span></span><span id="/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/" class="post-meta-item leancloud_visitors" data-flag-title="降维、特征提取与流形学习" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>利用无监督学习进行数据变换可能有很多种目的。最常见的目的就是可视化、压缩数据，以及寻找信息量更大的数据表示以用于进一步的处理。</p><p>为了实现这些目的，最简单也最常用的一种算法就是主成分分析。我们也将学习另外两种算法：非负矩阵分解（NMF）和t-SNE，前者通常用于特征提取，后者通常用于二维散点的可视化。</p><h1 id="主成分分析">主成分分析</h1><p>主成分分析（principal component analysis，PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转之后，通常是根据新特征对解释数据的重要性来选择它的一个子集。下面的例子展示了PCA对一个模拟二维数据集的作用。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.decomposition.pca" target="_blank" rel="noopener"><code>class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0, iterated_power=’auto’, random_state=None)</code></a></p><p>Principal component analysis (PCA)</p></blockquote><a id="more"></a><h2 id="将pca应用于模拟数据">将PCA应用于模拟数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 函数源码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_pca_illustration</span><span class="params">()</span>:</span></span><br><span class="line">    rnd = np.random.RandomState(<span class="number">5</span>)</span><br><span class="line">    X_ = rnd.normal(size=(<span class="number">300</span>, <span class="number">2</span>))</span><br><span class="line">    X_blob = np.dot(X_, rnd.normal(size=(<span class="number">2</span>, <span class="number">2</span>))) + rnd.normal(size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    pca = PCA()</span><br><span class="line">    pca.fit(X_blob)</span><br><span class="line">    X_pca = pca.transform(X_blob)</span><br><span class="line"></span><br><span class="line">    S = X_pca.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    axes = axes.ravel()</span><br><span class="line"></span><br><span class="line">    axes[<span class="number">0</span>].set_title(<span class="string">"Original data"</span>)</span><br><span class="line">    axes[<span class="number">0</span>].scatter(X_blob[:, <span class="number">0</span>], X_blob[:, <span class="number">1</span>], c=X_pca[:, <span class="number">0</span>], linewidths=<span class="number">0</span>,</span><br><span class="line">                    s=<span class="number">60</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    axes[<span class="number">0</span>].set_xlabel(<span class="string">"feature 1"</span>)</span><br><span class="line">    axes[<span class="number">0</span>].set_ylabel(<span class="string">"feature 2"</span>)</span><br><span class="line">    axes[<span class="number">0</span>].arrow(pca.mean_[<span class="number">0</span>], pca.mean_[<span class="number">1</span>], S[<span class="number">0</span>] * pca.components_[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  S[<span class="number">0</span>] * pca.components_[<span class="number">0</span>, <span class="number">1</span>], width=<span class="number">.1</span>, head_width=<span class="number">.3</span>,</span><br><span class="line">                  color=<span class="string">'k'</span>)</span><br><span class="line">    axes[<span class="number">0</span>].arrow(pca.mean_[<span class="number">0</span>], pca.mean_[<span class="number">1</span>], S[<span class="number">1</span>] * pca.components_[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                  S[<span class="number">1</span>] * pca.components_[<span class="number">1</span>, <span class="number">1</span>], width=<span class="number">.1</span>, head_width=<span class="number">.3</span>,</span><br><span class="line">                  color=<span class="string">'k'</span>)</span><br><span class="line">    axes[<span class="number">0</span>].text(<span class="number">-1.5</span>, <span class="number">-.5</span>, <span class="string">"Component 2"</span>, size=<span class="number">14</span>)</span><br><span class="line">    axes[<span class="number">0</span>].text(<span class="number">-4</span>, <span class="number">-4</span>, <span class="string">"Component 1"</span>, size=<span class="number">14</span>)</span><br><span class="line">    axes[<span class="number">0</span>].set_aspect(<span class="string">'equal'</span>)</span><br><span class="line"></span><br><span class="line">    axes[<span class="number">1</span>].set_title(<span class="string">"Transformed data"</span>)</span><br><span class="line">    axes[<span class="number">1</span>].scatter(X_pca[:, <span class="number">0</span>], X_pca[:, <span class="number">1</span>], c=X_pca[:, <span class="number">0</span>], linewidths=<span class="number">0</span>,</span><br><span class="line">                    s=<span class="number">60</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_xlabel(<span class="string">"First principal component"</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_ylabel(<span class="string">"Second principal component"</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_ylim(<span class="number">-8</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    pca = PCA(n_components=<span class="number">1</span>)</span><br><span class="line">    pca.fit(X_blob)</span><br><span class="line">    X_inverse = pca.inverse_transform(pca.transform(X_blob))</span><br><span class="line"></span><br><span class="line">    axes[<span class="number">2</span>].set_title(<span class="string">"Transformed data w/ second component dropped"</span>)</span><br><span class="line">    axes[<span class="number">2</span>].scatter(X_pca[:, <span class="number">0</span>], np.zeros(X_pca.shape[<span class="number">0</span>]), c=X_pca[:, <span class="number">0</span>],</span><br><span class="line">                    linewidths=<span class="number">0</span>, s=<span class="number">60</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    axes[<span class="number">2</span>].set_xlabel(<span class="string">"First principal component"</span>)</span><br><span class="line">    axes[<span class="number">2</span>].set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">    axes[<span class="number">2</span>].set_ylim(<span class="number">-8</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    axes[<span class="number">3</span>].set_title(<span class="string">"Back-rotation using only first component"</span>)</span><br><span class="line">    axes[<span class="number">3</span>].scatter(X_inverse[:, <span class="number">0</span>], X_inverse[:, <span class="number">1</span>], c=X_pca[:, <span class="number">0</span>],</span><br><span class="line">                    linewidths=<span class="number">0</span>, s=<span class="number">60</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    axes[<span class="number">3</span>].set_xlabel(<span class="string">"feature 1"</span>)</span><br><span class="line">    axes[<span class="number">3</span>].set_ylabel(<span class="string">"feature 2"</span>)</span><br><span class="line">    axes[<span class="number">3</span>].set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">    axes[<span class="number">3</span>].set_xlim(<span class="number">-8</span>, <span class="number">4</span>)</span><br><span class="line">    axes[<span class="number">3</span>].set_ylim(<span class="number">-8</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">mglearn.plots.plot_pca_illustration()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/pca.png"></p><p>算法首先找到方法最大的方向，将其标记为“成分1”（Component 1）。这是数据中包含最多信息的方向（或向量），换句话说，沿着这个方向的特征之间最为相关。然后，算法找到与第一个方向正交（成直角）且包含最多信息的方向。在二维空间中，只有一个成直角的方向，但在更高维的空间中会有（无穷）多的正交方向。虽然这两个成分都画成箭头，但其头尾的位置并不重要。我们也可以将第一个成分化成从中心指向左上，而不是右下。利用这一过程找到的方向被称为主成分，因为它们是数据方差的主要方向。一般来说，主成分的个数与原始特征相同。</p><p>第二张图（右上）显示的是同样的数据，但现在将其旋转，使得第一主成分与x轴平行且第二主成分与y轴平行。在旋转之前，从数据中减去平均值，使得变换后的数据以零为中心。在PCA找到的旋转表示中，两个坐标轴是不相关的，也就是说，对于这种数据表示，除了对角线，相关矩阵全部为零。</p><p>我们可以通过仅保留一部分主成分来使用PCA进行降维。在这个例子中，我们可以仅保留第一主成分，正如第三张图（左下）所示。这将数据从二维数据集降为一维数据集。但要注意，我们没有保留原始特征之一，而是找到了最有趣的方向（第一张图中从左上到右下）并保留这一方向，即第一主成分。</p><p>最后，我们可以反向旋转并将平均值重新加到数据集中。这样会得到最后一张图中的数据。这些数据点位于原始特征空间中，但我们仅保留了第一主成分中包含的信息。这种变换有时用于去除数据中的噪声影响，或者将主成分中保留的那部分信息可视化。</p><h2 id="将pca应用于cancer数据集并可视化">将PCA应用于cancer数据集并可视化</h2><p>PCA最常见的应用之一就是将高维数据可视化。对于有两个以上特征的数据，很难绘制散点图。对于Iris（鸢尾花）数据集，我们可以创建散点图矩阵，通过展示特征所有可能的两两组和来展示数据的局部图像。但如果我们想要查看乳腺癌数据集，即便用散点图矩阵也很困难。这个数据集包含30个特征，这就导致需要绘制<code>30*14=420</code>张散点图！我们永远不可能仔细观察所有这些图像，更不用说试图理解它们了。</p><p>不过我们可以使用一种更简单的可视化方法——对每个特征分别计算两个类别（良性肿瘤和恶性肿瘤）的直方图。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">15</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">20</span>))</span><br><span class="line">malignant = cancer.data[cancer.target == <span class="number">0</span>]</span><br><span class="line">benign = cancer.data[cancer.target == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">ax = axes.ravel()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">    _, bins = np.histogram(cancer.data[:, i], bins=<span class="number">50</span>)</span><br><span class="line">    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(<span class="number">0</span>), alpha=<span class="number">.5</span>)</span><br><span class="line">    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(<span class="number">2</span>), alpha=<span class="number">.5</span>)</span><br><span class="line">    ax[i].set_title(cancer.feature_names[i])</span><br><span class="line">    ax[i].set_yticks(())</span><br><span class="line"></span><br><span class="line">ax[<span class="number">0</span>].set_xlabel(<span class="string">'Feature magnitude'</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">'Frequency'</span>)</span><br><span class="line">ax[<span class="number">0</span>].legend([<span class="string">'malignant'</span>, <span class="string">'benign'</span>], loc=<span class="string">'best'</span>)</span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/feature-hist.png"></p><p>这里我们为每个特征创建一个直方图，计算具有某一特征的数据点在特定范围内（叫作bin）的出现频率。每张图都包含两个直方图，一个是良性类别的所有点（蓝色），一个是恶性类别的所有点（红色）。这样我们可以了解每个特征在两个类别中的分布情况，也可以猜测哪些特征能够更好地区分良性样本和恶性样本。例如，“smoothness error”特征似乎没有什么信息量，因为两个直方图大部分都重叠在一起，而“worst concave points”特征看起来信息量相当大，因为两个直方图的交集很小。</p><p>但是，这种图无法向我们展示变量之间的相互作用以及这种相互作用与类别之间的关系。<strong>利用PCA，我们可以获取到主要的相互作用，并得到稍为完整的图像。</strong>我们可以找到前两个主成分，并在这个新的二维空间中用散点图将数据可视化。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在应用PCA之前，利用StandardScaler缩放数据，使每个特征的方差均为1</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(cancer.data)</span><br><span class="line">X_scaled = scaler.transform(cancer.data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留数据的前两个主要成分</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 对乳腺癌数据拟合PCA模型</span></span><br><span class="line">pca.fit(X_scaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据变换到前两个主要成分上</span></span><br><span class="line">X_pca = pca.transform(X_scaled)</span><br><span class="line">print(<span class="string">'Original shape: &#123;&#125;'</span>.format(str(X_scaled.shape)))</span><br><span class="line">print(<span class="string">'Reduced shape: &#123;&#125;'</span>.format(str(X_pca.shape)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对第一个和第二个主成分作图，按类别着色</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">mglearn.discrete_scatter(X_pca[:, <span class="number">0</span>], X_pca[:, <span class="number">1</span>], cancer.target)</span><br><span class="line">plt.legend(cancer.target_names, loc=<span class="string">'best'</span>)</span><br><span class="line">plt.gca().set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'First principal component'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Second principal component'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Original shape: (569, 30)</span><br><span class="line">Reduced shape: (569, 2)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/pca-on-cancer.png"></p><p>重要的是要注意，PCA是一种无监督方法，在寻找旋转方向时没有用到任何类别信息。它只是观察数据中的相关性。对于这里所示的散点图，我们绘制了第一主成分和第二主成分之间的关系，然后利用类别信息对数据点进行着色。你可以看到，在这个二维空间中两个类别被很好的分离。这让我们相信，即使是线性分类器（在这个空间中学习一条直线）也可以在区分这两个类别时表现得相当不错。我们还可以看到，恶性点比良性点更加分散，这一点也可以在直方图中看出来。</p><p>PCA的一个缺点在于，通常不容易对图中的两个轴做出解释。主成分对应于原始数据中的方向，所以它们是原始特征的组合。但这些组合往往非常复杂，这一点我们很快就会看到。在拟合过程中，主成分被保存在PCA对象的<code>components_</code>属性中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'PCA component shape: &#123;&#125;'</span>.format(pca.components_.shape))</span><br><span class="line">print(<span class="string">'PCA components:\n&#123;&#125;'</span>.format(pca.components_))</span><br><span class="line"><span class="comment"># 用热图将系数可视化</span></span><br><span class="line">plt.matshow(pca.components_, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">plt.yticks([<span class="number">0</span>, <span class="number">1</span>], [<span class="string">'First component'</span>, <span class="string">'Second component'</span>])</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.xticks(range(len(cancer.feature_names)),</span><br><span class="line">    cancer.feature_names, rotation=<span class="number">60</span>, ha=<span class="string">'left'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Principal components'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PCA component shape: (2, 30)</span><br><span class="line">PCA components:</span><br><span class="line">[[ 0.21890244  0.10372458  0.22753729  0.22099499  0.14258969  0.23928535</span><br><span class="line">   0.25840048  0.26085376  0.13816696  0.06436335  0.20597878  0.01742803</span><br><span class="line">   0.21132592  0.20286964  0.01453145  0.17039345  0.15358979  0.1834174</span><br><span class="line">   0.04249842  0.10256832  0.22799663  0.10446933  0.23663968  0.22487053</span><br><span class="line">   0.12795256  0.21009588  0.22876753  0.25088597  0.12290456  0.13178394]</span><br><span class="line"> [-0.23385713 -0.05970609 -0.21518136 -0.23107671  0.18611302  0.15189161</span><br><span class="line">   0.06016536 -0.0347675   0.19034877  0.36657547 -0.10555215  0.08997968</span><br><span class="line">  -0.08945723 -0.15229263  0.20443045  0.2327159   0.19720728  0.13032156</span><br><span class="line">   0.183848    0.28009203 -0.21986638 -0.0454673  -0.19987843 -0.21935186</span><br><span class="line">   0.17230435  0.14359317  0.09796411 -0.00825724  0.14188335  0.27533947]]</span><br></pre></td></tr></table></figure><p><code>components_</code>中的每一行对应于一个主成分，它们按重要性排序（第一主成分排在首位，以此类推）。列对应于PCA的原始特征属性。</p><p><img src="/uploads/image/introduction-to-ml-with-python/2-principal-components-of-cancer.png"></p><p>在第一个主成分中，<strong>所有特征的符号相同（均为正，但前面我们提到过，箭头指向哪个方向无关紧要）。这意味着在所有特征之间存在普遍的相关性。</strong>如果一个测量值较大的话，其他的测量值可能也较大。第二个主成分的符号有正有负，而且两个主成分都包含30个特征。这种所有特征的混合使得解释上图中的坐标轴变得十分困难。</p><h2 id="特征提取的特征脸">特征提取的特征脸</h2><p>PCA的另一个应用是特征提取。特征提取背后的思想是，可以找到一种数据表示，比给定的原始表示更适合于分析。特征提取很有用，它的一个很好的应用实例就是图像。图像由像素组成，通常存储为红绿蓝（RGB）强度。图像中的对象通常由千万个像素组成，它们只有放在一起才有意义。</p><p>我们将给出用PCA对图像做特征提取的一个简单应用，即处理Wild数据集Labeled Faces中的人脸图像。这一数据集包含从互联网下载的名人脸部图像，它包含从21世纪初开始的政治家、歌手、演员和运动员的人脸图像。我们使用这些图像的灰度版本，并将它们按比例缩小以加快处理速度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"></span><br><span class="line">people = fetch_lfw_people(min_faces_per_person=<span class="number">20</span>, resize=<span class="number">.7</span>)</span><br><span class="line">image_shape = people.images[<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">8</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> target, image, ax <span class="keyword">in</span> zip(people.target, people.images, axes.ravel()):</span><br><span class="line">    ax.imshow(image, cmap=plt.cm.gray_r)</span><br><span class="line">    ax.set_title(people.target_names[target])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'people.images.shape: &#123;&#125;'</span>.format(people.images.shape))</span><br><span class="line">print(<span class="string">'Number of classes: &#123;&#125;'</span>.format(len(people.target_names)))</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/datasets-lfw-people.png"></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">people.images.shape: (3023, 87, 65)  # 一共有3023张图像，每张大小为87像素x65像素</span><br><span class="line">Number of classes: 62  # 分别属于62个不同的人</span><br></pre></td></tr></table></figure><p>这个数据集有些偏斜，其中包含George W. Bush（小布什）和Colin Powell（科林·鲍威尔）的大量图像。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算每个目标出现的次数</span></span><br><span class="line">counts = np.bincount(people.target)</span><br><span class="line"><span class="comment"># 将次数与目标名称一起打印出来</span></span><br><span class="line"><span class="keyword">for</span> i, (count, name) <span class="keyword">in</span> enumerate(zip(counts, people.target_names)):</span><br><span class="line">    print(<span class="string">'&#123;0:25&#125; &#123;1:3&#125;'</span>.format(name, count), end=<span class="string">'    '</span>)</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">3</span> == <span class="number">0</span>:</span><br><span class="line">        print()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Alejandro Toledo           39    Alvaro Uribe               35    Amelie Mauresmo            21</span><br><span class="line">Andre Agassi               36    Angelina Jolie             20    Ariel Sharon               77</span><br><span class="line">Arnold Schwarzenegger      42    Atal Bihari Vajpayee       24    Bill Clinton               29</span><br><span class="line">Carlos Menem               21    Colin Powell              236    David Beckham              31</span><br><span class="line">Donald Rumsfeld           121    George Robertson           22    George W Bush             530</span><br><span class="line">Gerhard Schroeder         109    Gloria Macapagal Arroyo    44    Gray Davis                 26</span><br><span class="line">Guillermo Coria            30    Hamid Karzai               22    Hans Blix                  39</span><br><span class="line">Hugo Chavez                71    Igor Ivanov                20    Jack Straw                 28</span><br><span class="line">Jacques Chirac             52    Jean Chretien              55    Jennifer Aniston           21</span><br><span class="line">Jennifer Capriati          42    Jennifer Lopez             21    Jeremy Greenstock          24</span><br><span class="line">Jiang Zemin                20    John Ashcroft              53    John Negroponte            31</span><br><span class="line">Jose Maria Aznar           23    Juan Carlos Ferrero        28    Junichiro Koizumi          60</span><br><span class="line">Kofi Annan                 32    Laura Bush                 41    Lindsay Davenport          22</span><br><span class="line">Lleyton Hewitt             41    Luiz Inacio Lula da Silva  48    Mahmoud Abbas              29</span><br><span class="line">Megawati Sukarnoputri      33    Michael Bloomberg          20    Naomi Watts                22</span><br><span class="line">Nestor Kirchner            37    Paul Bremer                20    Pete Sampras               22</span><br><span class="line">Recep Tayyip Erdogan       30    Ricardo Lagos              27    Roh Moo-hyun               32</span><br><span class="line">Rudolph Giuliani           26    Saddam Hussein             23    Serena Williams            52</span><br><span class="line">Silvio Berlusconi          33    Tiger Woods                23    Tom Daschle                25</span><br><span class="line">Tom Ridge                  33    Tony Blair                144    Vicente Fox                32</span><br><span class="line">Vladimir Putin             49    Winona Ryder               24</span><br></pre></td></tr></table></figure><p>为了降低数据偏斜，我们对每个人最多只取50张图像（否则，特征提取将会被George w. Bush影响的可能性大大增加）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask = np.zeros(people.target.shape, dtype=np.bool)</span><br><span class="line"><span class="keyword">for</span> target <span class="keyword">in</span> np.unique(people.target):</span><br><span class="line">    mask[np.where(people.target == target)[<span class="number">0</span>][:<span class="number">50</span>]] = <span class="number">1</span>  <span class="comment"># 各取前50个</span></span><br><span class="line"></span><br><span class="line">X_people = people.data[mask]</span><br><span class="line">y_people = people.target[mask]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将灰度值缩放到0到1之间，而不是在0到255之间</span></span><br><span class="line"><span class="comment"># 以得到更好的数据稳定性</span></span><br><span class="line">X_people = X_people / <span class="number">255</span></span><br></pre></td></tr></table></figure><p>人脸识别的一个常见任务就是看某个前所未见的人脸是否属于数据库中某个已知人物。这在照片收集、社交媒体和安全应用中都有应用。<strong>解决这个问题的方法之一就是构建一个分类器，每个人都是一个单独的类别。但人脸数据库中通常有许多不同的人，而同一个人的图像很少（也就是说，每个类别的训练样例很少）。这使得大多数分类器的训练都很困难。另外，通常你还想要能够轻松添加新的人物，不需要重新训练一个大型模型。</strong></p><p><strong>一种简单的解决方法是使用单一最近邻分类器，寻找与你要分类的人脸最为相似的人脸。这个分类器原则上可以处理每个类别只有一个训练样例的情况。</strong>下面看一下<code>KNeighborsClassifier</code>的表现如何。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将数据划分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_people, y_people, stratify=y_people, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 使用一个邻居构建</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">'Test set score of 1-nn: &#123;:.2f&#125;'</span>.format(knn.score(X_test, y_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test set score of 1-nn: 0.23</span><br></pre></td></tr></table></figure><p>我们得到的精度为23%。对于包含62个类别的分类问题来说，这实际上不算太差，但也不算好。</p><p>这里就可以用到PCA。<strong>想要度量人脸的相似度，计算原始像素空间中的距离是一种相当糟糕的方法。用像素表示来比较两张图时，我们比较的是每个像素的灰度值与另一张图像对应位置的像素灰度值。这种表示与人们对人脸图像的解释方式有很大不同，使用这种原始表示很难获取到面部特征。例如，如果使用像素距离，那么将人脸向右移动一个像素将会发生巨大的变化，得到一个完全不同的表示。</strong>我们希望，使用沿着主成分方向的距离可以提高精度。这里我们启动PCA的<strong>白化</strong>（whitening）选项，它将主成分缩放到相同的尺度。变换后的结果与使用<code>StandardScaler</code>相同。白化不仅对应于旋转数据，还对应于缩放数据使其形状是圆形而不是椭圆。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_pca_whitening()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/pca-whitening.png"></p><p>我们对训练数据拟合PCA对象，并提取前100个主成分。然后对训练数据和测试数据进行变换。再对新表示使用单一最近邻分类器来将图像分类。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">100</span>, whiten=<span class="literal">True</span>, random_state=<span class="number">0</span>).fit(X_train)</span><br><span class="line">X_train_pca = pca.transform(X_train)</span><br><span class="line">X_test_pca = pca.transform(X_test)</span><br><span class="line">print(<span class="string">'X_train_pca.shape: &#123;&#125;'</span>.format(X_train_pca.shape))</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">knn.fit(X_train_pca, y_train)</span><br><span class="line">print(<span class="string">'Test set accuracy: &#123;:.2f&#125;'</span>.format(knn.score(X_test_pca, y_test)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test set accuracy: 0.31</span><br></pre></td></tr></table></figure><p>我们的精度有了相当显著的提高，这证实了我们的直觉，即<strong>主成分可能提供了一种更好的数据表示。</strong></p><p><strong>对于图像数据，我们还可以很容易地将找到的主成分可视化。请记住，成分对应于输入空间里的方向。</strong>这里的输入空间是87像素x65像素的灰度图像，所以在这个空间中的方向也是87像素x65像素的灰度图像。我们来看一下前几个主成分。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'pca.components_.shape: &#123;&#125;'</span>.format(pca.components_.shape))</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">12</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> i, (component, ax) <span class="keyword">in</span> enumerate(zip(pca.components_, axes.ravel())):</span><br><span class="line">    ax.imshow(component.reshape(image_shape), cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    ax.set_title(<span class="string">"&#123;&#125;. component"</span>.format(i + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pca.components_.shape: (100, 5655)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/visualization-of-image-pca.png"></p><p>虽然我们肯定无法理解这些成分的所有内容，但可以猜测一些主成分捕捉到了人脸图像的哪些方面。第一个主成分似乎主要编码的是人脸与背景的对比，第二个主成分编码的是人脸左半部分和右半部分的明暗程度差异，如此等等。虽然这种表示比原始像素值的语义稍强，但它仍与人们感知人脸的方式相去甚远。<strong>由于PCA模型是基于像素的，因此人脸的相对位置（眼睛、下巴和鼻子的位置）和明暗程度都对两张图像在像素表示中的相似程度有很大印象。但人脸的相对位置和明暗程度可能并不是人们首先感知的内容。</strong>在要求人们评价人脸的相似度时，他们更可能会使用年龄、性别、面部表情和发型等属性，而这些属性很难从像素强度中推断出来。重要的是要记住，算法对数据（特别是视觉数据，比如人们非常熟悉的图像）的解释通常与人类的解释方式有很大不同。</p><p>不过让我们回到PCA的具体案例。我们对PCA变换的介绍是：先旋转数据，然后删除方差较小的成分。另一种有用的解释是尝试找到一些数字（PCA旋转后的新特征值），使我们可以将测试点表示为主成分的加权求和。</p><p><img src="/uploads/image/introduction-to-ml-with-python/break-down-image-to-weighted-summation-of-pca.png"></p><p>这里<span class="math inline">\(x_0、x_1\)</span>等是这个数据点的主成分的系数，换句话说，它们是图像在旋转后的空间中的表示。</p><p>我们还可以用另一种方法来理解PCA模型。就是仅使用一些成分对原始数据进行重建。在<code>plot_pca_illustration</code>函数中，去掉第二个主成分，再反向旋转并加上平均值，这样就在原始空间中获得去掉第二个成分的新数据点。我们可以对人脸做类似的变换，将数据降维到只包含一些主成分，然后反向旋转回到原始空间。回到原始特征空间可以通过<code>inverse_transform(X)</code>方法来实现。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.decomposition.pca#sklearn.decomposition.PCA.inverse_transform" target="_blank" rel="noopener"><code>inverse_transform(X)</code></a></p><p>Transform data back to its original space.</p><p>In other words, return an input X_original whose transform would be X.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"><span class="comment"># 分别利用10个、50个、100个和500个成分对一些人脸进行重建并将其可视化</span></span><br><span class="line">mglearn.plots.plot_pca_faces(X_train, X_test, image_shape)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/pca-faces.png"></p><p>可以看到，在仅使用前10个主成分时，仅捕捉到了图片的基本特点，比如人脸方向和明暗程度。随着使用的主成分越来越多，图像中也保留了越来越多的细节。这对应于前一幅图的求和中包含越来越多的项。如果使用的成分个数与像素个数相等，意味着我们在旋转后不会丢弃任何信息，可以完美重建图像。</p><p>我们还可以尝试使用PCA的前两个主成分，将数据集中的所有人脸在散点图中可视化，其类别在图中给出。这与我们对cancer数据集所做的类似。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.discrete_scatter(X_train_pca[:, <span class="number">0</span>], X_train_pca[:, <span class="number">1</span>], y_train)</span><br><span class="line">plt.xlabel(<span class="string">'First principal component'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Second principal component'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/pca-faces-scatter.png"></p><p>如你所见，如果我们只使用前两个主成分，整个数据只是一大团，看不到类别之间的分界。这并不意外，因为即使有10个成分，PCA也仅捕捉到人脸非常粗略的特征。</p><h1 id="非负矩阵分解">非负矩阵分解</h1><p>非负矩阵分解（non-negative matrix factorization，NMF）是另一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似PCA，也可以用于降维。与PCA相同，我们试图将每个数据点写成一些分量的加权求和。但在PCA中，我们想要的是正交分量，并且能够解释尽可能多的数据方差；而在NMF中，我们希望分量和系数均为非负。因此，这种方法只能应用于每个特征都是非负的数据，因为非负分量的非负求和不可能变为负值。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.decomposition.nmf" target="_blank" rel="noopener"><code>class sklearn.decomposition.NMF(n_components=None, init=None, solver=’cd’, beta_loss=’frobenius’, tol=0.0001, max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0, verbose=0, shuffle=False)</code></a></p><p>Non-Negative Matrix Factorization (NMF)</p></blockquote><p>将数据分解成非负加权求和的这个过程，对由多个独立源相加（或叠加）创建而成的数据特别有用，比如多人说话的音轨或包含多种乐器的音乐。在这种情况下，NMF可以识别出组合合成数据的原始分量。总的来说，与PCA相比，NMF得到的分量更容易理解，因为负的分量和系数可能会导致难以理解的抵消效应（cancellation effect）。</p><h2 id="将nmf应用于模拟数据">将NMF应用于模拟数据</h2><p>与使用PCA不同，我们需要保证数据是正的，NMF能够对数据进行操作。这说明数据相对于原点(0, 0)的位置实际上对NMF很重要。因此，你可以将提取出来的非负分量看作是从(0, 0)到数据的方向。下面的例子给出了NMF在二维玩具数据上的结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_nmf_illustration()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/nmf.png"></p><p>对于两个分量的NMF（左图），显然所有数据点都可以写成这两个分量的正数组合。如果有足够多的分量能够完美地重建数据（分量个数与特征个数相同），那么算法会选择指向数据极值的方向。</p><p>如果我们仅使用一个分量，那么NMF会创建一个指向平均值的分量，因为指向这里可以对数据做出最好的解释。<strong>与PCA不同，减少分量个数不仅会删除一些方向，而且会创建一组完全不同的分量！NMF的分量也没有按任何特定方法排序，所以不存在“第一非负分量”：所有分量的地位平等。</strong></p><p>NMF使用了随机初始化，根据随机种子的不同可能会产生不同的结果。在相对简单的情况下（比如两个分量的模拟数据），所有数据都可以被完美地解释，那么随机性的影响很小（虽然可能会影响分量的顺序或尺度）。在更加复杂的情况下，影响可能会很大。</p><h2 id="将nmf应用于人脸图像">将NMF应用于人脸图像</h2><p>NMF的主要参数是我们想要提取的分量个数。通常来说，这个数字要小于输入特征的个数（否则的话，将每个像素作为单独的分量就可以对数据进行解释）。</p><p>首先，我们来观察分量个数如何影响NMF重建数据的好坏。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/nmf-faces.png"></p><p><strong>反向变换的数据质量与使用PCA时类似，但要稍差一些。这是符合预期的，因为PCA找到的是重建的最佳方向。NMF通常并不用于对数据进行重建或者编码，而是用于在数据中寻找有趣的模式。</strong></p><p>我们尝试仅提取一部分分量，初步观察一下数据。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> NMF</span><br><span class="line"></span><br><span class="line">nmf = NMF(n_components=<span class="number">15</span>, random_state=<span class="number">0</span>)</span><br><span class="line">nmf.fit(X_train)</span><br><span class="line">X_train_nmf = nmf.transform(X_train)</span><br><span class="line">X_test_nmf = nmf.transform(X_test)</span><br><span class="line"></span><br><span class="line">fix, axes = plt.subplots(<span class="number">3</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">12</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> i, (component, ax) <span class="keyword">in</span> enumerate(zip(nmf.components_, axes.ravel())):</span><br><span class="line">    ax.imshow(component.reshape(image_shape))</span><br><span class="line">    ax.set_title(<span class="string">'&#123;&#125;. component'</span>.format(i))</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/nmf-components.png"></p><p>这些分量都是正的，因此比PCA分量更像人脸模型。例如，你可以看到，分量3显示了稍微向右转动的人脸，而分量7则显示了稍微向左转动的人脸。我们来看一下这两个分量特别大的那些图像。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> compn <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">7</span>):</span><br><span class="line">    inds = np.argsort(X_train_nmf[:, compn])[::<span class="number">-1</span>]</span><br><span class="line">    fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">8</span>),</span><br><span class="line">        subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line">    <span class="keyword">for</span> i, (ind, ax) <span class="keyword">in</span> enumerate(zip(inds, axes.ravel())):</span><br><span class="line">        ax.imshow(X_train[ind].reshape(image_shape))</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/nmf-components-argsort-3.png"></p><p><img src="/uploads/image/introduction-to-ml-with-python/nmf-components-argsort-7.png"></p><p>正如所料，分量3系数较大的人脸都是向右看的人脸，而分量7系数较大的人脸都向左看。<strong>提取这样的模式最适合于具有叠加结构的数据，包括音频、基因表达和文本数据。</strong></p><p>我们通过一个模拟数据的例子来看一下这种用法。加入我们对一个信号感兴趣，它是三个不同信号源合成的。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">S = mglearn.datasets.make_signals()</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">1</span>))</span><br><span class="line">plt.plot(S, <span class="string">'-'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Signal'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/datasets-signals.png"></p><p>不幸的是，我们无法观测到原始信号，只能观测到三个信号的叠加混合。我们想要将混合信号分解成原始分量。假设我们有许多种不同的方法来观测信号（比如有100台测量装置），每种方法都为我们提供了一些列观测结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将数据混合成100维的状态</span></span><br><span class="line">A = np.random.RandomState(<span class="number">0</span>).uniform(size=(<span class="number">100</span>, <span class="number">3</span>))</span><br><span class="line">X = np.dot(S, A.T)</span><br><span class="line">print(<span class="string">'Shape of measurements: &#123;&#125;'</span>.format(X.shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用NMF来还原这三个信号</span></span><br><span class="line">nmf = NMF(n_components=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">S_ = nmf.fit_transform(X)</span><br><span class="line">print(<span class="string">'Recovered signal shape: &#123;&#125;'</span>.format(S_.shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了对比，应用PCA</span></span><br><span class="line">pca = PCA(n_components=<span class="number">3</span>)</span><br><span class="line">H = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给出NMF和PCA发现的信号活动</span></span><br><span class="line">models = [X, S, S_, H]</span><br><span class="line">names = [<span class="string">'Observations (first three measurements)'</span>, <span class="string">'True sources'</span>,</span><br><span class="line">    <span class="string">'NMF recovered signals'</span>, <span class="string">'PCA recovered signals'</span>]</span><br><span class="line">fig, axes = plt.subplots(<span class="number">4</span>, figsize=(<span class="number">8</span>, <span class="number">4</span>), gridspec_kw=&#123;<span class="string">'hspace'</span>: <span class="number">.5</span>&#125;,</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> model, name, ax <span class="keyword">in</span> zip(models, names, axes):</span><br><span class="line">    ax.set_title(name)</span><br><span class="line">    ax.plot(model[:, :<span class="number">3</span>], <span class="string">'-'</span>)  <span class="comment"># 绘制X时，仅包含100次中的前3次测量</span></span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/nmf-and-pca-on-signals.png"></p><p>可以看到，NMF在发现原始信号源时得到了不错的结果，而PCA则失败了，仅使用第一个成分来解释数据中的大部分变化。<strong>要记住，NMF生成的分量是没有顺序的。在这个例子中，NMF分量的顺序与原始信号完全相同（参见三条曲线的颜色），但这纯属偶然。</strong></p><p>还有许多其他算法可用于将每个数据点分解成一系列固定分量的加权求和，正如PCA和NMF所做的那样。如果你对这种类型的模式提取感兴趣，我们推荐你学习<code>scikit-learn</code>用户指南中关于独立成分分析（ICA）、因子分析（FA）和稀疏编码（字典学习）等的内容，所有这些内容都可以在关于分解方法的页面中找到（<a href="http://scikit-learn.org/stable/modules/decomposition.html" class="uri" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/decomposition.html</a>）。</p><h1 id="用t-sne进行流形学习">用t-SNE进行流形学习</h1><p>虽然PCA通常是用于变换数据的首选方法，使你能够用散点图将其可视化，但这一方法的性质（先旋转然后减少方向）限制了其有效性。有一类用于可视化的算法叫作<strong>流形学习算法</strong>（manifold learning algorithm），它允许进行更复杂的映射，通常也可以给出更好的可视化。其中特别有用的一个就是t-SNE（t-distributed Stochastic Neighbor Embedding）算法。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.manifold.tsne" target="_blank" rel="noopener"><code>class sklearn.manifold.TSNE(n_components=2, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric=’euclidean’, init=’random’, verbose=0, random_state=None, method=’barnes_hut’, angle=0.5)</code></a></p><p>t-distributed Stochastic Neighbor Embedding.</p></blockquote><p><strong>流形学习算法主要用于可视化，因此很少用来生成两个以上的新特征。其中一些算法（包括t-SNE）计算训练数据的一种新表示，但不允许变换新数据。这意味着这些算法不能用于测试集：更确切地说，它们只能变换用于训练的数据。</strong>流形学习对探索性数据分析是很有用的，但如果最终目标是监督学习的话，则很少使用。t-SNE背后的思想是找到数据的一个二维表示，尽可能地保持数据点之间的距离。t-SNE首先给出每个数据点的随机二维表示，然后尝试让在原始特征空间中距离较近的点更加靠近，原始特征空间中相距较远的点更加远离。t-SNE重点关注距离较近的点，而不是保持距离较远的点之间的距离。换句话说，它试图保存那些表示哪些点比较靠近的信息。</p><p>我们将对<code>scikit-learn</code>包含的一个手写数字数据集应用t-SNE流形学习算法。在这个数据集中，每个数据点都是0到9之间手写数字的一张8x8灰度图像。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> ax, img <span class="keyword">in</span> zip(axes.ravel(), digits.images):</span><br><span class="line">    ax.imshow(img)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/datasets-digits.png"></p><p>我们用PCA将降到二维的数据可视化。对前两个主成分作图，并按类别对数据点着色。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(digits.data)</span><br><span class="line">digits_pca = pca.transform(digits.data)</span><br><span class="line">colors = [<span class="string">"#476A2A"</span>, <span class="string">"#7851B8"</span>, <span class="string">"#BD3430"</span>, <span class="string">"#4A2D4E"</span>, <span class="string">"#875525"</span>,</span><br><span class="line">          <span class="string">"#A83683"</span>, <span class="string">"#4E655E"</span>, <span class="string">"#853541"</span>, <span class="string">"#3A3120"</span>, <span class="string">"#535D8E"</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">plt.xlim(digits_pca[:, <span class="number">0</span>].min(), digits_pca[:, <span class="number">0</span>].max())</span><br><span class="line">plt.ylim(digits_pca[:, <span class="number">1</span>].min(), digits_pca[:, <span class="number">1</span>].max())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(digits.data)):</span><br><span class="line">    <span class="comment"># 用每个类别对应的数字分别作为符号来显示每个类别的位置</span></span><br><span class="line">    plt.text(digits_pca[i, <span class="number">0</span>], digits_pca[i, <span class="number">1</span>], str(digits.target[i]),</span><br><span class="line">        color=colors[digits.target[i]], fontdict=&#123;<span class="string">'weight'</span>: <span class="string">'bold'</span>, <span class="string">'size'</span>: <span class="number">9</span>&#125;)</span><br><span class="line">plt.xlabel(<span class="string">'First principal component'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Second principal component'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/pca-digits.png"></p><p>利用前两个主成分可以将数字0、6和4相对较好地分开，尽管仍有重叠。大部分其他数字都大量重叠在一起。</p><p>我们将t-SNE应用于同一个数据集，并对结果进行比较。由于t-SNE不支持变换新数据，所以<code>TSNE</code>类没有transform方法。我们可以调用<code>fit_transform</code>方来来代替，它会构建模型并立即返回变换后的数据。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"></span><br><span class="line">tsne = TSNE(random_state=<span class="number">42</span>)</span><br><span class="line">digits_tsne = tsne.fit_transform(digits.data)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">plt.xlim(digits_tsne[:, <span class="number">0</span>].min(), digits_tsne[:, <span class="number">0</span>].max() + <span class="number">1</span>)</span><br><span class="line">plt.ylim(digits_tsne[:, <span class="number">1</span>].min(), digits_tsne[:, <span class="number">1</span>].max() + <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(digits.data)):</span><br><span class="line">    plt.text(digits_tsne[i, <span class="number">0</span>], digits_tsne[i, <span class="number">1</span>], str(digits.target[i]),</span><br><span class="line">        color=colors[digits.target[i]], fontdict=&#123;<span class="string">'weight'</span>: <span class="string">'bold'</span>, <span class="string">'size'</span>: <span class="number">9</span>&#125;)</span><br><span class="line">plt.xlabel(<span class="string">'t-SNE feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'t-SNE feature 1'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/t-sne-digits.png"></p><p>t-SNE的结果非常棒。所有类别都被明确分开。数字1和9被分成几块，但大多数类别都形成以一个密集的组。<strong>要记住，这种方法并不知道类别标签：它完全是无监督的。但它能够找到数据的一种二维表示，仅根据原始空间中数据点之间的靠近程度就能够将各个类别明确分开。</strong></p><p>t-SNE算法还有一些调节参数，虽然默认参数的效果通常就很好。你可以尝试修改<code>perplexity</code>和<code>early_exaggeration</code>，但作用一般很小。</p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/introduction-to-ml-with-python/preprocess-and-scaling/" rel="bookmark">预处理与缩放</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/introduction-to-ml-with-python/unsupervised-learning-and-preprocessing/" rel="bookmark">无监督学习与预处理</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/introduction-to-ml-with-python/clustering/" rel="bookmark">聚类</a></div></li></ul><footer class="post-footer"><div class="post-tags"> <a href="/tags/unsupervised/" rel="tag"># 无监督</a> <a href="/tags/preprocessing/" rel="tag"># 预处理</a></div><div class="post-nav"><div class="post-nav-item"><a href="/introduction-to-ml-with-python/preprocess-and-scaling/" rel="prev" title="预处理与缩放"><i class="fa fa-chevron-left"></i> 预处理与缩放</a></div><div class="post-nav-item"> <a href="/introduction-to-ml-with-python/clustering/" rel="next" title="聚类">聚类<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#主成分分析"><span class="nav-number">1.</span> <span class="nav-text">主成分分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#将pca应用于模拟数据"><span class="nav-number">1.1.</span> <span class="nav-text">将PCA应用于模拟数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#将pca应用于cancer数据集并可视化"><span class="nav-number">1.2.</span> <span class="nav-text">将PCA应用于cancer数据集并可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征提取的特征脸"><span class="nav-number">1.3.</span> <span class="nav-text">特征提取的特征脸</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#非负矩阵分解"><span class="nav-number">2.</span> <span class="nav-text">非负矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#将nmf应用于模拟数据"><span class="nav-number">2.1.</span> <span class="nav-text">将NMF应用于模拟数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#将nmf应用于人脸图像"><span class="nav-number">2.2.</span> <span class="nav-text">将NMF应用于人脸图像</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#用t-sne进行流形学习"><span class="nav-number">3.</span> <span class="nav-text">用t-SNE进行流形学习</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="菜农陈文生" src="/uploads/avatar/nekosensei.png"><p class="site-author-name" itemprop="name">菜农陈文生</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">255</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">24</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">108</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/cncws" title="GitHub → https://github.com/cncws" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a></span><span class="links-of-author-item"><a href="mailto:1031616423@qq.com" title="E-Mail → mailto:1031616423@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">菜农陈文生</span></div><script src="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="//cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script><script src="/js/aplayer-el.js"></script><script src="//unpkg.com/video.js/dist/video.min.js"></script><script src="/js/videojs-bilibili.js"></script><script src="/js/videojs-bilibili-el.js"></script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Axl15EIRi5o5AatKaxXxV4Oq-gzGzoHsz',
      appKey     : 'E0qm04UjsP0qQN1l8ME3GQ25',
      placeholder: "Just go go",
      avatar     : 'identicon',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script></div></body></html>
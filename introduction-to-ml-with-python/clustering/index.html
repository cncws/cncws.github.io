<!DOCTYPE html><html lang="zh-CN,en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/uploads/icon/drop/128x128.png"><link rel="icon" type="image/png" sizes="32x32" href="/uploads/icon/drop/32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/uploads/icon/drop/16x16.png"><link rel="mask-icon" href="/uploads/icon/drop/drop.svg" color="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="NKt2iJb3Hnl6-Sm7LB-fTT7LRyi9cg5yZrB-zd0ohtk"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"cwscn.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1,width:240},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!1,mediumzoom:!0,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"fadeIn"}},path:"search.xml"}</script><meta name="description" content="聚类（clustering）是将数据集划分成组的任务，这些组叫作簇（cluster）。其目标是划分数据，使得一个簇内的数据点非常相似且不同簇内的数据点非常不同。与分类算法类似，聚类算法为每个数据点分配（或预测）一个数字，表示这个点属于哪个簇。 k均值聚类 k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心（cluster center）。算法交替执行以下两个步骤：将每个数"><meta name="keywords" content="无监督,聚类"><meta property="og:type" content="article"><meta property="og:title" content="聚类"><meta property="og:url" content="https://cwscn.github.io/introduction-to-ml-with-python/clustering/index.html"><meta property="og:site_name" content="春夏秋冬"><meta property="og:description" content="聚类（clustering）是将数据集划分成组的任务，这些组叫作簇（cluster）。其目标是划分数据，使得一个簇内的数据点非常相似且不同簇内的数据点非常不同。与分类算法类似，聚类算法为每个数据点分配（或预测）一个数字，表示这个点属于哪个簇。 k均值聚类 k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心（cluster center）。算法交替执行以下两个步骤：将每个数"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-algorithm.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-boundaries.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-cluster-centers.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-more-or-less-cluster-centers.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-different-density.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-aspherical.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-on-two-moons.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-pca-nmf-extract-components.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-pca-nmf-reconstructions.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-on-two-moons-10-centers.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/agglomerative-algorithm.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/agglomerative-clustering-on-blobs.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/agglomerative-hierarchical-clustering.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/agglomerative-clustering-dendrogram.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/dbscan-different-parameters.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/dbscan-on-moons.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/ari-of-kmeans-agglomerative-clustering-and-dbscan.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/silhouette-score-of-kmeans-agglomerative-clustering-and-dbscan.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/noise-in-dbscan.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/clusters-of-7-eps-dbscan.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/cluster-centers-of-kmeans-faces.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-faces-close-and-far.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/agglomerative-clustering-dendrogram-on-faces.png"><meta property="og:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/clusters-of-agglomerative-clustering.png"><meta property="og:updated_time" content="2020-08-22T06:01:21.668Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="聚类"><meta name="twitter:description" content="聚类（clustering）是将数据集划分成组的任务，这些组叫作簇（cluster）。其目标是划分数据，使得一个簇内的数据点非常相似且不同簇内的数据点非常不同。与分类算法类似，聚类算法为每个数据点分配（或预测）一个数字，表示这个点属于哪个簇。 k均值聚类 k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心（cluster center）。算法交替执行以下两个步骤：将每个数"><meta name="twitter:image" content="https://cwscn.github.io/uploads/image/introduction-to-ml-with-python/kmeans-algorithm.png"><link rel="canonical" href="https://cwscn.github.io/introduction-to-ml-with-python/clustering/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><link rel="stylesheet" href="//unpkg.com/video.js/dist/video-js.min.css"><link rel="stylesheet" href="/css/videojs-bilibili.css"><style>.aplayer.aplayer-arrow .aplayer-icon-loop,.aplayer.aplayer-arrow .aplayer-icon-order{display:inline-block}</style><title>聚类 | 春夏秋冬</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">春夏秋冬</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">人有悲欢离合 月有阴晴圆缺</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档<span class="badge">301</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类<span class="badge">28</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签<span class="badge">118</span></a></li><li class="menu-item menu-item-收藏"><a href="/favlist/" rel="section"><i class="fa fa-star fa-fw"></i> 收藏</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div><meting-js server="netease" type="playlist" id="67155774" theme="#ff5555" loop="all" order="list" preload="none" volume="" mutex="" list-folded="NaN" fixed="true"></meting-js></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://cwscn.github.io/introduction-to-ml-with-python/clustering/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar/nekosensei.png"><meta itemprop="name" content="菜农陈文生"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="春夏秋冬"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 聚类</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-08-22 14:01:21" itemprop="dateModified" datetime="2020-08-22T14:01:21+08:00">2020-08-22</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/books/" itemprop="url" rel="index"><span itemprop="name">书籍</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/books/introduction-to-ml-with-python/" itemprop="url" rel="index"><span itemprop="name">Python 机器学习基础教程</span></a></span></span><span id="/introduction-to-ml-with-python/clustering/" class="post-meta-item leancloud_visitors" data-flag-title="聚类" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/introduction-to-ml-with-python/clustering/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/introduction-to-ml-with-python/clustering/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>聚类（clustering）是将数据集划分成组的任务，这些组叫作簇（cluster）。其目标是划分数据，使得一个簇内的数据点非常相似且不同簇内的数据点非常不同。与分类算法类似，聚类算法为每个数据点分配（或预测）一个数字，表示这个点属于哪个簇。</p><h1 id="k均值聚类">k均值聚类</h1><p>k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的<strong>簇中心</strong>（cluster center）。算法交替执行以下两个步骤：将每个数据点分配给最近的簇中心，然后将每个簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化，那么算法结束。下面的例子在一个模拟数据集上对这一算法进行说明。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_kmeans_algorithm()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-algorithm.png"></p><a id="more"></a><p>簇中心用三角形表示，而数据点用圆形表示。颜色表示簇成员。我们指定要寻找三个簇，所以通过声明三个随机数据点为簇中心来将算法初始化。然后开始迭代算法。首先每个数据点被分配给距离最近的簇中心。接下来，将簇中心修改为所分配点的平均值。然后将这一过程再重复两次。在第三次迭代之后，为簇中心分配的数据点保持不变，因此算法结束。</p><p>给定新的数据点，k均值会将其分配给最近的簇中心。下一例子展示了学到的簇中心的边界。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_kmeans_boundaries()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-boundaries.png"></p><p>用<code>scikit-learn</code>应用k均值相当简单。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.cluster.kmeans" target="_blank" rel="noopener"><code>class sklearn.cluster.KMeans(n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001, precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=’auto’)</code></a></p><p>K-Means clustering</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成模拟的二维数据</span></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 构建聚类模型</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">print(<span class="string">'Cluster memberships:\n&#123;&#125;'</span>.format(kmeans.labels_))</span><br><span class="line"><span class="comment"># 对训练集运行predict会返回与labels_相同的结果</span></span><br><span class="line">print(kmeans.predict(X))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cluster memberships:</span><br><span class="line">[0 2 2 2 1 1 1 2 0 0 2 2 1 0 1 1 1 0 2 2 1 2 1 0 2 1 1 0 0 1 0 0 1 0 2 1 2</span><br><span class="line"> 2 2 1 1 2 0 2 2 1 0 0 0 0 2 1 1 1 0 1 2 2 0 0 2 1 1 2 2 1 0 1 0 2 2 2 1 0</span><br><span class="line"> 0 2 1 1 0 2 0 2 2 1 0 0 0 0 2 0 1 0 0 2 2 1 1 0 1 0]</span><br><span class="line">[0 2 2 2 1 1 1 2 0 0 2 2 1 0 1 1 1 0 2 2 1 2 1 0 2 1 1 0 0 1 0 0 1 0 2 1 2</span><br><span class="line"> 2 2 1 1 2 0 2 2 1 0 0 0 0 2 1 1 1 0 1 2 2 0 0 2 1 1 2 2 1 0 1 0 2 2 2 1 0</span><br><span class="line"> 0 2 1 1 0 2 0 2 2 1 0 0 0 0 2 0 1 0 0 2 2 1 1 0 1 0]</span><br></pre></td></tr></table></figure><p>因为要们要找的是3个簇，所有簇的编号是0到2。你也可以用predict方法为新数据点分配标签。预测时会将最近的簇中心分配个每个新数据点，但现有模型不会改变（即不会更新簇中心的位置）。</p><p>与分类算法类似，聚类算法的每个元素也有一个标签。但并不存在真实的标签，因此标签本身并没有<strong>先验</strong>意义。回到人脸图像聚类的例子。聚类的结果可能是，算法找到的第3个簇仅包含你朋友Bela的面孔。但只有在查看图片之后才能知道这一点，而且数字3是任意的。算法给你的唯一信息就是所有标签为3的人脸都是相似的。</p><p>对于我们刚刚在二维玩具数据集上运行的聚类算法，这意味着我们不应该为其中一组的标签是0、另一组的标签是1这一事实赋予任何意义。再次运行该算法可能会导致不同的簇编号，原因在于初始化的随机性质。</p><p>下面又给出了这个数据的图像。簇中心被保存在<code>cluster_centers_</code>属性中，我们用三角形表示它们。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], kmeans.labels_, markers=<span class="string">'o'</span>)</span><br><span class="line">mglearn.discrete_scatter(kmeans.cluster_centers_[:, <span class="number">0</span>],</span><br><span class="line">    kmeans.cluster_centers_[:, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], markers=<span class="string">'^'</span>, markeredgewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-cluster-centers.png"></p><p>我们也可以使用更多或更少的簇中心。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 使用2个簇中心</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">2</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">assignments = kmeans.labels_</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], assignments, ax=axes[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 使用5个簇中心</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">5</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">assignments = kmeans.labels_</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], assignments, ax=axes[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-more-or-less-cluster-centers.png"></p><h2 id="k均值的失败案例">k均值的失败案例</h2><p>即使你知道给定数据集中簇的“正确”个数，k均值可能也不是总能找到它们。<strong>每个簇仅有其中心定义，这意味着每个簇都是凸形（convex）。因此，k均值只能找到相对简单的形状。</strong>k均值还假设所有簇在某种程度上具有相同的“直径”，它总是将簇之间的边界刚好画在簇中心的中间位置。有时这会导致令人惊讶的结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_varied, y_varied = make_blobs(n_samples=<span class="number">200</span>, cluster_std=[<span class="number">1.0</span>, <span class="number">2.5</span>, <span class="number">0.5</span>],</span><br><span class="line">    random_state=<span class="number">170</span>)</span><br><span class="line">y_pred = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>).fit_predict(X_varied)</span><br><span class="line"></span><br><span class="line">mglearn.discrete_scatter(X_varied[:, <span class="number">0</span>], X_varied[:, <span class="number">1</span>], y_pred)</span><br><span class="line">plt.legend([<span class="string">'cluster 0'</span>, <span class="string">'cluster 1'</span>, <span class="string">'cluster 2'</span>], loc=<span class="string">'best'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-different-density.png"></p><p>你可能会认为，左下方的密集区域是第一个簇，右上方的密集区域是第二个，中间密度较小的区域是第三个。但事实上，簇0和簇1都包含一些远离簇中其他点的点。</p><p>k均值还假设所有方向对每个簇都同等重要。下图显示了一个二维数据集，数据中包含明确分开的三部分。但是这三部分被沿着对角线方向拉长。由于k均值仅考虑到最近簇中心的距离，所以它无法处理这种类型的数据。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成一些随机分组数据</span></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">170</span>, n_samples=<span class="number">600</span>)</span><br><span class="line">rng = np.random.RandomState(<span class="number">74</span>)</span><br><span class="line"><span class="comment"># 变换数据使其拉长</span></span><br><span class="line">transformation = rng.normal(size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">X = np.dot(X, transformation)</span><br><span class="line"><span class="comment"># 将数据聚类成3个簇</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">y_pred = kmeans.predict(X)</span><br><span class="line"><span class="comment"># 画出簇分配和簇中心</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred, cmap=mglearn.cm3)</span><br><span class="line">plt.scatter(kmeans.cluster_centers_[:, <span class="number">0</span>], kmeans.cluster_centers_[:, <span class="number">1</span>],</span><br><span class="line">    marker=<span class="string">'^'</span>, c=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], s=<span class="number">100</span>, linewidth=<span class="number">2</span>, cmap=mglearn.cm3)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-aspherical.png"></p><p>如果簇的形状更加复杂，那么k均值的表现也很差。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成模拟的two_moons数据</span></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 将数据聚类成2个簇</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">2</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">y_pred = kmeans.predict(X)</span><br><span class="line"><span class="comment"># 画出簇分配和簇中心</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred, cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">plt.scatter(kmeans.cluster_centers_[:, <span class="number">0</span>], kmeans.cluster_centers_[:, <span class="number">1</span>],</span><br><span class="line">    marker=<span class="string">'^'</span>, c=[mglearn.cm2(<span class="number">0</span>), mglearn.cm2(<span class="number">1</span>)], s=<span class="number">100</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-on-two-moons.png"></p><p>这里我们希望聚类算法能够发现两个半月形。但利用k均值算法是不可能做到这一点的。</p><h2 id="矢量量化或者将k均值看作分解">矢量量化，或者将k均值看作分解</h2><p>虽然k均值是一种聚类算法，但在k均值和分解方法（比如PCA和NMF）之间存在一些有趣的相似之处。PCA试图找到数据中方差最大的方向，而NMF试图找到累加的分量，这通常对应于数据的“极值”或“部分”。两种方法都试图将数据点表示为一些分量之和。与之相反，k均值则尝试利用簇中心来表示每个数据点。你可以将其看作仅用一个分量来表示每个数据点，该分量由簇中心给出。这种观点将k均值看作是一种分解方法，其中每个点用单一分量来表示，这种观点被称为<strong>矢量量化</strong>（vector quantization）。</p><p>我们来并排比较PCA、NMF和k均值，分别显示提取的分量，以及利用100个分量对测试集中人脸的重建。对于k均值，重建就是在训练集中找到最近的簇中心。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_people, y_people, stratify=y_people, random_state=<span class="number">0</span>)</span><br><span class="line">nmf = NMF(n_components=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br><span class="line">nmf.fit(X_train)</span><br><span class="line">pca = PCA(n_components=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br><span class="line">pca.fit(X_train)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br><span class="line">kmeans.fit(X_train)</span><br><span class="line"></span><br><span class="line">X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))</span><br><span class="line">X_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)</span><br><span class="line">X_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">5</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line">fig.suptitle(<span class="string">'Extracted Components'</span>)</span><br><span class="line"><span class="keyword">for</span> ax, comp_kmeans, comp_pca, comp_nmf <span class="keyword">in</span> zip(</span><br><span class="line">    axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):</span><br><span class="line">    ax[<span class="number">0</span>].imshow(comp_kmeans.reshape(image_shape))</span><br><span class="line">    ax[<span class="number">1</span>].imshow(comp_pca.reshape(image_shape), cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    ax[<span class="number">2</span>].imshow(comp_nmf.reshape(image_shape))</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].set_ylabel(<span class="string">'kmeas'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">0</span>].set_ylabel(<span class="string">'pca'</span>)</span><br><span class="line">axes[<span class="number">2</span>, <span class="number">0</span>].set_ylabel(<span class="string">'nmf'</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">4</span>, <span class="number">5</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line">fig.suptitle(<span class="string">'Reconstructions'</span>)</span><br><span class="line"><span class="keyword">for</span> ax, org, rec_kmeans, rec_pca, rec_nmf <span class="keyword">in</span> zip(axes.T, X_test,</span><br><span class="line">    X_reconstructed_kmeans, X_reconstructed_pca, X_reconstructed_nmf):</span><br><span class="line">    ax[<span class="number">0</span>].imshow(org.reshape(image_shape))</span><br><span class="line">    ax[<span class="number">1</span>].imshow(rec_kmeans.reshape(image_shape))</span><br><span class="line">    ax[<span class="number">2</span>].imshow(rec_pca.reshape(image_shape))</span><br><span class="line">    ax[<span class="number">3</span>].imshow(rec_nmf.reshape(image_shape))</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].set_ylabel(<span class="string">'original'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">0</span>].set_ylabel(<span class="string">'kmeas'</span>)</span><br><span class="line">axes[<span class="number">2</span>, <span class="number">0</span>].set_ylabel(<span class="string">'pca'</span>)</span><br><span class="line">axes[<span class="number">3</span>, <span class="number">0</span>].set_ylabel(<span class="string">'nmf'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-pca-nmf-extract-components.png"></p><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-pca-nmf-reconstructions.png"></p><p><strong>利用k均值做矢量量化的一个有趣之处在于，可以用比输入维度更多的簇来对数据进行编码</strong>让我们回到<code>two_moons</code>数据。利用PCA或NMF，我们对这个数据无能为力，因为它只有两个维度。使用PCA或NMF将其降到一维，将会完全破坏数据的结构。但通过使用更多的簇中心，我们可以用k均值找到一种更具表现力的表示。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">y_pred = kmeans.predict(X)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred, s=<span class="number">60</span>, cmap=<span class="string">'Paired'</span>)</span><br><span class="line">plt.scatter(kmeans.cluster_centers_[:, <span class="number">0</span>], kmeans.cluster_centers_[:, <span class="number">1</span>],</span><br><span class="line">    s=<span class="number">60</span>, marker=<span class="string">'^'</span>, c=range(kmeans.n_clusters), linewidth=<span class="number">2</span>, cmap=<span class="string">'Paired'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br><span class="line">print(<span class="string">'Cluster memberships:\n&#123;&#125;'</span>.format(y_pred))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cluster memberships:</span><br><span class="line">[9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0</span><br><span class="line"> 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5</span><br><span class="line"> 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1</span><br><span class="line"> 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6</span><br><span class="line"> 3 5 0 2 9 3 0 8 6 0 3 3 5 6 3 2 0 2 3 0 2 6 3 4 4 1 5 6 7 1 1 3 2 4 7 2 7</span><br><span class="line"> 3 8 6 4 1 4 3 9 9 5 1 7 5 8 2]</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-on-two-moons-10-centers.png"></p><p>我们使用了10个簇中心，也就是说，现在每个点都被分配了0到9之间的一个数字。我们可以将其看作10个分量表示的数据（我们有是10个新特征），只有表示该点对应的簇中心的那个特征不为0，其他特征均为0。利用这个10维表示，现在可以用线性模型来划分两个半月形，而利用原始的两个特征是不可能做到这一点的。将到每个簇中心的距离作为特征，还可以得到一种表现力更强的数据表示。可以利用kmeans的transform方法来完成这一点。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distance_features = kmeans.transform(X)</span><br><span class="line">print(<span class="string">'Distance feature shape: &#123;&#125;'</span>.format(distance_features.shape))</span><br><span class="line">print(<span class="string">'Distance features:\n&#123;&#125;'</span>.format(distance_features))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Distance feature shape: (200, 10)</span><br><span class="line">Distance features:</span><br><span class="line">[[0.9220768  1.46553151 1.13956805 ... 1.16559918 1.03852189 0.23340263]</span><br><span class="line"> [1.14159679 2.51721597 0.1199124  ... 0.70700803 2.20414144 0.98271691]</span><br><span class="line"> [0.78786246 0.77354687 1.74914157 ... 1.97061341 0.71561277 0.94399739]</span><br><span class="line"> ...</span><br><span class="line"> [0.44639122 1.10631579 1.48991975 ... 1.79125448 1.03195812 0.81205971]</span><br><span class="line"> [1.38951924 0.79790385 1.98056306 ... 1.97788956 0.23892095 1.05774337]</span><br><span class="line"> [1.14920754 2.4536383  0.04506731 ... 0.57163262 2.11331394 0.88166689]]</span><br></pre></td></tr></table></figure><p>k均值是非常流行的聚类算法，因为它不仅相对容易理解和实现，而且运行速度也相对较快。k均值可以轻松扩展到大型数据集，<code>scikit-learn</code>甚至在<code>MiniBatchKMeans</code>类中包含了一种更具扩展性的变体，可以处理非常大的数据集。</p><p>k均值的缺点之一在于，它依赖于随机初始化，也就是说，算法的输出依赖于随机种子。默认情况下，<code>scikit-learn</code>用10种不同的随机初始化将算法运行10次，并返回最佳（簇的方差之和最小）结果。k均值还有一个缺点，就是对簇形状的假设的约束性较强，而且还要求指定所要寻找的簇的个数（在现实世界的应用中可能并不知道这个数字）。</p><p>接下来，我们将学习另外两种聚类算法，它们都在某些方面对这些性质做了改进。</p><h1 id="凝聚聚类">凝聚聚类</h1><p>凝聚聚类（agglomerative clustering）指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直到满足某种停止准则为止。<code>scikit-learn</code>中实现的停止准则是簇的个数，因此相似的簇被合并，直到仅剩下指定个数的簇。还有一些链接（linkage）准则，规定如何度量“最相似的簇”。这种度量总是定义在两个现有的簇之间。</p><p><code>scikit-learn</code>中实现了以下三种选项。</p><ul><li><p>ward</p><p>默认选项。ward挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。</p></li><li><p>average</p><p>average链接将簇中所有点之间平均距离最小的两个簇合并。</p></li><li><p>complete</p><p>complete链接（也称为最大链接）将簇中点之间最大距离最小的两个簇合并。</p></li></ul><p>ward适用于大多数数据集，在我们的例子中将使用它。如果簇中的成员个数非常不同（比如其中一个比其他所有都大得多），那么average或complete可能效果更好。</p><p>下图给出了在一个二维数据集上的凝聚聚类过程，要寻找三个簇。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_agglomerative_algorithm()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/agglomerative-algorithm.png"></p><p>我们来看一下凝聚聚类对简单三簇数据的效果如何。由于算法的工作原理，凝聚算法不能对新数据点作出预测。因此<code>AgglomerativeClustering</code>没有predict方法。为了构造模型并得到训练集上簇的成员表示，可以改用<code>fit_predict</code>方法，或使用<code>labels_</code>属性。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.cluster.agglomerativeclustering" target="_blank" rel="noopener"><code>class sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity=’euclidean’, memory=None, connectivity=None, compute_full_tree=’auto’, linkage=’ward’, pooling_func=’deprecated’)</code></a></p><p>Agglomerative Clustering</p><p>Recursively merges the pair of clusters that minimally increases a given linkage distance.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">1</span>)</span><br><span class="line">agg = AgglomerativeClustering(n_clusters=<span class="number">3</span>)</span><br><span class="line">assignment = agg.fit_predict(X)</span><br><span class="line"></span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], assignment)</span><br><span class="line">plt.legend([<span class="string">'Cluster 0'</span>, <span class="string">'Cluster 1'</span>, <span class="string">'Cluster 1'</span>], loc=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/agglomerative-clustering-on-blobs.png"></p><p>正如缩小，算法完美地完成了聚类。虽然凝聚聚类的<code>scikit-learn</code>实现需要你指定希望算法找到的簇的个数，但凝聚聚类方法为选择正确的个数提供了一些帮助，我们将在下面讨论。</p><h2 id="层次聚类与树状图">层次聚类与树状图</h2><p>凝聚聚类生成了所谓的层次聚类（hierarchical clustering）。聚类过程迭代进行，每个点都从一个单点簇变为属于最终的某个簇。每个中间步骤都提供了数据的一种聚类（簇的个数也不相同）。有时候，同时查看所有可能的聚类是有帮助的。下一个例子叠加显示了所有可能的聚类，有助于深入了解每个簇如何分解为较小的簇。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_agglomerative()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/agglomerative-hierarchical-clustering.png"></p><p>虽然这种可视化为层次聚类提供了非常详细的试图，但它依赖于数据的二维性质，因此不能用于具有两个以上特征的数据集。但还有另一个将层次聚类可视化的工具，叫作树状图（dendrogram），它可以处理多维数据集。</p><p>不幸的是，目前<code>scikit-learn</code>没有绘制树状图的功能。但你可以利用SciPy轻松生成树状图。SciPy的聚类算法接口与<code>scikit-learn</code>的聚类算法稍有不同。SciPy提供了一个函数，接受数据数组X并计算出一个链接数据（linkage array），它对层次聚类的相似度进行编码。然后我们可以将这个链接数据提供给scipy的dendrogram函数来绘制树状图。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从SciPy中导入dendrogram函数和ward聚类函数</span></span><br><span class="line"><span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> dendrogram, ward</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">0</span>, n_samples=<span class="number">12</span>)</span><br><span class="line"><span class="comment"># 将ward聚类应用于数据数组X</span></span><br><span class="line"><span class="comment"># SciPy的ward函数返回一个数组，指定执行凝聚聚类时跨越的距离</span></span><br><span class="line">linkage_array = ward(X)</span><br><span class="line"><span class="comment"># 现在为包含簇之间距离的linkage_array绘制树状图</span></span><br><span class="line">dendrogram(linkage_array)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在树中标记划分成两个簇或三个簇的位置</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">bounds = ax.get_xbound()</span><br><span class="line">ax.plot(bounds, [<span class="number">7.25</span>, <span class="number">7.25</span>], <span class="string">'--'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">ax.plot(bounds, [<span class="number">4</span>, <span class="number">4</span>], <span class="string">'--'</span>, c=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line">ax.text(bounds[<span class="number">1</span>], <span class="number">7.25</span>, <span class="string">' two clusters'</span>, va=<span class="string">'center'</span>, fontdict=&#123;<span class="string">'size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">ax.text(bounds[<span class="number">1</span>], <span class="number">4</span>, <span class="string">' three clusters'</span>, va=<span class="string">'center'</span>, fontdict=&#123;<span class="string">'size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.xlabel(<span class="string">'Sample index'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cluster distance'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/agglomerative-clustering-dendrogram.png"></p><p>树状图在底部显示数据点（编号从0到11）。然后以这些点（表示单点簇）作为叶节点绘制一棵树，每合并两个簇就添加一个新的父节点。</p><p>从下往上看，数据点1和4首先被合并。接下来，点6和9被合并为一个簇，以此类推。在顶层有两个分支，一个由点11、0、5、10、7、6和9组成，另一个由点1、4、3、2和8组成。这对应于图中两个最大的簇。</p><p>树状图的y轴不仅说明凝聚算法中两个簇何时合并，每个分支的长度还表示被合并的簇之间的距离。在这张树状图中，最长的分支是标记为“three clusters”的虚线表示的三条线。它们是最长的分支，这表示从三个簇到两个簇的过程中合并了一些距离非常远的点。将剩下的两个簇合并为一个簇也需要跨越相对较大的距离。</p><p>不幸的是，凝聚算法仍然无法分离像<code>two_moons</code>数据集这样复杂的形状。但我们要学习的下一个算法DBSCAN可以解决这个问题。</p><h1 id="dbscan">DBSCAN</h1><p>另一个非常有用的聚类算法是DBSCAN（density-based spatial clustering of applications with noise，即“具有噪声的基于密度的空间聚类应用”）。DBSCAN的主要优点是它不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇，还可以找出不属于任何簇的点。DBSCAN比凝聚聚类和k均值稍慢，但仍可以扩展到相对较大的数据集。</p><p>DBSCAN的原理是识别特征空间的“拥挤”区域中的点，在这些区域中许多数据点靠近在一起。这些区域被称为特征空间中的密集（dense）区域。DBSCAN背后的思想是，簇形成数据的密集区域，并由相对较空的区域分隔开。</p><p>在密集区域内的点被称为<strong>核心样本</strong>（core sample，或核心点），它们的定义如下。DBSCAN有两个参数：<code>min_samples</code>和<code>eps</code>。如果距一个给定数据点<code>eps</code>的距离至少有<code>min_samples</code>个数据点，那么这个数据点就是核心样本。DBSCAN将彼此距离小于<code>eps</code>的核心样本放到同一个簇中。</p><p>算法首先任意选取一个点，然后找到这个点的距离小于等于eps的所有的点。如果距起始点的距离在eps之内的数据点个数小于<code>min_samples</code>，那么这个点被标记为<strong>噪声</strong>（noise），也就是说它不属于任何簇。如果距离在eps之内的数据点个数大于<code>min_samples</code>，则这个点被标记为核心样本，并被分配一个新的簇标签。然后访问该点的所有邻居（在距离eps之内）。如果它们还没有被分配一个簇，那么就将刚刚创建的新的簇标签分配给它们。如果它们是核心样本，那么就依次访问其邻居，以此类推。簇逐渐增大，直到在簇的eps距离内没有更多的核心样本为止。然后选取另一个尚未被访问过的点，并重复相同的过程。</p><p><strong>最后，一共有三种类型的点：核心点，与核心点的距离在eps之内的点（叫作边界点，boundary point）和噪声。如果DBSCAN算法在特定数据集上多次运行，那么核心点的聚类始终相同，同样的点也始终被标记为噪声。但边界点可能与不止一个簇的核心样本相邻。因此，边界点所属的簇依赖于数据点的访问顺序。一般来说只有很少的边界点，这种对访问顺序的轻度依赖并不重要。</strong></p><p>我们将DBSCAN应用于演示凝聚聚类的模拟数据集。与凝聚聚类类似，DBSCAN也不允许对新的测试数据进行预测，所以我们将使用<code>fit_predict</code>方法来执行聚类并返回簇标签。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.cluster.dbscan" target="_blank" rel="noopener"><code>class sklearn.cluster.DBSCAN(eps=0.5, min_samples=5, metric=’euclidean’, metric_params=None, algorithm=’auto’, leaf_size=30, p=None, n_jobs=None)</code></a></p><p>Perform DBSCAN clustering from vector array or distance matrix.</p><p>DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">0</span>, n_samples=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">dbscan = DBSCAN()</span><br><span class="line">clusters = dbscan.fit_predict(X)</span><br><span class="line">print(<span class="string">'Cluster memberships:\n&#123;&#125;'</span>.format(clusters))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cluster memberships:</span><br><span class="line">[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]</span><br></pre></td></tr></table></figure><p>所有数据点都被分配了标签-1，这代表噪声。这是eps和<code>min_samples</code>默认参数设置的结果，对于小型的玩具数据集并没有调节这些参数。<code>min_samples</code>和eps取不同值时的簇分类如下所示。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_dbscan()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">min_samples: 2 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]</span><br><span class="line">min_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]</span><br><span class="line">min_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]</span><br><span class="line">min_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">min_samples: 3 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]</span><br><span class="line">min_samples: 3 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]</span><br><span class="line">min_samples: 3 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]</span><br><span class="line">min_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">min_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]</span><br><span class="line">min_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]</span><br><span class="line">min_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]</span><br><span class="line">min_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/dbscan-different-parameters.png"></p><p>在这张图中，属于簇的点是实心的，而噪声点则显示为空心的。核心样本显示为较大的标记，而边界点则显示为较小的标记。增大eps（在图中从左到右），更多的点会被包含在一个簇中。这让簇变大，但可能也会导致多个簇合并成一个。增大<code>min_samples</code>（在图中从上到下），核心点会变得更少，更多的点被标记为噪声。</p><p>参数eps在某种程度上更加重要，因为它决定了点与点之间“接近”的含义。将eps设置的非常小，意味着没有点是核心样本，可能会导致所有点都被标记为噪声。将eps设置的非常大，可能会导致所有点形成单个簇。</p><p>设置<code>min_samples</code>主要是为了判断稀疏区域内的点被标记为异常值还是形成自己的簇。如果增大<code>min_samples</code>，任何一个包含少于<code>min_samples</code>个样本的簇现在将被标记为噪声。因此，<strong><code>min_samples</code>决定簇的最小尺寸。</strong>在上图中你可以清楚地看到这一点。</p><p><strong>虽然DBSCAN不需要显式地设置簇的个数，但设置eps可以隐式地控制找到的簇的个数。</strong>使用StandardScaler或MinMaxScaler对数据进行缩放之后，有时会更容易找到eps的较好取值，因为使用这些缩放技术将确保所有特征具有相似的范围。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据缩放成平均值为0、方差为1</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(X)</span><br><span class="line">X_scaled = scaler.transform(X)</span><br><span class="line"></span><br><span class="line">dbscan = DBSCAN()</span><br><span class="line">clusters = dbscan.fit_predict(X_scaled)</span><br><span class="line"><span class="comment"># 绘制簇分配</span></span><br><span class="line">plt.scatter(X_scaled[:, <span class="number">0</span>], X_scaled[:, <span class="number">1</span>], c=clusters,</span><br><span class="line">    cmap=mglearn.cm2, s=<span class="number">60</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/dbscan-on-moons.png"></p><p>由于算法找到了我们想要的簇的个数（2个），因此参数设置的效果似乎很好。如果将eps减小到0.2（默认值为0.5），我们将会得到8个簇，这显然太多了。将eps增大到0.7则会导致只有一个簇。</p><p>在使用DBSCAN时，你需要谨慎处理返回的簇分配。如果使用簇标签对另一个数据进行索引，那么使用-1表示噪声可能会产生意料之外的结果。</p><h1 id="聚类算法的对比与评估">聚类算法的对比与评估</h1><p>在应用聚类算法时，其挑战之一就是很难评估一个算法的效果好坏，也很难比较不同算法的结果。</p><h2 id="用真实值评估聚类">用真实值评估聚类</h2><p>有一些指标可用于评估聚类算法相对于真实聚类的结果，其中最重要的是<strong>调整rand指数</strong>（adjusted rand index，ARI）和<strong>归一化互信息</strong>（normalized mutual information，NMI），二者都给出了定量的度量，其最佳值为1，0表示不相关的聚类（虽然ARI可以取负值）。</p><p>下面我们使用ARI来比较k均值、凝聚聚类和DBSCAN算法。对了对比，我们还添加了将点随机分配到两个簇中的图像。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.metrics.adjusted_rand_score" target="_blank" rel="noopener"><code>sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)</code></a></p><p>Rand index adjusted for chance.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> adjusted_rand_score</span><br><span class="line"></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 将数据缩放成平均值为0、方差为1</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(X)</span><br><span class="line">X_scaled = scaler.transform(X)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, figsize=(<span class="number">15</span>, <span class="number">3</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="comment"># 要使用的算法</span></span><br><span class="line">algorithms = [KMeans(n_clusters=<span class="number">2</span>), AgglomerativeClustering(n_clusters=<span class="number">2</span>),</span><br><span class="line">    DBSCAN() ]</span><br><span class="line"><span class="comment"># 创建一个随机的簇分配，作为参考</span></span><br><span class="line">random_state = np.random.RandomState(seed=<span class="number">0</span>)</span><br><span class="line">random_clusters = random_state.randint(low=<span class="number">0</span>, high=<span class="number">2</span>, size=len(X))</span><br><span class="line"><span class="comment"># 绘制随机分配</span></span><br><span class="line">axes[<span class="number">0</span>].scatter(X_scaled[:, <span class="number">0</span>], X_scaled[:, <span class="number">1</span>], c=random_clusters,</span><br><span class="line">    cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">'Random assignment - ARI: &#123;:.2f&#125;'</span>.format(</span><br><span class="line">    adjusted_rand_score(y, random_clusters)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax, algorithm <span class="keyword">in</span> zip(axes[<span class="number">1</span>:], algorithms):</span><br><span class="line">    clusters = algorithm.fit_predict(X_scaled)</span><br><span class="line">    ax.scatter(X_scaled[:, <span class="number">0</span>], X_scaled[:, <span class="number">1</span>], c=clusters,</span><br><span class="line">    cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">    ax.set_title(<span class="string">'&#123;&#125; - ARI: &#123;:.2f&#125;'</span>.format(</span><br><span class="line">        algorithm.__class__.__name__, adjusted_rand_score(y, clusters)))</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/ari-of-kmeans-agglomerative-clustering-and-dbscan.png"></p><p>ARI给出了符合直觉的结果，随机簇分配的分数为0，而DBSCAN（完美地找到了期望中的簇）的分数为1。</p><p>用这种方式评估聚类时，一个常见的错误是使用<code>accuracy_score</code>而不是<code>adjusted_rand_score</code>、<code>normalized_mutual_info_score</code>或其他聚类指标。<strong>使用精度的问题在于，它要求分配的簇标签与真实值完全匹配。但簇标签本身毫无意义——唯一重要的是哪些点位于同一个簇中。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这两种点标签对应于相同的聚类</span></span><br><span class="line">clusters1 = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">clusters2 = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy: &#123;:.2f&#125;'</span>.format(accuracy_score(clusters1, clusters2)))</span><br><span class="line">print(<span class="string">'ARI: &#123;:.2f&#125;'</span>.format(adjusted_rand_score(clusters1, clusters2)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Accuracy: 0.00</span><br><span class="line">ARI: 1.00</span><br></pre></td></tr></table></figure><h2 id="在没有真实值的情况下评估聚类">在没有真实值的情况下评估聚类</h2><p>在实践中，使用诸如ARI之类的指标有一个很大的问题。<strong>在应用聚类算法时，通常没有真实值来比较结果。</strong>如果我们知道了数据的正确聚类，那么可以使用这一信息构建一个监督模型（比如分类器）。因此，使用类似ARI和NMI的指标通常仅有助于开发算法，但对评估应用是否成功没有帮助。</p><p>有一些聚类的评分指标不需要真实值，比如<strong>轮廓系数</strong>（silhouette coefficient）。但它们在实践中的效果并不好。轮廓分数计算一个簇的紧致度，其值越大越好，最高分数为1。<strong>虽然紧致的簇很好，但紧致度不允许复杂的形状。</strong></p><p>下面是一个例子，利用轮廓分数在<code>two_moons</code>数据集上比较k均值、凝聚聚类和DBSCAN。</p><blockquote><p><a href="https://devdocs.io/scikit_learn/modules/generated/sklearn.metrics.silhouette_score" target="_blank" rel="noopener"><code>sklearn.metrics.silhouette_score(X, labels, metric=’euclidean’, sample_size=None, random_state=None, **kwds)</code></a></p><p>Compute the mean Silhouette Coefficient of all samples.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> silhouette_score</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, figsize=(<span class="number">15</span>, <span class="number">3</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="comment"># 绘制随机分配</span></span><br><span class="line">axes[<span class="number">0</span>].scatter(X_scaled[:, <span class="number">0</span>], X_scaled[:, <span class="number">1</span>], c=random_clusters,</span><br><span class="line">    cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">'Random assignment: &#123;:.2f&#125;'</span>.format(</span><br><span class="line">    silhouette_score(X_scaled, random_clusters)))</span><br><span class="line"><span class="keyword">for</span> ax, algorithm <span class="keyword">in</span> zip(axes[<span class="number">1</span>:], algorithms):</span><br><span class="line">    clusters = algorithm.fit_predict(X_scaled)</span><br><span class="line">    ax.scatter(X_scaled[:, <span class="number">0</span>], X_scaled[:, <span class="number">1</span>], c=clusters,</span><br><span class="line">    cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">    ax.set_title(<span class="string">'&#123;&#125;: &#123;:.2f&#125;'</span>.format(</span><br><span class="line">        algorithm.__class__.__name__, silhouette_score(X_scaled, clusters)))</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/silhouette-score-of-kmeans-agglomerative-clustering-and-dbscan.png"></p><p>k均值的轮廓分数最高，尽管我们可能更喜欢DBSCAN的结果。对于评估聚类，稍好的策略是使用<strong>基于鲁棒性的</strong>（robustness-based）聚类指标。这种指标优先向数据中添加一些噪声，或者使用不同的参数设定，然后运行算法，并对结果进行比较。其思想是，如果许多算法参数和许多数据扰动返回相同的结果，那么它很可能是可信的。不幸的是，<code>scikit-learn</code>还没有实现这一策略。</p><p>即使我们得到一个鲁棒性很好的聚类或者非常高的轮廓分数，但仍然不知道聚类中是否有任何语义含义，或者聚类是否反映了数据中我们感兴趣的某个方面。回到人脸图像的例子。我们希望找到类似人脸的分组，比如男人和女人、老人和年轻人，或者有胡子的人和没胡子的人。假设我们将数据分为两个簇，关于哪些点应该被聚类在一起，所有算法的结果一致。我们仍不知道找到的簇是否以某种方式对应我们感兴趣的概念。算法找到的可能是侧视图和正面视图、夜间拍摄的照片和白天拍摄的照片，或者iPhone拍摄的照片和安卓手机拍摄的照片。要想知道聚类是否对应于我们感兴趣的内容，唯一的办法就是对簇进行人工分析。</p><h2 id="在人脸数据集上比较算法">在人脸数据集上比较算法</h2><h3 id="用dbscan分析">用DBSCAN分析</h3><p>将k均值、DBSCAN和凝聚聚类算法应用于人脸数据集，并查看它们是否找到了有趣的结构。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从数据中提取特征脸，并对数组进行变换</span></span><br><span class="line">pca = PCA(n_components=<span class="number">100</span>, whiten=<span class="literal">True</span>, random_state=<span class="number">0</span>)</span><br><span class="line">X_pca = pca.fit_transform(X_people)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用默认参数的DBSCAN</span></span><br><span class="line">dbscan = DBSCAN()</span><br><span class="line">labels = dbscan.fit_predict(X_pca)</span><br><span class="line">print(<span class="string">'Unique labels: &#123;&#125;'</span>.format(np.unique(labels)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Unique labels: [-1]</span><br></pre></td></tr></table></figure><p>所有返回的标签都是-1，因此所有数据都被标记为“噪声”。我们可以改变两个参数来改进这一点：第一，我们可以增大eps，从而扩展每个点的邻域；第二，我们可以减小<code>min_samples</code>，从而将更小的点组视为簇。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 尝试改变min_samples</span></span><br><span class="line">dbscan = DBSCAN(min_samples=<span class="number">3</span>)</span><br><span class="line">labels = dbscan.fit_predict(X_pca)</span><br><span class="line">print(<span class="string">'Unique labels: &#123;&#125;'</span>.format(np.unique(labels)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Unique labels: [-1]</span><br></pre></td></tr></table></figure><p>即使仅考虑由三个点构成的组，所有点也都被标记为噪声。因此我们需要增大eps。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dbscan = DBSCAN(min_samples=<span class="number">3</span>, eps=<span class="number">15</span>)</span><br><span class="line">labels = dbscan.fit_predict(X_pca)</span><br><span class="line">print(<span class="string">'Unique labels: &#123;&#125;'</span>.format(np.unique(labels)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Unique labels: [-1  0]</span><br></pre></td></tr></table></figure><p>使用更大的eps，我们得到了单一簇和噪声点。我们可以利用这一结果找出“噪声”相对于其他数据的形状。为了进一步理解发生的事情，我们查看有多少点是噪声，有多少点在簇内。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算所有簇中的点数和噪声中的点数</span></span><br><span class="line"><span class="comment"># bincount不允许负值，所以我们需要加1</span></span><br><span class="line"><span class="comment"># 结果中的第一个数字对应于噪声点</span></span><br><span class="line">print(<span class="string">'Number of points per cluster: &#123;&#125;'</span>.format(np.bincount(labels + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">noise = X_people[labels == <span class="number">-1</span>]</span><br><span class="line">fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">11</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: (), <span class="string">'frameon'</span>: <span class="literal">False</span>&#125;)</span><br><span class="line"><span class="keyword">for</span> image, ax <span class="keyword">in</span> zip(noise, axes.ravel()):</span><br><span class="line">    ax.imshow(image.reshape(image_shape), vmin=<span class="number">0</span>, vmax=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Number of points per cluster: [  32 2031]</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/noise-in-dbscan.png"></p><p>我们可以猜测它们被标记为噪声的原因：第2行第3张图像显示一个人正在用玻璃杯喝水，还有人戴帽子，人脸前面有一只手的图像。其他图像都包含奇怪的角度，或者太近或太宽的剪切。这种类型的分析——尝试找出“奇怪的哪一个”——被称为<strong>异常值检测</strong>（outlier detection）。如果这是一个真实的应用，那么我们可能会尝试更好地裁切图像，以得到更加均匀的数据。对于照片中的人有时戴着帽子、喝水或在面前举着某物，我们能做的事情很少。但需要知道它们是数据中存在的问题，我们应用任何算法都需要解决这些问题。</p><p>如果我们想要找到更有趣的簇，而不是一个非常大的簇，那么需要将eps设置的很小，取值在15和0.5（默认值）之间。我们来看一下eps不同取值对应的结果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> eps <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>, <span class="number">13</span>]:</span><br><span class="line">    print(<span class="string">'\neps=&#123;&#125;'</span>.format(eps))</span><br><span class="line">    dbscan = DBSCAN(eps=eps, min_samples=<span class="number">3</span>)</span><br><span class="line">    labels = dbscan.fit_predict(X_pca)</span><br><span class="line">    print(<span class="string">'Clusters present: &#123;&#125;'</span>.format(np.unique(labels)))</span><br><span class="line">    print(<span class="string">'Cluster sizes: &#123;&#125;'</span>.format(np.bincount(labels + <span class="number">1</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">eps=1</span><br><span class="line">Clusters present: [-1]</span><br><span class="line">Cluster sizes: [2063]</span><br><span class="line"></span><br><span class="line">eps=3</span><br><span class="line">Clusters present: [-1]</span><br><span class="line">Cluster sizes: [2063]</span><br><span class="line"></span><br><span class="line">eps=5</span><br><span class="line">Clusters present: [-1]</span><br><span class="line">Cluster sizes: [2063]</span><br><span class="line"></span><br><span class="line">eps=7</span><br><span class="line">Clusters present: [-1  0  1  2  3  4  5  6  7  8  9 10 11 12]</span><br><span class="line">Cluster sizes: [2004    3   14    7    4    3    3    4    4    3    3    5    3    3]</span><br><span class="line"></span><br><span class="line">eps=9</span><br><span class="line">Clusters present: [-1  0  1  2]</span><br><span class="line">Cluster sizes: [1307  750    3    3]</span><br><span class="line"></span><br><span class="line">eps=11</span><br><span class="line">Clusters present: [-1  0]</span><br><span class="line">Cluster sizes: [ 413 1650]</span><br><span class="line"></span><br><span class="line">eps=13</span><br><span class="line">Clusters present: [-1  0]</span><br><span class="line">Cluster sizes: [ 120 1943]</span><br></pre></td></tr></table></figure><p>对于较小的eps，所有点都被标记为噪声。eps=7时，我们得到许多噪声点和许多较小的簇。eps=9时，我们仍得到许多噪声点，但我们得到了一个较大的簇和一些较小的簇。从eps=11开始，我们仅得到一个较大的簇和噪声。</p><p>有趣的是，<strong>较大的簇从来没有超过一个。最多有一个较大的簇包含大多数点，还有一些较小的簇。这表示数据中没有两类或三类非常不同的人脸图像，而是所有图像或多或少地都与其他图像具有相同的相似度（或不相似度）。</strong></p><p>eps=7的结果看起来最有趣，它有许多较小的簇。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dbscan = DBSCAN(min_samples=<span class="number">3</span>, eps=<span class="number">7</span>)</span><br><span class="line">labels = dbscan.fit_predict(X_pca)</span><br><span class="line"></span><br><span class="line">n_clusters = max(labels) + <span class="number">1</span></span><br><span class="line">masks = [labels == i <span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters)]</span><br><span class="line">n_cols = max(np.sum(mask) <span class="keyword">for</span> mask <span class="keyword">in</span> masks)</span><br><span class="line"></span><br><span class="line">content = []</span><br><span class="line">i = j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> mask <span class="keyword">in</span> masks:</span><br><span class="line">    <span class="keyword">if</span> n_cols - j - <span class="number">1</span> &lt; np.sum(mask):</span><br><span class="line">        i, j = i + <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> image, label <span class="keyword">in</span> zip(X_people[mask], y_people[mask]):</span><br><span class="line">        content.append((image, label, i, j))</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">    j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">n_rows = i + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(n_rows, n_cols,</span><br><span class="line">    figsize=(n_cols * <span class="number">1.5</span>, n_rows * <span class="number">2</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: (), <span class="string">'frameon'</span>: <span class="literal">False</span>&#125;)</span><br><span class="line"><span class="keyword">for</span> image, label, i, j <span class="keyword">in</span> content:</span><br><span class="line">    ax = axes[i, j]</span><br><span class="line">    ax.imshow(image.reshape(image_shape), vmin=<span class="number">0</span>, vmax=<span class="number">1</span>)</span><br><span class="line">    ax.set_title(people.target_names[label].split()[<span class="number">-1</span>])</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/clusters-of-7-eps-dbscan.png"></p><p>有一些簇对应于数据集中脸部非常不同的人，比如Sharon（沙龙）或Koizumi（小泉）。在每个簇中，人脸方向和面部表情也是固定的。有些簇中包含多个人的面孔，但他们的表情和方向都相似。</p><p>这就是将DBSCAN算法应用于人脸数据集的分析结论。如你所见，我们这里进行了人工分析，不同于监督学习中基于<span class="math inline">\(R^2\)</span>分数或精度的更为自动化的方法。</p><h3 id="用k均值分析">用k均值分析</h3><p>我们看到，利用DBSCAN无法创建多于一个较大的簇。凝聚聚类和k均值更可能创建君悦大小的簇，但我们需要设置簇的目标个数。我们可以将簇的数量设置为数据集中的已知人数，虽然无监督聚类算法不太可能完全找到它们。相反，我们可以首先设置一个比较小的簇的数量，比如10个，这样我们可以分析每个簇。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用k均值提取簇</span></span><br><span class="line">km = KMeans(n_clusters=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">labels_km = km.fit_predict(X_pca)</span><br><span class="line">print(<span class="string">'Cluster sizes k-means: &#123;&#125;'</span>.format(np.bincount(labels_km)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cluster sizes k-means: [155 175 238  75 358 257  91 219 323 172]</span><br></pre></td></tr></table></figure><p>k均值聚类将数据划分成大小相似的簇，其大小在75和358之间。这与DBSCAN的结果非常不同。我们可以通过将簇中心可视化来进一步分析k均值的结果。由于我们是在PCA生成的表示中进行聚类，因此我们需要使用<code>pca.inverse_transform</code>将簇中心旋转回到原始空间并可视化。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> center, ax <span class="keyword">in</span> zip(km.cluster_centers_, axes.ravel()):</span><br><span class="line">    ax.imshow(pca.inverse_transform(center).reshape(image_shape),</span><br><span class="line">        vmin=<span class="number">0</span>, vmax=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/cluster-centers-of-kmeans-faces.png"></p><p>k均值找到的簇中心是非常平滑的人脸。这并不奇怪，因为每个簇中心都是许多张人脸图像的平均。使用降维的PCA表示，可以增加图像的平滑度（对比利用100个PCA维度重建的人物图像）。聚类似乎捕捉到人脸的不同方向（下图的第7个簇）、不同表情（下图的第4个簇）。下图给出了更详细的视图，我们对每个簇中心给出了簇中5张最典型的图像（该簇中与簇中心距离最近的图像）与5张最不典型的图像（该簇中与簇中心距离最远的图像）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people, y_people,</span><br><span class="line">    people.target_names)</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/kmeans-faces-close-and-far.png"></p><p>“非典型的”点与簇中心不太相似，而且它们的分配似乎有些随意。这可以归因于以下事实：k均值对所有数据点进行划分，不像DBSCAN那样具有“噪声”点的概念。利用更多数量的簇，算法可以找到更细微的区别。但添加更多的簇会使得人工检查更加困难。</p><h3 id="用凝聚聚类分析">用凝聚聚类分析</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用ward凝聚聚类提取簇</span></span><br><span class="line">agglomerative = AgglomerativeClustering(n_clusters=<span class="number">10</span>)</span><br><span class="line">labels_agg = agglomerative.fit_predict(X_pca)</span><br><span class="line">print(<span class="string">'Cluster sizes agglomerative clustering: &#123;&#125;'</span>.format(</span><br><span class="line">    np.bincount(labels_agg)))</span><br><span class="line"><span class="comment"># 通过计算ARI来度量凝聚聚类和k均值给出的两种数据划分是否相似</span></span><br><span class="line">print(<span class="string">'ARI: &#123;:.2f&#125;'</span>.format(adjusted_rand_score(labels_agg, labels_km)))</span><br><span class="line"><span class="comment"># 绘制树状图，限制了树的深度，因为如果分支到2063个数据点，图像将密密麻麻无法阅读</span></span><br><span class="line">linkage_array = ward(X_pca)</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line">dendrogram(linkage_array, p=<span class="number">7</span>, truncate_mode=<span class="string">'level'</span>, no_labels=<span class="literal">True</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Sample index'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cluster distance'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cluster sizes agglomerative clustering: [169 660 144 329 217  85  18 261  31 149]</span><br><span class="line">ARI: 0.09</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/agglomerative-clustering-dendrogram-on-faces.png"></p><p>凝聚聚类生成的簇大小在18和660之间。这比k均值生成的簇更不均匀，但比DBSCAN生成的簇要更均匀。ARI只有0.09，说明<code>labels_agg</code>和<code>labels_km</code>这两种聚类的共同点很少。这不奇怪，原因在于以下事实：对于k均值，远离簇中心的点似乎没有什么共同点。</p><p>要想创建10个簇，我们在顶部有10条竖线的位置将树横切。在图示的玩具数据的树状图中，你可以从分支长度中看出，两个或三个簇就可以很好地划分数据。对于人脸数据而言，似乎没有非常自然的切割点。有一些分支代表更为不同的组，但似乎没有一个特别合适的簇的数量。这并不奇怪，因为DBSCAN（原书可能有错误，此处应该是凝聚聚类）的结果是视图将所有点都聚类在一起。</p><p>我们将10个簇可视化，正如之前对k均值所做的那样。在凝聚聚类中没有簇中心的概念（虽然我们计算平均值），我们只是给出了每个簇的前几个点。我们在第一张图的左侧给出了每个簇中的点的数量。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_clusters = <span class="number">10</span></span><br><span class="line">flg, axes = plt.subplots(n_clusters, <span class="number">10</span>, figsize=(<span class="number">15</span>, <span class="number">2</span> * n_clusters),</span><br><span class="line">    subplot_kw=&#123;<span class="string">'xticks'</span>: (), <span class="string">'yticks'</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> cluster <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    mask = labels_agg == cluster</span><br><span class="line">    axes[cluster, <span class="number">0</span>].set_ylabel(np.sum(mask))</span><br><span class="line">    <span class="keyword">for</span> image, label, asdf, ax <span class="keyword">in</span> zip(X_people[mask],</span><br><span class="line">            y_people[mask], labels_agg[mask], axes[cluster]):</span><br><span class="line">        ax.imshow(image.reshape(image_shape), vmin=<span class="number">0</span>, vmax=<span class="number">1</span>)</span><br><span class="line">        ax.set_title(people.target_names[label].split()[<span class="number">-1</span>],</span><br><span class="line">            fontdict=&#123;<span class="string">'fontsize'</span>: <span class="number">9</span>&#125;)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/uploads/image/introduction-to-ml-with-python/clusters-of-agglomerative-clustering.png"></p><p>虽然某些簇似乎具有语义上的主题，但许多簇都太大而实际上很难是均匀的。</p><h3 id="聚类方法小结">聚类方法小结</h3><p>聚类的应用与评估是一个非常定性的过程，通常在数据分析的探索阶段很有帮助。我们学习了三种聚类算法：k均值、DBSCAN和凝聚聚类。这三种算法都可以控制聚类的粒度（granularity）。k均值和凝聚聚类允许你指定想要的簇的数量，而DBSCAN允许你用eps参数定义接近程度，从而间接影响簇的大小。</p><p>每种算法的优点稍有不同。k均值可以用簇的平均值来表示簇。它还可以被看作一种分解方法，每个数据点都由其簇中心表示。DBSCAN可以检测到没有分配任何簇的“噪声点”，还可以帮助自动判断簇的数量。与其他两种方法不同，它允许簇具有复杂的形状。DBSCAN有时会生成大小差别很大的簇，这可能是它的优点，也可能是缺点。凝聚聚类可以提供数据的可能划分的整个层次结构，可以通过树状图轻松查看。</p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/" rel="bookmark">降维、特征提取与流形学习</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/introduction-to-ml-with-python/preprocess-and-scaling/" rel="bookmark">预处理与缩放</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/introduction-to-ml-with-python/unsupervised-learning-and-preprocessing/" rel="bookmark">无监督学习与预处理</a></div></li></ul><footer class="post-footer"><div class="post-tags"> <a href="/tags/unsupervised/" rel="tag"># 无监督</a> <a href="/tags/聚类/" rel="tag"># 聚类</a></div><div class="post-nav"><div class="post-nav-item"><a href="/introduction-to-ml-with-python/dimensionality-reduction-feature-extraction-and-manifold-learning/" rel="prev" title="降维、特征提取与流形学习"><i class="fa fa-chevron-left"></i> 降维、特征提取与流形学习</a></div><div class="post-nav-item"> <a href="/cplex-cutting-stock/" rel="next" title="cplex cutting stock">cplex cutting stock<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#k均值聚类"><span class="nav-number">1.</span> <span class="nav-text">k均值聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#k均值的失败案例"><span class="nav-number">1.1.</span> <span class="nav-text">k均值的失败案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#矢量量化或者将k均值看作分解"><span class="nav-number">1.2.</span> <span class="nav-text">矢量量化，或者将k均值看作分解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#凝聚聚类"><span class="nav-number">2.</span> <span class="nav-text">凝聚聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#层次聚类与树状图"><span class="nav-number">2.1.</span> <span class="nav-text">层次聚类与树状图</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dbscan"><span class="nav-number">3.</span> <span class="nav-text">DBSCAN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#聚类算法的对比与评估"><span class="nav-number">4.</span> <span class="nav-text">聚类算法的对比与评估</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#用真实值评估聚类"><span class="nav-number">4.1.</span> <span class="nav-text">用真实值评估聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#在没有真实值的情况下评估聚类"><span class="nav-number">4.2.</span> <span class="nav-text">在没有真实值的情况下评估聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#在人脸数据集上比较算法"><span class="nav-number">4.3.</span> <span class="nav-text">在人脸数据集上比较算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#用dbscan分析"><span class="nav-number">4.3.1.</span> <span class="nav-text">用DBSCAN分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用k均值分析"><span class="nav-number">4.3.2.</span> <span class="nav-text">用k均值分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用凝聚聚类分析"><span class="nav-number">4.3.3.</span> <span class="nav-text">用凝聚聚类分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#聚类方法小结"><span class="nav-number">4.3.4.</span> <span class="nav-text">聚类方法小结</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="菜农陈文生" src="/uploads/avatar/nekosensei.png"><p class="site-author-name" itemprop="name">菜农陈文生</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">301</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">28</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">118</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/cncws" title="GitHub → https://github.com/cncws" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:1031616423@qq.com" title="E-Mail → mailto:1031616423@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">菜农陈文生</span></div><script src="//cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="//cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script><script src="/js/aplayer-el.js"></script><script src="//unpkg.com/video.js/dist/video.min.js"></script><script src="/js/videojs-bilibili.js"></script><script src="/js/videojs-bilibili-el.js"></script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Axl15EIRi5o5AatKaxXxV4Oq-gzGzoHsz',
      appKey     : 'E0qm04UjsP0qQN1l8ME3GQ25',
      placeholder: "Just go go",
      avatar     : 'identicon',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script></div></body></html>